var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Readme",
    "title": "Readme",
    "category": "page",
    "text": ""
},

{
    "location": "#MLDataPattern-1",
    "page": "Readme",
    "title": "MLDataPattern",
    "category": "section",
    "text": "Utility package for subsetting, partitioning, iterating, and resampling of Machine Learning datasets. Aside from providing common functionality, this library also allows for first class support of custom user-defined data structures.Package Status Package Evaluator Build Status\n(Image: License) (Image: Documentation Status) (Image: MLDataPattern) (Image: MLDataPattern) (Image: Build Status) (Image: Build status) (Image: Coverage Status)"
},

{
    "location": "#Introduction-1",
    "page": "Readme",
    "title": "Introduction",
    "category": "section",
    "text": "Typical Machine Learning experiments require a lot of rather mundane but error prone data handling glue code. One particularly interesting category of data handling functionality - and the sole focus of this package - are what we call data access pattern. These \"pattern\" include subsetting, resampling, iteration, and partitioning of various types of data sets.MLDataPattern was designed around the core requirement of providing first class support for user-defined data sources. This idea is based on the assumption that the data source a user is working with, is likely of some very user-specific custom type. That said, we also put a lot of attention into first class support for the most commonly used data container, such as Array.Note that this package serves as a back-end for the end-user facing MLDataUtils.jl. If you are an end-user who is mainly interested in working with data interactively and conveniently you may want to use MLDataUtils instead."
},

{
    "location": "#Example-1",
    "page": "Readme",
    "title": "Example",
    "category": "section",
    "text": "Let us take a look at a hello world example (with little explanation) to get a feeling for how to use this package in a typical ML scenario. Note how the code snippet below does not reason about any training algorithm in any way. It is not the responsibility of the access pattern to decide in what form some algorithm needs the data. The output of the pattern depend solely on the input. In fact, the package is designed to be data agnostic. Instead, the focus is on efficiently chaining subsetting operations and thus to avoid temporary allocations where possible.using MLDataUtils # reexports MLDataPattern\n\n# X is a matrix of floats\n# Y is a vector of strings\nX, Y = MLDataUtils.load_iris()\n\n# The iris dataset is ordered according to their labels,\n# which means that we should shuffle the dataset before\n# partitioning it into training- and test-set.\nXs, Ys = shuffleobs((X, Y))\n# Notice how we use tuples to group data.\n\n# We leave out 15 % of the data for testing\n(cv_X, cv_Y), (test_X, test_Y) = splitobs((Xs, Ys); at = 0.85)\n\n# Next we partition the data using a 10-fold scheme.\n# Notice how we do not need to splat train into X and Y\nfor (train, (val_X, val_Y)) in kfolds((cv_X, cv_Y); k = 10)\n\n    for epoch = 1:100\n        # Iterate over the data using mini-batches of 5 observations each\n        for (batch_X, batch_Y) in eachbatch(train, size = 5)\n            # ... train supervised model on minibatches here\n        end\n    end\nendIn the above code snippet, the inner loop for eachbatch is the only place where data other than indices is actually being copied. That is because cv_X, test_X, val_X, etc. are all array views of type SubArray (the same applies to all the Y\'s of course). In contrast to this, batch_X and batch_Y will be of type Array. Naturally, array views only work for arrays, but this package provides a generalization of such a \"subset\" for any type of data container.Furthermore both, batch_X and batch_Y, will be the same instances each iteration with only their values changed. In other words, they both are preallocated buffers that will be reused each iteration and filled with the data for the current batch. Naturally, it is not a requirement to work with buffers like this, as stateful iterators can have undesired side effects when used without care. This package provides different alternatives for different use cases."
},

{
    "location": "#Documentation-1",
    "page": "Readme",
    "title": "Documentation",
    "category": "section",
    "text": "For a much more detailed treatment check out the latest documentationAdditionally, you can make use of Julia\'s native docsystem. The following example shows how to get additional information on kfolds within Julia\'s REPL:?kfolds"
},

{
    "location": "#Installation-1",
    "page": "Readme",
    "title": "Installation",
    "category": "section",
    "text": "This package is registered in METADATA.jl and can be installed as usual. Just start up Julia and type the following code snippet into the REPL. It makes use of the native Julia package manger.import Pkg\nPkg.add(\"MLDataPattern\")"
},

{
    "location": "#License-1",
    "page": "Readme",
    "title": "License",
    "category": "section",
    "text": "This code is free to use under the terms of the MIT license"
},

{
    "location": "autodocs/#MLDataPattern.BalancedObs",
    "page": "Docstrings",
    "title": "MLDataPattern.BalancedObs",
    "category": "type",
    "text": "BalancedObs([f], data, [count], [obsdim])\n\nDescription\n\nCreate an iterator that generates count randomly sampled observations from data. In the case count is not provided, it will generate random samples indefinitely.\n\nIn contrast to RandomObs, BalancedObs expects data to be a labeled data container. It uses the label distribution of data to make sure every label has an equal probability to be sampled from.\n\nArguments\n\nf : Optional. A function that should be applied to each   observation individually in order to extract or compute the   target for that observation. This function is only used once   during construction to determine which label each observation   belongs to.\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs,   nobs, and optionally gettargets (see   Details for more information).\ncount : Optional. The number of randomly sampled   observations that the iterator will generate before stopping.   If omitted, the iterator will generate randomly sampled   observations forever.\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nDetails\n\nFor BalancedObs to work on some data structure, the type of the given variable data must implement the labeled data container interface. See ?DataSubset for more info.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\n\nExamples\n\n# load first 55 observations of the iris data as example\n# - 50 observations for \"setosa\"\n# -  5 observations for \"versicolor\"\nX, Y = MLDataUtils.load_iris(55)\n\n# go over 100 balanced samples observations in X\nnum_versicolor = 0\nfor (x,y) in BalancedObs((X,Y), 100) # also: BalancedObs((X,Y), count = 100)\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert length(x) == 4\n    # count how many times we sample a versicolor observation\n    num_versicolor += y[] == \"versicolor\" ? 1 : 0\nend\nprintln(num_versicolor) # around 50\n\n# if no count it provided the iterator will generate samples forever\nfor (x,y) in BalancedObs((X,Y))\n    # this loop will never stop unless break is used\n    if true; break; end\nend\n\nsee also\n\nRandomObs, targets, RandomBatches, ObsView, BatchView, shuffleobs, DataSubset, BufferGetObs\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.BatchView",
    "page": "Docstrings",
    "title": "MLDataPattern.BatchView",
    "category": "type",
    "text": "BatchView(data, [size|maxsize], [count], [obsdim])\n\nDescription\n\nCreate a view of the given data that represents it as a vector of batches. Each batch will contain an equal amount of observations in them. The number of batches and the batch-size can be specified using (keyword) parameters count and size. In the case that the size of the dataset is not dividable by the specified (or inferred) size, the remaining observations will be ignored with a warning.\n\nNote that any data access is delayed until getindex is called, and even getindex returns the result of datasubset which in general avoids data movement until getobs is called.\n\nIf used as an iterator, the object will iterate over the dataset once, effectively denoting an epoch. Each iteration will return a mini-batch of constant nobs, which effectively allows to iterator over data one batch at a time.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\nsize : The batch-size of each batch. I.e. the number of   observations that each batch must contain.\nmaxsize : The maximum batch-size of each batch. I.e. the   number of observations that each batch should contain. If the   number of total observation is not divideable by the size it   will be reduced until it is.\ncount : The number of batches that the view will contain.\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nMethods\n\nAside from the AbstractArray interface following additional methods are provided.\n\ngetobs(data::BatchView, batchindices) :   Returns a Vector of the batches specified by batchindices.\nnobs(data::BatchView) :   Returns the total number of observations in data. Note that   unless the batch-size is 1, this number will differ from   length.\n\nDetails\n\nFor BatchView to work on some data structure, the type of the given variable data must implement the data container interface. See ?DataSubset for more info.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\nusing MLDataUtils\nX, Y = MLDataUtils.load_iris()\n\nA = batchview(X, size = 30)\n@assert typeof(A) <: BatchView <: AbstractVector\n@assert eltype(A) <: SubArray{Float64,2}\n@assert length(A) == 5 # Iris has 150 observations\n@assert size(A[1]) == (4,30) # Iris has 4 features\n\n# 5 batches of size 30 observations\nfor x in batchview(X, size = 30)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert nobs(x) === 30\nend\n\n# 7 batches of size 20 observations\n# Note that the iris dataset has 150 observations,\n# which means that with a batchsize of 20, the last\n# 10 observations will be ignored\nfor (x,y) in batchview((X,Y), size = 20)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\n    @assert nobs(x) === nobs(y) === 20\nend\n\n# 10 batches of size 15 observations\nfor (x,y) in batchview((X,Y), maxsize = 20)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\n    @assert nobs(x) === nobs(y) === 15\nend\n\n# randomly assign observations to one and only one batch.\nfor (x,y) in batchview(shuffleobs((X,Y)))\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\nend\n\n# iterate over the first 2 batches of 15 observation each\nfor (x,y) in batchview((X,Y), size=15, count=2)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\n    @assert size(x) == (4, 15)\n    @assert size(y) == (15,)\nend\n\nsee also\n\neachbatch, ObsView, shuffleobs, getobs, nobs, DataSubset\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.BufferGetObs",
    "page": "Docstrings",
    "title": "MLDataPattern.BufferGetObs",
    "category": "type",
    "text": "BufferGetObs(iterator, [buffer])\n\nA stateful iterator that stores the output of iterate(iterator,state) into buffer using getobs!(buffer, ...). Depending on the type of data provided by iterator this may be more memory efficient than getobs(...). In the case of array data, for example, this allows for cache-efficient processing of each element without allocating a temporary array.\n\nNote that not all types of data support buffering, because it is the author\'s choice to opt-in and implement a custom getobs!. For those types that do not provide a custom getobs!, the buffer will be ignored and the result of getobs(...) returned.\n\nsee eachobs and eachbatch for concrete examples.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.DataSubset",
    "page": "Docstrings",
    "title": "MLDataPattern.DataSubset",
    "category": "type",
    "text": "DataSubset(data, [indices], [obsdim])\n\nDescription\n\nUsed to represent a subset of some data of arbitrary type by storing which observation-indices the subset spans. Furthermore, subsequent subsettings are accumulated without needing to access actual data.\n\nThe main purpose for the existence of DataSubset is to delay data access and movement until an actual batch of data (or single observation) is needed for some computation. This is particularily useful when the data is not located in memory, but on the hard drive or some remote location. In such a scenario one wants to load the required data only when needed.\n\nThis type is usually not constructed manually, but instead instantiated by calling datasubset, shuffleobs, or splitobs\n\nIn case data is some Tuple, the constructor will be mapped over its elements. That means that the constructor returns a Tuple of DataSubset instead of a DataSubset of Tuple.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\nindices : Optional. The index or indices of the   observation(s) in data that the subset should represent.   Can be of type Int or some subtype of AbstractVector.\nobsdim : Optional. If it makes sense for the type of data,   obsdim can be used to specify which dimension of data   denotes the observations. It can be specified in a type-stable   manner as a positional argument (see ?LearnBase.ObsDim), or   more conveniently as a smart keyword argument.\n\nMethods\n\ngetindex : Returns the observation(s) of the given   index/indices as a new DataSubset. No data is copied aside   from the required indices.\nnobs : Returns the total number observations in the subset   (not the whole data set underneath).\ngetobs : Returns the underlying data that the   DataSubset represents at the given relative indices. Note   that these indices are in \"subset space\", and in general will   not directly correspond to the same indices in the underlying   data set.\n\nDetails\n\nFor DataSubset to work on some data structure, the desired type MyType must implement the following interface:\n\nLearnBase.getobs(data::MyType, idx, [obsdim::ObsDimension]) :   Should return the observation(s) indexed by idx.   In what form is up to the user.   Note that idx can be of type Int or AbstractVector.\nLearnBase.nobs(data::MyType, [obsdim::ObsDimension]) :   Should return the total number of observations in data\n\nThe following methods can also be provided and are optional:\n\nLearnBase.getobs(data::MyType) :   By default this function is the identity function.   If that is not the behaviour that you want for your type,   you need to provide this method as well.\nLearnBase.datasubset(data::MyType, idx, obsdim::ObsDimension) :   If your custom type has its own kind of subset type, you can   return it here. An example for such a case are SubArray for   representing a subset of some AbstractArray.   Note: If your type has no use for obsdim then dispatch on   ::ObsDim.Undefined in the signature.\nLearnBase.getobs!(buffer, data::MyType, [idx], [obsdim::ObsDimension]) :   Inplace version of getobs(data, idx, obsdim). If this method   is provided for MyType, then eachobs and eachbatch   (among others) can preallocate a buffer that is then reused   every iteration. Note: buffer should be equivalent to the   return value of getobs(::MyType, ...), since this is how   buffer is preallocated by default.\nLearnBase.gettargets(data::MyType, idx, [obsdim::ObsDimension]) :   If MyType has a special way to query targets without   needing to invoke getobs, then you can provide your own   logic here. This can be useful when the targets of your are   always loaded as metadata, while the data itself remains on   the hard disk until actually needed.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\nX, y = MLDataUtils.load_iris()\n\n# The iris set has 150 observations and 4 features\n@assert size(X) == (4,150)\n\n# Represents the 80 observations as a DataSubset\nsubset = DataSubset(X, 21:100)\n@assert nobs(subset) == 80\n@assert typeof(subset) <: DataSubset\n# getobs indexes into the subset\n@assert getobs(subset, 1:10) == X[:, 21:30]\n\n# You can also work with data that uses some other dimension\n# to denote the observations.\n@assert size(X\') == (150,4)\nsubset = DataSubset(X\', 21:100, obsdim = :first) # or \"obsdim = 1\"\n@assert nobs(subset) == 80\n\n# To specify the obsdim in a type-stable way, use positional arguments\n# provided by the submodule `ObsDim`.\n@inferred DataSubset(X\', 21:100, ObsDim.First())\n\n# Subsets also works for tuple of data. (useful for labeled data)\nsubset = DataSubset((X,y), 1:100)\n@assert nobs(subset) == 100\n@assert typeof(subset) <: Tuple # Tuple of DataSubset\n\n# The lowercase version tries to avoid boxing into DataSubset\n# for types that provide a custom \"subset\", such as arrays.\n# Here it instead creates a native SubArray.\nsubset = datasubset(X, 1:100)\n@assert nobs(subset) == 100\n@assert typeof(subset) <: SubArray\n\n# Also works for tuples of arbitrary length\nsubset = datasubset((X,y), 1:100)\n@assert nobs(subset) == 100\n@assert typeof(subset) <: Tuple # tuple of SubArray\n\n# Split dataset into training and test split\ntrain, test = splitobs(shuffleobs((X,y)), at = 0.7)\n@assert typeof(train) <: Tuple # of SubArray\n@assert typeof(test)  <: Tuple # of SubArray\n@assert nobs(train) == 105\n@assert nobs(test) == 45\n\nsee also\n\ndatasubset,  getobs, nobs, splitobs, shuffleobs, KFolds, BatchView, ObsView,\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.FoldsView",
    "page": "Docstrings",
    "title": "MLDataPattern.FoldsView",
    "category": "type",
    "text": "FoldsView(data, train_indices, val_indices, [obsdim])\n\nDescription\n\nCreate a vector-like representation of data where each individual element is partition of data in the form of a tuple of two data subsets (a training- and a validation subset).\n\nThe purpose of FoldsView is to apply a precomputed sequence of subset assignment indices to some data container in a convenient manner. By itself, FoldsView is agnostic to any particular repartitioning strategy (such as k-folds). Instead, the assignments, train_indices and val_indices, need to be precomputed by such a strategy and then passed to FoldsView with a concrete data container. The resulting object can then be queried for its individual folds using getindex, or simply iterated over.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\ntrain_indices : Vector of integer vectors containing the   sequence of training assignments. This means that each   element is a vector of indices that describe each training   subset. The length of this vector must match val_indices.\nval_indices : Vector of integer vectors containing the   sequence of validation assignments. This means that each   element is a vector of indices that describe each   validation subset. The length of this vector must match   train_indices.\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nDetails\n\nFor FoldsView to work on some data structure, the desired type MyType must implement the following interface:\n\nLearnBase.getobs(data::MyType, idx, [obsdim::ObsDimension]) :   Should return the observation(s) indexed by idx.   In what form is up to the user.   Note that idx can be of type Int or AbstractVector.\nLearnBase.nobs(data::MyType, [obsdim::ObsDimension]) :   Should return the total number of observations in data\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\n# Load iris data for demonstration purposes\nX, y = MLDataUtils.load_iris()\n\n# Compute train- and validation-partition indices using kfolds\ntrain_idx, val_idx = kfolds(nobs(X), 10)\n\n# These two vectors contain the indices vector for each partitioning\n@assert typeof(train_idx) <: Vector{Vector{Int64}}\n@assert typeof(val_idx)   <: Vector{UnitRange{Int64}}\n@assert length(train_idx) == length(val_idx) == 10\n\n# Using the repartitioning as an oterator\nfor (train_X, val_X) in FoldsView(X, train_idx, val_idx)\n    @assert size(train_X) == (4, 135)\n    @assert size(val_X) == (4, 15)\nend\n\n# Calling kfolds with the dataset will create\n# the FoldsView for you automatically.\n# Thus this code is equivalent to above\nfor (train_X, val_X) in kfolds(X, 10)\n    @assert size(train_X) == (4, 135)\n    @assert size(val_X) == (4, 15)\nend\n\n# leavout is a shortcut for setting k = nobs(X)\nfor (train_X, val_X) in leaveout(X)\n    @assert size(val_X) == (4, 1)\nend\n\nsee also\n\nkfolds, leaveout, splitobs, DataSubset\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.ObsView",
    "page": "Docstrings",
    "title": "MLDataPattern.ObsView",
    "category": "type",
    "text": "ObsView(data, [obsdim])\n\nDescription\n\nCreate a view of the given data in the form of a vector of individual observations. Any data access is delayed until getindex is called, and even getindex returns the result of datasubset which in general avoids data movement until getobs is called.\n\nIf used as an iterator, the view will iterate over the dataset once, effectively denoting an epoch. Each iteration will return a lazy subset to the current observation.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nMethods\n\nAside from the AbstractArray interface following additional methods are provided:\n\ngetobs(data::ObsView, indices::AbstractVector) :   Returns a Vector of indivdual observations specified by   indices.\nnobs(data::ObsView) :   Returns the number of observations in data that the   iterator will go over.\n\nDetails\n\nFor ObsView to work on some data structure, the type of the given variable data must implement the data container interface. See ?DataSubset for more info.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\nX, Y = MLDataUtils.load_iris()\n\nA = obsview(X)\n@assert typeof(A) <: ObsView <: AbstractVector\n@assert eltype(A) <: SubArray{Float64,1}\n@assert length(A) == 150 # Iris has 150 observations\n@assert size(A[1]) == (4,) # Iris has 4 features\n\nfor x in obsview(X)\n    @assert typeof(x) <: SubArray{Float64,1}\nend\n\n# iterate over each individual labeled observation\nfor (x,y) in obsview((X,Y))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\n# same but in random order\nfor (x,y) in obsview(shuffleobs((X,Y)))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\nsee also\n\neachobs, BatchView, shuffleobs, getobs, nobs, DataSubset\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.RandomBatches",
    "page": "Docstrings",
    "title": "MLDataPattern.RandomBatches",
    "category": "type",
    "text": "RandomBatches(data, [size], [count], [obsdim])\n\nDescription\n\nCreate an iterator that generates count randomly sampled batches from data with a batch-size of size . In the case count is not provided, it will generate random batches indefinitely.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\nsize : Optional. The batch-size of each batch.   I.e. the number of randomly sampled observations in each batch\ncount : Optional. The number of randomly sampled batches   that the iterator will generate before stopping. If omitted,   the iterator will generate randomly sampled batches forever.\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nDetails\n\nFor RandomBatches to work on some data structure, the type of given variable data must implement the data container interface. See ?DataSubset for more info.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\nX, Y = MLDataUtils.load_iris()\n\n# go over 500 randomly sampled batches of batchsize 10\ni = 0\nfor x in RandomBatches(X, 10, 500) # also: RandomObs(X, size = 10, count = 500)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert size(x) == (4,10)\n    i += 1\nend\n@assert i == 500\n\n# if no count it provided the iterator will generate samples forever\nfor x in RandomBatches(X, 10)\n    # this loop will never stop unless break is used\n    if true; break; end\nend\n\n# also works for multiple data arguments (e.g. labeled data)\nfor (x,y) in RandomBatches((X,Y), 10, 500)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\nend\n\nsee also\n\nRandomObs, BatchView, ObsView, shuffleobs, DataSubset, BufferGetObs\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.RandomObs",
    "page": "Docstrings",
    "title": "MLDataPattern.RandomObs",
    "category": "type",
    "text": "RandomObs(data, [count], [obsdim])\n\nDescription\n\nCreate an iterator that generates count randomly sampled observations from data. In the case count is not provided, it will generate random samples indefinitely.\n\nArguments\n\ndata : The object describing the dataset. Can be of any   type as long as it implements getobs and   nobs (see Details for more information).\ncount : Optional. The number of randomly sampled   observations that the iterator will generate before stopping.   If omitted, the iterator will generate randomly sampled   observations forever.\nobsdim : Optional. If it makes sense for the type of   data, obsdim can be used to specify which dimension of   data denotes the observations. It can be specified in a   type-stable manner as a positional argument (see   ?LearnBase.ObsDim), or more conveniently as a smart keyword   argument.\n\nDetails\n\nFor RandomObs to work on some data structure, the type of the given variable data must implement the data container interface. See ?DataSubset for more info.\n\nAuthor(s)\n\nChristof Stocker (Github: https://github.com/Evizero)\nTom Breloff (Github: https://github.com/tbreloff)\n\nExamples\n\nX, Y = MLDataUtils.load_iris()\n\n# go over 500 randomly sampled observations in X\ni = 0\nfor x in RandomObs(X, 500) # also: RandomObs(X, count = 500)\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert length(x) == 4\n    i += 1\nend\n@assert i == 500\n\n# if no count it provided the iterator will generate samples forever\nfor x in RandomObs(X)\n    # this loop will never stop unless break is used\n    if true; break; end\nend\n\n# also works for multiple data arguments (e.g. labeled data)\nfor (x,y) in RandomObs((X,Y), count = 100)\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\nsee also\n\nBalancedObs, RandomBatches, ObsView, BatchView, shuffleobs, DataSubset, BufferGetObs\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern._batchrange",
    "page": "Docstrings",
    "title": "MLDataPattern._batchrange",
    "category": "function",
    "text": "Helper function to translate a batch-index into a range of observations.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern._compute_batch_settings",
    "page": "Docstrings",
    "title": "MLDataPattern._compute_batch_settings",
    "category": "function",
    "text": "Helper function to compute sensible and compatible values for the size and count\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.batchsize",
    "page": "Docstrings",
    "title": "MLDataPattern.batchsize",
    "category": "function",
    "text": "batchsize(data) -> Int\n\nReturn the fixed size of each batch in data.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#LearnBase.datasubset",
    "page": "Docstrings",
    "title": "LearnBase.datasubset",
    "category": "function",
    "text": "datasubset(data, [indices], [obsdim])\n\nReturns a lazy subset of the observations in data that correspond to the given indices. No data will be copied except of the indices. It is similar to calling DataSubset(data, [indices], [obsdim]), but returns a SubArray if the type of data is Array or SubArray. Furthermore, this function may be extended for custom types of data that also want to provide their own subset-type.\n\nIf instead you want to get the subset of observations corresponding to the given indices in their native type, use getobs.\n\nIf it makes sense for the type of data, obsdim can be used to specify which dimension of data denotes the observations. It can be specified in a type-stable manner as a positional argument (see ?LearnBase.ObsDim), or more conveniently as a smart keyword argument.\n\nsee DataSubset for more information.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.eachbatch",
    "page": "Docstrings",
    "title": "MLDataPattern.eachbatch",
    "category": "function",
    "text": "eachbatch(data, [size], [count], [obsdim])\n\nIterate over data one batch at a time. If supported by the type of data, a buffer will be preallocated and reused for memory efficiency.\n\nIMPORTANT: Avoid using collect, because in general each iteration could return the same object with mutated values. If that behaviour is undesired use BatchView instead.\n\nThe (constant) batch-size can be either provided directly using size or indirectly using count, which derives size based on nobs. In the case that the size of the dataset is not dividable by the specified (or inferred) size, the remaining observations will be ignored.\n\nX = rand(4,150)\nfor x in eachbatch(X, size = 10) # or: eachbatch(X, count = 15)\n    # loop entered 15 times\n    @assert typeof(x) <: Matrix{Float64}\n    @assert size(x) == (4,10)\nend\n\nIn the case of arrays it is assumed that the observations are represented by the last array dimension. This can be overwritten.\n\n# This time flip the dimensions of the matrix\nX = rand(150,4)\nA = eachbatch(X, size = 10, obsdim = 1)\n# The behaviour remains the same as before\n@assert eltype(A) <: Array{Float64,2}\n@assert length(A) == 15\n\nMultiple variables are supported (e.g. for labeled data)\n\nfor (x,y) in eachbatch((X,Y))\n    # ...\nend\n\nNote that internally eachbatch(data, ...) maps to BufferGetObs(batchview(data, ...)).\n\n@assert typeof(eachbatch(X)) <: BufferGetObs\n@assert typeof(eachbatch(X).iter) <: BatchView\n\nThis means that the following code:\n\nfor batch in eachbatch(data, batchsize, obsdim)\n    # ...\nend\n\nis roughly equivalent to:\n\nbatch = getobs(data, collect(1:batchsize), obsdim) # use first element to preallocate buffer\nfor _ in batchview(data, batchsize, obsdim)\n    getobs!(batch, _) # reuse buffer each iteration\n    # ...\nend\n\nsee BufferGetObs, batchview, and getobs! for more info. also see eachobs for a single-observation version.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.eachobs",
    "page": "Docstrings",
    "title": "MLDataPattern.eachobs",
    "category": "function",
    "text": "eachobs(data, [obsdim])\n\nIterate over data one observation at a time. If supported by the type of data, a buffer will be preallocated and reused for memory efficiency.\n\nIMPORTANT: Avoid using collect, because in general each iteration could return the same object with mutated values. If that behaviour is undesired use obsview instead.\n\nX = rand(4,100)\nfor x in eachobs(X)\n    # loop entered 100 times\n    @assert typeof(x) <: Vector{Float64}\n    @assert size(x) == (4,)\nend\n\nIn the case of arrays it is assumed that the observations are represented by the last array dimension. This can be overwritten.\n\n# This time flip the dimensions of the matrix\nX = rand(100,4)\nA = eachobs(X, obsdim=1)\n# The behaviour remains the same as before\n@assert eltype(A) <: Array{Float64,1}\n@assert length(A) == 100\n\nMultiple variables are supported (e.g. for labeled data)\n\nfor (x,y) in eachobs((X,Y))\n    # ...\nend\n\nNote that internally eachobs(data, obsdim) maps to BufferGetObs(obsview(data, obsdim)).\n\n@assert typeof(eachobs(X)) <: BufferGetObs\n@assert typeof(eachobs(X).iter) <: ObsView\n\nThis means that the following code:\n\nfor obs in eachobs(data, obsdim)\n    # ...\nend\n\nis roughly equivalent to:\n\nobs = getobs(data, 1, obsdim) # use first element to preallocate buffer\nfor _ in obsview(data, obsdim)\n    getobs!(obs, _) # reuse buffer each iteration\n    # ...\nend\n\nsee BufferGetObs, obsview, and getobs! for more info. also see eachbatch for a mini-batch version.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#LearnBase.getobs",
    "page": "Docstrings",
    "title": "LearnBase.getobs",
    "category": "function",
    "text": "getobs(data, [idx], [obsdim])\n\nReturn the observation(s) in data that correspond to the given index/indices in idx. Note that idx can be of type Int or AbstractVector.\n\nThe returned observation(s) should be in the form intended to be passed as-is to some learning algorithm. There is no strict requirement that dictates what form or type that is. We do, however, expect it to be consistent for idx being an integer, as well as idx being an abstract vector, respectively.\n\nIf it makes sense for the type of data, obsdim can be used to specify which dimension of data denotes the observations. It can be specified in a type-stable manner as a positional argument (see ?LearnBase.ObsDim), or more conveniently as a smart keyword argument.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#LearnBase.getobs!",
    "page": "Docstrings",
    "title": "LearnBase.getobs!",
    "category": "function",
    "text": "getobs!(buffer, data, [idx], [obsdim]) -> buffer\n\nWrite the observation(s) from data that correspond to the given index/indices in idx into buffer. Note that idx can be of type Int or AbstractVector.\n\nUnless explicitly implemented for data it defaults to returning getobs(data, idx, obsdim) in which case buffer is ignored.\n\nIf it makes sense for the type of data, obsdim can be used to specify which dimension of data denotes the observations. It can be specified in a type-stable manner as a positional argument (see ?LearnBase.ObsDim), or more conveniently as a smart keyword argument.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.kfolds",
    "page": "Docstrings",
    "title": "MLDataPattern.kfolds",
    "category": "function",
    "text": "kfolds(n::Integer, [k = 5]) -> Tuple\n\nCompute the train/validation assignments for k repartitions of n observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. A general rule of thumb is to use either k = 5 or k = 10. The following code snippet generates the indices assignments for k = 5\n\njulia> train_idx, val_idx = kfolds(10, 5);\n\nEach observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range 1:n. Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset.\n\njulia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10\n\n\n\n\n\nkfolds(data, [k = 5], [obsdim]) -> FoldsView\n\nRepartition a data container k times using a k folds strategy and return the sequence of folds as a lazy FoldsView. The resulting FoldsView can then be indexed into or iterated over. Either way, only data subsets are created. That means that no actual data is copied until getobs is invoked.\n\nConceptually, a k-folds repartitioning strategy divides the given data into k roughly equal-sized parts. Each part will serve as validation set once, while the remaining parts are used for training. This results in k different partitions of data.\n\nIn the case that the size of the dataset is not dividable by the specified k, the remaining observations will be evenly distributed among the parts.\n\nfor (x_train, x_val) in kfolds(X, k = 10)\n    # code called 10 times\n    # nobs(x_val) may differ up to ±1 over iterations\nend\n\nMultiple variables are supported (e.g. for labeled data)\n\nfor ((x_train, y_train), val) in kfolds((X, Y), k = 10)\n    # ...\nend\n\nBy default the folds are created using static splits. Use shuffleobs to randomly assign observations to the folds.\n\nfor (x_train, x_val) in kfolds(shuffleobs(X), k = 10)\n    # ...\nend\n\nsee FoldsView for more info, or leaveout for a related function.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.leaveout",
    "page": "Docstrings",
    "title": "MLDataPattern.leaveout",
    "category": "function",
    "text": "leaveout(n::Integer, [size = 1]) -> Tuple\n\nCompute the train/validation assignments for k ≈ n/size repartitions of n observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. Each validation subset will have either size or size+1 observations assigned to it. The following code snippet generates the index-vectors for size = 2.\n\njulia> train_idx, val_idx = leaveout(10, 2);\n\nEach observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range 1:n. Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset.\n\njulia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10\n\n\n\n\n\nleaveout(data, [size = 1], [obsdim]) -> FoldsView\n\nRepartition a data container using a k-fold strategy, where k is chosen in such a way, that each validation subset of the resulting folds contains roughly size observations. Defaults to size = 1, which is also known as \"leave-one-out\" partitioning.\n\nThe resulting sequence of folds is returned as a lazy FoldsView, which can be index into or iterated over. Either way, only data subsets are created. That means no actual data is copied until getobs is invoked.\n\nfor (train, val) in leaveout(X, size = 2)\n    # if nobs(X) is dividable by 2,\n    # then nobs(val) will be 2 for each iteraton,\n    # otherwise it may be 3 for the first few iterations.\nend\n\nsee FoldsView for more info, or kfolds for a related function.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#StatsBase.nobs",
    "page": "Docstrings",
    "title": "StatsBase.nobs",
    "category": "function",
    "text": "nobs(data, [obsdim]) -> Int\n\nReturn the total number of observations contained in data.\n\nThe optional parameter obsdim can be used to specify which dimension denotes the observations, if that concept makes sense for the type of data. See ?LearnBase.ObsDim for more information.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.oversample",
    "page": "Docstrings",
    "title": "MLDataPattern.oversample",
    "category": "function",
    "text": "oversample([f], data, [fraction = 1], [shuffle = true], [obsdim])\n\nGenerate a re-balanced version of data by repeatedly sampling existing observations in such a way that every class will have at least fraction times the number observations of the largest class. This way, all classes will have a minimum number of observations in the resulting data set relative to what largest class has in the given (original) data.\n\nAs an example, by default (i.e. with fraction = 1) the resulting dataset will be near perfectly balanced. On the other hand, with fraction = 0.5 every class in the resulting data with have at least 50% as many observations as the largest class.\n\nThe convenience parameter shuffle determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the repeated samples will be together at the end, sorted by class. Defaults to true.\n\nThe optional parameter obsdim can be used to specify which dimension denotes the observations, if that concept makes sense for the type of data. See ?ObsDim for more information.\n\n# 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# oversample the class \"a\" to match \"b\"\nX_bal, Y_bal = oversample((X,Y))\n\n# this results in a bigger dataset with repeated data\n@assert size(X_bal) == (3,8)\n@assert length(Y_bal) == 8\n\n# now both \"a\", and \"b\" have 4 observations each\n@assert sum(Y_bal .== \"a\") == 4\n@assert sum(Y_bal .== \"b\") == 4\n\nFor this function to work, the type of data must implement nobs and getobs. For example, the following code allows oversample to work on a DataTable.\n\n# Make DataTables.jl work\nLearnBase.getobs(data::DataTable, i) = data[i,:]\nLearnBase.nobs(data::DataTable) = nrow(data)\n\nYou can use the parameter f to specify how to extract or retrieve the targets from each observation of the given data. Note that if data is a tuple, then it will be assumed that the last element of the tuple contains the targets and f will be applied to each observation in that element.\n\njulia> data = DataTable(Any[rand(6), rand(6), [:a,:b,:b,:b,:b,:a]], [:X1,:X2,:Y])\n6×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(oversample(row->row[:Y], data))\n8×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.0997825 │ 0.000341996 │ a │\n│ 2   │ 0.505208  │ 0.11202     │ b │\n│ 3   │ 0.226582  │ 0.0443222   │ a │\n│ 4   │ 0.0997825 │ 0.000341996 │ a │\n│ 5   │ 0.504629  │ 0.722906    │ b │\n│ 6   │ 0.522172  │ 0.245457    │ b │\n│ 7   │ 0.226582  │ 0.0443222   │ a │\n│ 8   │ 0.933372  │ 0.812814    │ b │\n\nsee DataSubset for more information on data subsets.\n\nsee also undersample and stratifiedobs.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.randobs",
    "page": "Docstrings",
    "title": "MLDataPattern.randobs",
    "category": "function",
    "text": "randobs(data, [n], [obsdim])\n\nPick a random observation or a batch of n random observations from data.\n\nThe optional (keyword) parameter obsdim allows one to specify which dimension denotes the observations. see LearnBase.ObsDim for more detail.\n\nFor this function to work, the type of data must implement nobs and getobs.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.shuffleobs",
    "page": "Docstrings",
    "title": "MLDataPattern.shuffleobs",
    "category": "function",
    "text": "shuffleobs(data, [obsdim])\n\nReturn a \"subset\" of data that spans all observations, but has the order of the observations shuffled.\n\nThe values of data itself are not copied. Instead only the indices are shuffled. This function calls datasubset to accomplish that, which means that the return value is likely of a different type than data.\n\n# For Arrays the subset will be of type SubArray\n@assert typeof(shuffleobs(rand(4,10))) <: SubArray\n\n# Iterate through all observations in random order\nfor (x) in eachobs(shuffleobs(X))\n    ...\nend\n\nThe optional (keyword) parameter obsdim allows one to specify which dimension denotes the observations. see LearnBase.ObsDim for more detail.\n\nFor this function to work, the type of data must implement nobs and getobs. See DataSubset for more information.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.slidingwindow",
    "page": "Docstrings",
    "title": "MLDataPattern.slidingwindow",
    "category": "function",
    "text": "slidingwindow(data, size, [stride], [obsdim]) -> UnlabeledSlidingWindow\n\nReturn a vector-like view of the data for which each element is a fixed size \"window\" of size adjacent observations. By default these windows are not overlapping. Note that only complete windows are included in the output, which implies that it is possible for excess observations to be omitted from the view.\n\njulia> A = slidingwindow(1:20, 6)\n3-element slidingwindow(::UnitRange{Int64}, 6) with element type SubArray{Int64,1,UnitRange{Int64},Tuple{UnitRange{Int64}},true}:\n [1, 2, 3, 4, 5, 6]\n [7, 8, 9, 10, 11, 12]\n [13, 14, 15, 16, 17, 18]\n\nNote that the values of data itself are not copied. Instead the function datasubset is called when getindex is invoked. To actually get a copy of the data at some window use the function getobs.\n\njulia> A[1]\n6-element SubArray{Int64,1,UnitRange{Int64},Tuple{UnitRange{Int64}},true}:\n 1\n ⋮\n 6\n\njulia> getobs(A, 1)\n6-element Array{Int64,1}:\n 1\n ⋮\n 6\n\nThe optional parameter stride can be used to specify the distance between the start elements of each adjacent window. By default the stride is equal to the window size.\n\njulia> slidingwindow(1:20, 6, stride = 3)\n5-element slidingwindow(::UnitRange{Int64}, 6, stride = 3) with element type SubArray{Int64,1,UnitRange{Int64},Tuple{UnitRange{Int64}},true}:\n [1, 2, 3, 4, 5, 6]\n [4, 5, 6, 7, 8, 9]\n [7, 8, 9, 10, 11, 12]\n [10, 11, 12, 13, 14, 15]\n [13, 14, 15, 16, 17, 18]\n\nThe optional (keyword) parameter obsdim allows one to specify which dimension denotes the observations. see LearnBase.ObsDim for more detail.\n\n\n\n\n\nslidingwindow(f, data, size, [stride], [excludetarget], [obsdim]) -> LabeledSlidingWindow\n\nReturn a vector-like view of the data for which each element is a tuple of two elements:\n\nA fixed size \"window\" of size adjacent observations. By default these windows are not overlapping. This can be changed by explicitly specifying a stride.\nA single target (or vector of targets) for the window. The content of the target(s) is defined by the label-index function f.\n\nThe label-index function f is a unary function that takes the index of the first observation in the current window and should return the index (or indices) of the associated target(s) for that window.\n\njulia> A = slidingwindow(i->i+6, 1:20, 6)\n3-element slidingwindow(::##3#4, ::UnitRange{Int64}, 6) with element type Tuple{...}\n ([1, 2, 3, 4, 5, 6], 7)\n ([7, 8, 9, 10, 11, 12], 13)\n ([13, 14, 15, 16, 17, 18], 19)\n\nNote that only complete and in-bound windows are included in the output, which implies that it is possible for excess observations to be omitted from the resulting view.\n\njulia> A = slidingwindow(i->i-1, 1:20, 6)\n2-element slidingwindow(::##5#6, ::UnitRange{Int64}, 6) with element type Tuple{...}\n ([7, 8, 9, 10, 11, 12], 6)\n ([13, 14, 15, 16, 17, 18], 12)\n\nAs hinted above, it is also allowed for f to return a vector of indices. This can be useful for emulating techniques such as skip-gram.\n\njulia> data = split(\"The quick brown fox jumps over the lazy dog\")\n9-element Array{SubString{String},1}:\n \"The\"\n \"quick\"\n ⋮\n \"lazy\"\n \"dog\"\n\njulia> A = slidingwindow(i->[i-2:i-1; i+1:i+2], data, 1)\n5-element slidingwindow(::##11#12, ::Array{SubString{String},1}, 1) with element type Tuple{...}:\n ([\"brown\"], [\"The\", \"quick\", \"fox\", \"jumps\"])\n ([\"fox\"], [\"quick\", \"brown\", \"jumps\", \"over\"])\n ([\"jumps\"], [\"brown\", \"fox\", \"over\", \"the\"])\n ([\"over\"], [\"fox\", \"jumps\", \"the\", \"lazy\"])\n ([\"the\"], [\"jumps\", \"over\", \"lazy\", \"dog\"])\n\nShould it so happen that the targets overlap with the features, then the affected observation(s) will be present in both. To change this behaviour one can set the optional parameter excludetarget = true. This will remove the target(s) from the feature window.\n\njulia> slidingwindow(i->i+2, data, 5, stride = 1, excludetarget = true)\n5-element slidingwindow(::##17#18, ::Array{SubString{String},1}, 5, stride = 1) with element type Tuple{...}:\n ([\"The\", \"quick\", \"fox\", \"jumps\"], \"brown\")\n ([\"quick\", \"brown\", \"jumps\", \"over\"], \"fox\")\n ([\"brown\", \"fox\", \"over\", \"the\"], \"jumps\")\n ([\"fox\", \"jumps\", \"the\", \"lazy\"], \"over\")\n ([\"jumps\", \"over\", \"lazy\", \"dog\"], \"the\")\n\nThe optional (keyword) parameter obsdim allows one to specify which dimension denotes the observations. see LearnBase.ObsDim for more detail.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.splitobs",
    "page": "Docstrings",
    "title": "MLDataPattern.splitobs",
    "category": "function",
    "text": "splitobs(n::Int, [at = 0.7]) -> Tuple\n\nPre-compute the indices for two disjoint subsets and return them as a tuple of two ranges. The first range will span the first at fraction of possible indices, while the second range will cover the rest. These indices are applicable to any data container of size n.\n\njulia> splitobs(100, at = 0.7)\n# (1:70,71:100)\n\n\n\n\n\nsplitobs(data, [at = 0.7], [obsdim]) -> Tuple\n\nSplit the data into multiple subsets proportional to the value(s) of at.\n\nNote that this function will perform the splits statically and thus not perform any randomization. The function creates a NTuple of data subsets in which the first N-1 elements/subsets contain the fraction of observations of data that is specified by at.\n\nFor example, if at is a Float64 then the return-value will be a tuple with two elements (i.e. subsets), in which the first element contains the fracion of observations specified by at and the second element contains the rest. In the following code the first subset train will contain the first 70% of the observations and the second subset test the rest.\n\ntrain, test = splitobs(X, at = 0.7)\n\nIf at is a tuple of Float64 then additional subsets will be created. In this example train will have the first 50% of the observations, val will have next 30%, and test the last 20%\n\ntrain, val, test = splitobs(X, at = (0.5, 0.3))\n\nIt is also possible to call splitobs with multiple data arguments as tuple, which all must have the same number of total observations. This is useful for labeled data.\n\ntrain, test = splitobs((X, y), at = 0.7)\n(x_train,y_train), (x_test,y_test) = splitobs((X, y), at = 0.7)\n\nIf the observations should be randomly assigned to a subset, then you can combine the function with shuffleobs\n\n# This time observations are randomly assigned.\ntrain, test = splitobs(shuffleobs((X,y)), at = 0.7)\n\nWhen working with arrays one may want to choose which dimension represents the observations. By default the last dimension is assumed, but this can be overwritten.\n\n# Here we say each row represents an observation\ntrain, test = splitobs(X, obsdim = 1)\n\nThe functions also provide a type-stable API\n\n# By avoiding keyword arguments, the compiler can infer the return type\ntrain, test = splitobs((X,y), 0.7)\ntrain, test = splitobs((X,y), 0.7, ObsDim.First())\n\nsee DataSubset for more information on data subsets.\n\nsee stratifiedobs for a related function that preserves the target distribution.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.stratifiedobs",
    "page": "Docstrings",
    "title": "MLDataPattern.stratifiedobs",
    "category": "function",
    "text": "stratifiedobs([f], data, [p = 0.7], [shuffle = true], [obsdim]) -> Tuple\n\nPartition the data into multiple disjoint subsets proportional to the value(s) of p. The observations are assignmed to a data subset using stratified sampling without replacement. These subsets are then returned as a Tuple of subsets, where the first element contains the fraction of observations of data that is specified by the first float in p.\n\nFor example, if p is a Float64 itself, then the return-value will be a tuple with two elements (i.e. subsets), in which the first element contains the fraction of observations specified by p and the second element contains the rest. In the following code the first subset train will contain around 70% of the observations and the second subset test the rest. The key difference to splitobs is that the class distribution in y will actively be preserved in train and test.\n\ntrain, test = stratifiedobs(y, p = 0.7)\n\nIf p is a tuple of Float64 then additional subsets will be created. In this example train will contain about 50% of the observations, val will contain around 30%, and test the remaining 20%.\n\ntrain, val, test = stratifiedobs(y, p = (0.5, 0.3))\n\nIt is also possible to call stratifiedobs with multiple data arguments as tuple, which all must have the same number of total observations. Note that if data is a tuple, then it will be assumed that the last element of the tuple contains the targets.\n\ntrain, test = stratifiedobs((X, y), p = 0.7)\n(X_train,y_train), (X_test,y_test) = stratifiedobs((X, y), p = 0.7)\n\nThe optional parameter shuffle determines if the resulting data subsets should be shuffled. If false, then the observations in the subsets will be grouped together according to their labels.\n\njulia> y = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"] # 2 imbalanced classes\n6-element Array{String,1}:\n \"a\"\n \"b\"\n \"b\"\n \"b\"\n \"b\"\n \"a\"\n\njulia> train, test = stratifiedobs(y, p = 0.5, shuffle = false)\n(String[\"b\",\"b\",\"a\"],String[\"b\",\"b\",\"a\"])\n\nThe optional parameter obsdim can be used to specify which dimension denotes the observations, if that concept makes sense for the type of data. See ?ObsDim for more information.\n\n# 2 imbalanced classes in one-of-k encoding\njulia> X = [1 0; 1 0; 1 0; 1 0; 0 1; 0 1]\n6×2 Array{Int64,2}:\n 1  0\n 1  0\n 1  0\n 1  0\n 0  1\n 0  1\n\njulia> train, test = stratifiedobs(argmax, X, p = 0.5, obsdim = 1)\n([1 0; 1 0; 0 1], [0 1; 1 0; 1 0])\n\nFor this function to work, the type of data must implement nobs and getobs. For example, the following code allows stratifiedobs to work on a DataTable.\n\n# Make DataTables.jl work\nLearnBase.getobs(data::DataTable, i) = data[i,:]\nLearnBase.nobs(data::DataTable) = nrow(data)\n\nYou can use the parameter f to specify how to extract or retrieve the targets from each observation of the given data.\n\njulia> data = DataTable(Any[rand(6), rand(6), [:a,:b,:b,:b,:b,:a]], [:X1,:X2,:Y])\n6×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> train, test = stratifiedobs(row->row[:Y], data, 0.5);\n\njulia> getobs(train)\n3×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.933372  │ 0.812814    │ b │\n│ 2   │ 0.522172  │ 0.245457    │ b │\n│ 3   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(test)\n3×3 DataTables.DataTable\n│ Row │ X1       │ X2        │ Y │\n├─────┼──────────┼───────────┼───┤\n│ 1   │ 0.504629 │ 0.722906  │ b │\n│ 2   │ 0.226582 │ 0.0443222 │ a │\n│ 3   │ 0.505208 │ 0.11202   │ b │\n\nsee DataSubset for more information on data subsets.\n\nsee also undersample, oversample, splitobs.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#LearnBase.targets",
    "page": "Docstrings",
    "title": "LearnBase.targets",
    "category": "function",
    "text": "targets([f], data, [obsdim])\n\nExtract the concrete targets from data and return them.\n\nThis function is eager in the sense that it will always call getobs unless a custom method for gettargets is implemented for the type of data. This will make sure that actual values are returned (in contrast to placeholders such as DataSubset or SubArray).\n\njulia> targets(DataSubset([1,2,3]))\n3-element Array{Int64,1}:\n 1\n 2\n 3\n\nIf data is a tuple, then the convention is that the last element of the tuple contains the targets and the function is recursed once (and only once).\n\njulia> targets(([1,2], [3,4]))\n2-element Array{Int64,1}:\n 3\n 4\n\njulia> targets(([1,2], ([3,4], [5,6])))\n([3,4],[5,6])\n\nIf f is provided, then gettarget will be applied to each observation in data and the results will be returned as a vector.\n\njulia> targets(argmax, [1 0 1; 0 1 0])\n3-element Array{Int64,1}:\n 1\n 2\n 1\n\nThe optional parameter obsdim can be used to specify which dimension denotes the observations, if that concept makes sense for the type of data. See ?ObsDim for more information.\n\njulia> targets(argmax, [1 0; 0 1; 1 0], obsdim=1)\n3-element Array{Int64,1}:\n 1\n 2\n 1\n\n\n\n\n\n"
},

{
    "location": "autodocs/#MLDataPattern.undersample",
    "page": "Docstrings",
    "title": "MLDataPattern.undersample",
    "category": "function",
    "text": "undersample([f], data, [shuffle = false], [obsdim])\n\nGenerate a class-balanced version of data by subsampling its observations in such a way that the resulting number of observations will be the same number for every class. This way, all classes will have as many observations in the resulting data set as the smallest class has in the given (original) data.\n\nThe convenience parameter shuffle determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the observations will be in their original order. Defaults to false.\n\nThe optional parameter obsdim can be used to specify which dimension denotes the observations, if that concept makes sense for the type of data. See ?ObsDim for more information.\n\n# 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# subsample the class \"b\" to match \"a\"\nX_bal, Y_bal = undersample((X,Y))\n\n# this results in a smaller dataset\n@assert size(X_bal) == (3,4)\n@assert length(Y_bal) == 4\n\n# now both \"a\", and \"b\" have 2 observations each\n@assert sum(Y_bal .== \"a\") == 2\n@assert sum(Y_bal .== \"b\") == 2\n\nFor this function to work, the type of data must implement nobs and getobs. For example, the following code allows undersample to work on a DataTable.\n\n# Make DataTables.jl work\nLearnBase.getobs(data::DataTable, i) = data[i,:]\nLearnBase.nobs(data::DataTable) = nrow(data)\n\nYou can use the parameter f to specify how to extract or retrieve the targets from each observation of the given data. Note that if data is a tuple, then it will be assumed that the last element of the tuple contains the targets and f will be applied to each observation in that element.\n\njulia> data = DataTable(Any[rand(6), rand(6), [:a,:b,:b,:b,:b,:a]], [:X1,:X2,:Y])\n6×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(undersample(row->row[:Y], data))\n4×3 DataTables.DataTable\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.522172  │ 0.245457    │ b │\n│ 4   │ 0.0997825 │ 0.000341996 │ a │\n\nsee DataSubset for more information on data subsets.\n\nsee also oversample and stratifiedobs.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#",
    "page": "Docstrings",
    "title": "Docstrings",
    "category": "page",
    "text": "MLDataPattern.BalancedObsMLDataPattern.BatchViewMLDataPattern.BufferGetObsMLDataPattern.DataSubsetMLDataPattern.FoldsViewMLDataPattern.LabeledSlidingWindowMLDataPattern.MLDataPatternMLDataPattern.ObsDimMLDataPattern.ObsViewMLDataPattern.RandomBatchesMLDataPattern.RandomObsMLDataPattern.SlidingWindowMLDataPattern.UnlabeledSlidingWindowMLDataPattern.WindowBatchViewMLDataPattern.__getobsMLDataPattern._batchrangeMLDataPattern._check_nobsMLDataPattern._check_nobs_errorMLDataPattern._check_windowargsMLDataPattern._compute_batch_settingsMLDataPattern._datastrMLDataPattern._getobsMLDataPattern._getobs_eltypeMLDataPattern._getobs_errorMLDataPattern._gettargetMLDataPattern._gettargetsMLDataPattern._gettargets_dispatch_idxMLDataPattern._getwindowbatchMLDataPattern._isposMLDataPattern._lengthMLDataPattern._length_strMLDataPattern._next_idxMLDataPattern._targetsMLDataPattern._toValMLDataPattern._viewMLDataPattern._windowsettingsMLDataPattern.allowcontainerMLDataPattern.batchsizeMLDataPattern.batchviewMLDataPattern.datasubsetMLDataPattern.default_batch_sizeMLDataPattern.downsampleMLDataPattern.eachbatchMLDataPattern.eachobsMLDataPattern.eachtargetMLDataPattern.evalMLDataPattern.getobsMLDataPattern.getobs!MLDataPattern.getobs_targetfunMLDataPattern.includeMLDataPattern.kfoldsMLDataPattern.leaveoutMLDataPattern.nobsMLDataPattern.obsdim_stringMLDataPattern.obsviewMLDataPattern.oversampleMLDataPattern.randobsMLDataPattern.shuffleobsMLDataPattern.slidingwindowMLDataPattern.splitobsMLDataPattern.stratifiedobsMLDataPattern.targetsMLDataPattern.undersampleMLDataPattern.upsample"
},

]}
