<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SVHN format 2 · MLDatasets.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="MLDatasets.jl logo"/></a><h1>MLDatasets.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><span class="toctext">Available Datasets</span><ul><li><span class="toctext">Image Classification</span><ul><li><a class="toctext" href="../MNIST/">MNIST handwritten digits</a></li><li><a class="toctext" href="../FashionMNIST/">Fashion MNIST</a></li><li><a class="toctext" href="../CIFAR10/">CIFAR-10</a></li><li><a class="toctext" href="../CIFAR100/">CIFAR-100</a></li><li class="current"><a class="toctext" href>SVHN format 2</a><ul class="internal"><li><a class="toctext" href="#Contents-1">Contents</a></li><li><a class="toctext" href="#Overview-1">Overview</a></li><li><a class="toctext" href="#API-Documentation-1">API Documentation</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li></ul></li></ul></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Available Datasets</li><li>Image Classification</li><li><a href>SVHN format 2</a></li></ul></nav><hr/><div id="topbar"><span>SVHN format 2</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="SVHN2-1" href="#SVHN2-1">The Street View House Numbers (SVHN) Dataset</a></h1><p>Description from the <a href="http://ufldl.stanford.edu/housenumbers/">official website</a>:</p><blockquote><p>SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.</p></blockquote><p>About Format 2 (Cropped Digits):</p><blockquote><p>All digits have been resized to a fixed resolution of 32-by-32 pixels. The original character bounding boxes are extended in the appropriate dimension to become square windows, so that resizing them to 32-by-32 pixels does not introduce aspect ratio distortions. Nevertheless this preprocessing introduces some distracting digits to the sides of the digit of interest.</p></blockquote><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>For non-commercial use only</p></div></div><h2><a class="nav-anchor" id="Contents-1" href="#Contents-1">Contents</a></h2><ul><li><a href="#SVHN2-1">The Street View House Numbers (SVHN) Dataset</a></li><ul><li><a href="#Contents-1">Contents</a></li><li><a href="#Overview-1">Overview</a></li><li><a href="#API-Documentation-1">API Documentation</a></li><ul><li><a href="#Trainingset-1">Trainingset</a></li><li><a href="#Testset-1">Testset</a></li><li><a href="#Extraset-1">Extraset</a></li><li><a href="#Utilities-1">Utilities</a></li></ul><li><a href="#References-1">References</a></li></ul></ul><h2><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h2><p>The <code>MLDatasets.SVHN2</code> sub-module provides a programmatic interface to download, load, and work with the SVHN2 dataset of handwritten digits.</p><pre><code class="language-julia">using MLDatasets

# load full training set
train_x, train_y = SVHN2.traindata()

# load full test set
test_x,  test_y  = SVHN2.testdata()

# load additional train set
extra_x, extra_y = SVHN2.extradata()</code></pre><p>The provided functions also allow for optional arguments, such as the directory <code>dir</code> where the dataset is located, or the specific observation <code>indices</code> that one wants to work with. For more information on the interface take a look at the documentation (e.g. <code>?SVHN2.traindata</code>).</p><table><tr><th>Function</th><th>Description</th></tr><tr><td><a href="#MLDatasets.SVHN2.download"><code>download([dir])</code></a></td><td>Trigger interactive download of the dataset</td></tr><tr><td><a href="#MLDatasets.SVHN2.classnames"><code>classnames()</code></a></td><td>Return the class names as a vector of strings</td></tr><tr><td><a href="#MLDatasets.SVHN2.traintensor"><code>traintensor([T], [indices]; [dir])</code></a></td><td>Load the training images as an array of eltype <code>T</code></td></tr><tr><td><a href="#MLDatasets.SVHN2.trainlabels"><code>trainlabels([indices]; [dir])</code></a></td><td>Load the labels for the training images</td></tr><tr><td><a href="#MLDatasets.SVHN2.traindata"><code>traindata([T], [indices]; [dir])</code></a></td><td>Load images and labels of the training data</td></tr><tr><td><a href="#MLDatasets.SVHN2.testtensor"><code>testtensor([T], [indices]; [dir])</code></a></td><td>Load the test images as an array of eltype <code>T</code></td></tr><tr><td><a href="#MLDatasets.SVHN2.testlabels"><code>testlabels([indices]; [dir])</code></a></td><td>Load the labels for the test images</td></tr><tr><td><a href="#MLDatasets.SVHN2.testdata"><code>testdata([T], [indices]; [dir])</code></a></td><td>Load images and labels of the test data</td></tr><tr><td><a href="#MLDatasets.SVHN2.extratensor"><code>extratensor([T], [indices]; [dir])</code></a></td><td>Load the extra images as an array of eltype <code>T</code></td></tr><tr><td><a href="#MLDatasets.SVHN2.extralabels"><code>extralabels([indices]; [dir])</code></a></td><td>Load the labels for the extra training images</td></tr><tr><td><a href="#MLDatasets.SVHN2.extradata"><code>extradata([T], [indices]; [dir])</code></a></td><td>Load images and labels of the extra training data</td></tr></table><p>This module also provides utility functions to make working with the SVHN (format 2) dataset in Julia more convenient.</p><table><tr><th>Function</th><th>Description</th></tr><tr><td><a href="#MLDatasets.SVHN2.convert2features"><code>convert2features(array)</code></a></td><td>Convert the SVHN tensor to a flat feature matrix</td></tr><tr><td><a href="#MLDatasets.SVHN2.convert2image"><code>convert2image(array)</code></a></td><td>Convert the SVHN tensor/matrix to a colorant array</td></tr></table><p>You can use the function <a href="#MLDatasets.SVHN2.convert2features"><code>convert2features</code></a> to convert the given SVHN tensor to a feature matrix (or feature vector in the case of a single image). The purpose of this function is to drop the spatial dimensions such that traditional ML algorithms can process the dataset.</p><pre><code class="language-julia">julia&gt; SVHN2.convert2features(SVHN2.traindata()[1]) # full training data
3072×73257 Array{N0f8,2}:
[...]</code></pre><p>To visualize an image or a prediction we provide the function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert the given SVHN2 horizontal-major tensor (or feature matrix) to a vertical-major <code>Colorant</code> array.</p><pre><code class="language-julia">julia&gt; SVHN2.convert2image(SVHN2.traindata(1)[1]) # first training image
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><h2><a class="nav-anchor" id="API-Documentation-1" href="#API-Documentation-1">API Documentation</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2" href="#MLDatasets.SVHN2"><code>MLDatasets.SVHN2</code></a> — <span class="docstring-category">Module</span>.</div><div><div><p>The Street View House Numbers (SVHN) Dataset</p><ul><li>Authors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng</li><li>Website: http://ufldl.stanford.edu/housenumbers</li></ul><p>SVHN was obtained from house numbers in Google Street View images. As such they are quite diverse in terms of orientation and image background. Similar to MNIST, SVHN has 10 classes (the digits 0-9), but unlike MNIST there is more data and the images are a little bigger (32x32 instead of 28x28) with an additional RGB color channel. The dataset is split up into three subsets: 73257 digits for training, 26032 digits for testing, and 531131 additional to use as extra training data.</p><p><strong>Interface</strong></p><ul><li><a href="#MLDatasets.SVHN2.traintensor"><code>SVHN2.traintensor</code></a>, <a href="#MLDatasets.SVHN2.trainlabels"><code>SVHN2.trainlabels</code></a>, <a href="#MLDatasets.SVHN2.traindata"><code>SVHN2.traindata</code></a></li><li><a href="#MLDatasets.SVHN2.testtensor"><code>SVHN2.testtensor</code></a>, <a href="#MLDatasets.SVHN2.testlabels"><code>SVHN2.testlabels</code></a>, <a href="#MLDatasets.SVHN2.testdata"><code>SVHN2.testdata</code></a></li><li><a href="#MLDatasets.SVHN2.extratensor"><code>SVHN2.extratensor</code></a>, <a href="#MLDatasets.SVHN2.extralabels"><code>SVHN2.extralabels</code></a>, <a href="#MLDatasets.SVHN2.extradata"><code>SVHN2.extradata</code></a></li></ul><p><strong>Utilities</strong></p><ul><li><a href="#MLDatasets.SVHN2.download"><code>SVHN2.download</code></a></li><li><a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a></li><li><a href="#MLDatasets.SVHN2.convert2features"><code>SVHN2.convert2features</code></a></li><li><a href="#MLDatasets.SVHN2.convert2image"><code>SVHN2.convert2image</code></a></li></ul></div></div></section><h3><a class="nav-anchor" id="Trainingset-1" href="#Trainingset-1">Trainingset</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.traintensor" href="#MLDatasets.SVHN2.traintensor"><code>MLDatasets.SVHN2.traintensor</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">traintensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>training</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.traintensor() # load all training images
32×32×3×73257 Array{N0f8,4}:
[...]

julia&gt; SVHN.traintensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.traintensor(1) # load first training image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.convert2image(SVHN2.traintensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.trainlabels" href="#MLDatasets.SVHN2.trainlabels"><code>MLDatasets.SVHN2.trainlabels</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">trainlabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>training</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.trainlabels() # full training set
73257-element Array{Int64,1}:
[...]

julia&gt; SVHN2.trainlabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.trainlabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.trainlabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.traindata" href="#MLDatasets.SVHN2.traindata"><code>MLDatasets.SVHN2.traindata</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">traindata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>trainset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full trainset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.traindata</code> will always load the full trainset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia">images, labels = SVHN2.traindata() # full dataset
images, labels = SVHN2.traindata(2) # only second observation
images, labels = SVHN2.traindata(dir=&quot;./SVHN&quot;) # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><h3><a class="nav-anchor" id="Testset-1" href="#Testset-1">Testset</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.testtensor" href="#MLDatasets.SVHN2.testtensor"><code>MLDatasets.SVHN2.testtensor</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">testtensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>test</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.testtensor() # load all test images
32×32×3×26032 Array{N0f8,4}:
[...]

julia&gt; SVHN.testtensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.testtensor(1) # load first test image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.convert2image(SVHN2.testtensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.testlabels" href="#MLDatasets.SVHN2.testlabels"><code>MLDatasets.SVHN2.testlabels</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">testlabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>test</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.testlabels() # full test set
26032-element Array{Int64,1}:
[...]

julia&gt; SVHN2.testlabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.testlabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.testlabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.testdata" href="#MLDatasets.SVHN2.testdata"><code>MLDatasets.SVHN2.testdata</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">testdata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>testset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full testset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.testdata</code> will always load the full testset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia">images, labels = SVHN2.testdata() # full dataset
images, labels = SVHN2.testdata(2) # only second observation
images, labels = SVHN2.testdata(dir=&quot;./SVHN&quot;) # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><h3><a class="nav-anchor" id="Extraset-1" href="#Extraset-1">Extraset</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.extratensor" href="#MLDatasets.SVHN2.extratensor"><code>MLDatasets.SVHN2.extratensor</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">extratensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>extra training</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.extratensor() # load all extra training images
32×32×3×531131 Array{N0f8,4}:
[...]

julia&gt; SVHN.extratensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.extratensor(1) # load first extra training image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.convert2image(SVHN2.extratensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.extralabels" href="#MLDatasets.SVHN2.extralabels"><code>MLDatasets.SVHN2.extralabels</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">extralabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>extra training</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl">julia&gt; SVHN2.extralabels() # full extra training set
531131-element Array{Int64,1}:
[...]

julia&gt; SVHN2.extralabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.extralabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.extralabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.extradata" href="#MLDatasets.SVHN2.extradata"><code>MLDatasets.SVHN2.extradata</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">extradata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>extra trainset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full extra trainset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.extradata</code> will always load the full extra trainset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia">images, labels = SVHN2.extradata() # full dataset
images, labels = SVHN2.extradata(2) # only second observation
images, labels = SVHN2.extradata(dir=&quot;./SVHN&quot;) # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div></div></section><h3><a class="nav-anchor" id="Utilities-1" href="#Utilities-1">Utilities</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.download" href="#MLDatasets.SVHN2.download"><code>MLDatasets.SVHN2.download</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">download([dir]; [i_accept_the_terms_of_use])</code></pre><p>Trigger the (interactive) download of the full dataset into &quot;<code>dir</code>&quot;. If no <code>dir</code> is provided the dataset will be downloaded into &quot;~/.julia/datadeps/SVHN2&quot;.</p><p>This function will display an interactive dialog unless either the keyword parameter <code>i_accept_the_terms_of_use</code> or the environment variable <code>DATADEPS_ALWAY_ACCEPT</code> is set to <code>true</code>. Note that using the data responsibly and respecting copyright/terms-of-use remains your responsibility.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.classnames" href="#MLDatasets.SVHN2.classnames"><code>MLDatasets.SVHN2.classnames</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">classnames() -&gt; Vector{Int}</code></pre><p>Return the 10 digits for the SVHN classes as a vector of integers.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.convert2features" href="#MLDatasets.SVHN2.convert2features"><code>MLDatasets.SVHN2.convert2features</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">convert2features(array)</code></pre><p>Convert the given SVHN tensor to a feature matrix (or feature vector in the case of a single image). The purpose of this function is to drop the spatial dimensions such that traditional ML algorithms can process the dataset.</p><pre><code class="language-julia">julia&gt; SVHN2.convert2features(SVHN2.traindata(Float32)[1]) # full training data
3072×50000 Array{Float32,2}:
[...]

julia&gt; SVHN2.convert2features(SVHN2.traindata(Float32,1)[1]) # first observation
3072-element Array{Float32,1}:
[...]</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLDatasets.SVHN2.convert2image" href="#MLDatasets.SVHN2.convert2image"><code>MLDatasets.SVHN2.convert2image</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">convert2image(array) -&gt; Array{RGB}</code></pre><p>Convert the given SVHN tensor (or feature vector/matrix) to a <code>RGB</code> array.</p><pre><code class="language-julia">julia&gt; SVHN2.convert2image(SVHN2.traindata()[1]) # full training dataset
32×32×50000 Array{RGB{N0f8},3}:
[...]

julia&gt; SVHN2.convert2image(SVHN2.traindata(1)[1]) # first training image
32×32 Array{RGB{N0f8},2}:
[...]</code></pre></div></div></section><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><ul><li><p><strong>Authors</strong>: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng</p></li><li><p><strong>Website</strong>: http://ufldl.stanford.edu/housenumbers</p></li><li><p><strong>[Netzer et al., 2011]</strong> Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng. &quot;Reading Digits in Natural Images with Unsupervised Feature Learning&quot; NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</p></li></ul><footer><hr/><a class="previous" href="../CIFAR100/"><span class="direction">Previous</span><span class="title">CIFAR-100</span></a><a class="next" href="../../indices/"><span class="direction">Next</span><span class="title">Indices</span></a></footer></article></body></html>
