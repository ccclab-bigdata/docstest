<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· NMF.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NMF.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#NMF.jl-1">NMF.jl</a></li><li><a class="toctext" href="#Development-Status-1">Development Status</a></li><li><a class="toctext" href="#Overview-1">Overview</a></li><li><a class="toctext" href="#High-Level-Interface-1">High-Level Interface</a></li><li><a class="toctext" href="#Initialization-1">Initialization</a></li><li><a class="toctext" href="#Factorization-Algorithms-1">Factorization Algorithms</a></li><li><a class="toctext" href="#Examples-1">Examples</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="NMF.jl-1" href="#NMF.jl-1">NMF.jl</a></h2><p>A Julia package for non-negative matrix factorization (NMF).</p><p><a href="https://travis-ci.org/JuliaStats/NMF.jl"><img src="https://travis-ci.org/JuliaStats/NMF.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/JuliaStats/NMF.jl?branch=master"><img src="https://coveralls.io/repos/github/JuliaStats/NMF.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><hr/><h2><a class="nav-anchor" id="Development-Status-1" href="#Development-Status-1">Development Status</a></h2><p><strong>Note:</strong> Nonnegative Matrix Factorization is an area of active research. New algorithms are proposed every year. Contributions are very welcomed.</p><h4><a class="nav-anchor" id="Done-1" href="#Done-1">Done</a></h4><ul><li>Lee &amp; Seung&#39;s Multiplicative Update (for both MSE &amp; Divergence objectives)</li><li>(Naive) Projected Alternate Least Squared</li><li>ALS Projected Gradient Methods</li><li>Random Initialization</li><li>NNDSVD Initialization</li></ul><h4><a class="nav-anchor" id="To-do-1" href="#To-do-1">To do</a></h4><ul><li>Sparse NMF</li><li>Probabilistic NMF</li></ul><h2><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h2><p><em>Non-negative Matrix Factorization (NMF)</em> generally refers to the techniques for factorizing a non-negative matrix <span>$X$</span> into the product of two lower rank matrices <span>$W$</span> and <span>$H$</span>, such that <span>$WH$</span> optimally approximates <span>$X$</span> in some sense. Such techniques are widely used in text mining, image analysis, and recommendation systems. </p><p>This package provides two sets of tools, respectively for <em>initilization</em> and <em>optimization</em>. A typical NMF procedure consists of two steps: (1) use an initilization function that initialize <span>$W$</span> and <span>$H$</span>; and (2) use an optimization algorithm to pursue the optimal solution.</p><p>Most types and functions (except the high-level function <span>$nnmf$</span>) in this package are not exported. Users are encouraged to use them with the prefix <span>$NMF.$</span>. This way allows us to use shorter names within the package and makes the codes more explicit and clear on the user side.</p><h2><a class="nav-anchor" id="High-Level-Interface-1" href="#High-Level-Interface-1">High-Level Interface</a></h2><p>The package provides a high-level function <span>$nnmf$</span> that runs the entire procedure (initialization + optimization):</p><p><strong>nnmf</strong>(X, k, ...)</p><p>This function factorizes the input matrix <span>$X$</span> into the product of two non-negative matrices <span>$W$</span> and <span>$H$</span>. </p><p>In general, it returns a result instance of type <span>$NMF.Result$</span>, which is defined as</p><pre><code class="language-julia">struct Result
    W::Matrix{Float64}    # W matrix
    H::Matrix{Float64}    # H matrix
    niters::Int           # number of elapsed iterations
    converged::Bool       # whether the optimization procedure converges
    objvalue::Float64     # objective value of the last step
end</code></pre><p>The function supports the following keyword arguments:</p><ul><li><p><span>$init$</span>:  A symbol that indicates the initialization method (default = <span>$:nndsvdar$</span>). </p><p>This argument accepts the following values:</p><ul><li><span>$random$</span>:  matrices filled with uniformly random values</li><li><span>$nndsvd$</span>:  standard version of NNDSVD</li><li><span>$nndsvda$</span>:  NNDSVDa variant</li><li><span>$nndsvdar$</span>:  NNDSVDar variant  </li></ul></li><li><p><span>$alg$</span>:  A symbol that indicates the factorization algorithm (default = <span>$:alspgrad$</span>).</p><p>This argument accepts the following values:</p><ul><li><span>$multmse$</span>:  Multiplicative update (using MSE as objective)</li><li><span>$multdiv$</span>:  Multiplicative update (using divergence as objective)</li><li><span>$projals$</span>:  (Naive) Projected Alternate Least Square</li><li><span>$alspgrad$</span>:  Alternate Least Square using Projected Gradient Descent</li></ul></li><li><p><span>$maxiter$</span>: Maximum number of iterations (default = <span>$100$</span>).</p></li><li><p><span>$tol$</span>: tolerance of changes upon convergence (default = <span>$1.0e-6$</span>).</p></li><li><p><span>$verbose$</span>: whether to show procedural information (default = <span>$false$</span>).</p></li></ul><h2><a class="nav-anchor" id="Initialization-1" href="#Initialization-1">Initialization</a></h2><ul><li><p><strong>NMF.randinit</strong>(X, k[; zeroh=false, normalize=false])</p><p>Initialize <span>$W$</span> and <span>$H$</span> given the input matrix <span>$X$</span> and the rank <span>$k$</span>. This function returns a pair <span>$(W, H)$</span>. </p><p>Suppose the size of <span>$X$</span> is <span>$(p, n)$</span>, then the size of <span>$W$</span> and <span>$H$</span> are respectively <span>$(p, k)$</span> and <span>$(k, n)$</span>.</p><p>Usage:</p><p><code>julia   W, H = NMF.randinit(X, 3)</code></p><p>For some algorithms (<em>e.g.</em> ALS), only <span>$W$</span> needs to be initialized. For such cases, one may set the keyword argument <span>$zeroh$</span>to be <span>$true$</span>, then in the output <span>$H$</span> will be simply a zero matrix of size <span>$(k, n)$</span>.</p><p>Another keyword argument is <span>$normalize$</span>. If <span>$normalize$</span> is set to <span>$true$</span>, columns of <span>$W$</span> will be normalized such that each column sum to one.</p></li><li><p><strong>NMF.nndsvd</strong>(X, k[; zeroh=false, variant=:std])</p><p>Use the <em>Non-Negative Double Singular Value Decomposition (NNDSVD)</em> algorithm to initialize <span>$W$</span> and <span>$H$</span>. </p><p>Reference: C. Boutsidis, and E. Gallopoulos. SVD based initialization: A head start for nonnegative matrix factorization. Pattern Recognition, 2007.</p><p>Usage:</p><p><code>julia   W, H = NMF.nndsvd(X, k)</code></p><p>This function has two keyword arguments:</p><ul><li><span>$zeroh$</span>: have <span>$H$</span> initialized when it is set to <span>$true$</span>, or set <span>$H$</span> to all zeros when it is set to <span>$false$</span>.</li><li><span>$variant$</span>: the variant of the algorithm. Default is <span>$std$</span>, meaning to use the standard version, which would generate a rather sparse <span>$W$</span>. Other values are <span>$a$</span> and <span>$ar$</span>, respectively corresponding to the variants: <em>NNDSVDa</em> and <em>NNDSVDar</em>. Particularly, <span>$ar$</span> is recommended for dense NMF.</li></ul></li></ul><h2><a class="nav-anchor" id="Factorization-Algorithms-1" href="#Factorization-Algorithms-1">Factorization Algorithms</a></h2><p>This package provides multiple factorization algorithms. Each algorithm corresponds to a type. One can create an algorithm <em>instance</em> by choosing a type and specifying the options, and run the algorithm using <span>$NMF.solve!$</span>:</p><h4><a class="nav-anchor" id="The-NMF.solve!-Function-1" href="#The-NMF.solve!-Function-1">The NMF.solve! Function</a></h4><p><strong>NMF.solve!</strong>(alg, X, W, H)</p><p>Use the algorithm <span>$alg$</span> to factorize <span>$X$</span> into <span>$W$</span> and <span>$H$</span>. </p><p>Here, <span>$W$</span> and <span>$H$</span> must be pre-allocated matrices (respectively of size <span>$(p, k)$</span> and <span>$(k, n)$</span>). <span>$W$</span> and <span>$H$</span> must be appropriately initialized before this function is invoked. For some algorithms, both <span>$W$</span> and <span>$H$</span> must be initialized (<em>e.g.</em> multiplicative updating); while for others, only <span>$W$</span> needs to be initialized (<em>e.g.</em> ALS).</p><p>The matrices <span>$W$</span> and <span>$H$</span> are updated in place.</p><h4><a class="nav-anchor" id="Algorithms-1" href="#Algorithms-1">Algorithms</a></h4><ul><li><p><strong>Multiplicative Updating</strong></p><p>Reference: Daniel D. Lee and H. Sebastian Seung. Algorithms for Non-negative Matrix Factorization. Advances in NIPS, 2001.</p><p>This algorithm has two different kind of objectives: minimizing mean-squared-error (<span>$:mse$</span>) and minimizing divergence (<span>$:div$</span>). Both <span>$W$</span> and <span>$H$</span> need to be initialized.</p><p><code>julia   MultUpdate(obj=:mse,        # objective, either :mse or :div              maxiter=100,     # maximum number of iterations              verbose=false,   # whether to show procedural information              tol=1.0e-6,      # tolerance of changes on W and H upon convergence              lambda=1.0e-9)   # regularization coefficients (added to the denominator)</code></p><p><strong>Note:</strong> the values above are default values for the keyword arguments. One can override part (or all) of them.</p></li></ul><ul><li><p><strong>(Naive) Projected Alternate Least Square</strong></p><p>This algorithm alternately updates <span>$W$</span> and <span>$H$</span> while holding the other fixed. Each update step solves <span>$W$</span> or <span>$H$</span> without enforcing the non-negativity constrait, and forces all negative entries to zeros afterwards. Only <span>$W$</span> needs to be initialized. </p><p><code>julia   ProjectedALS(maxiter::Integer=100,    # maximum number of iterations                verbose::Bool=false,     # whether to show procedural information                tol::Real=1.0e-6,        # tolerance of changes on W and H upon convergence                lambda_w::Real=1.0e-6,   # L2 regularization coefficient for W                lambda_h::Real=1.0e-6)   # L2 regularization coefficient for H</code></p></li><li><p><strong>Alternate Least Square Using Projected Gradient Descent</strong></p><p>Reference: Chih-Jen Lin. Projected Gradient Methods for Non-negative Matrix Factorization. Neural Computing, 19 (2007).</p><p>This algorithm adopts the alternate least square strategy. A efficient projected gradient descent method is used to solve each sub-problem. Both <span>$W$</span> and <span>$H$</span> need to be initialized.</p><p><code>julia   ALSPGrad(maxiter::Integer=100,      # maximum number of iterations (in main procedure)            maxsubiter::Integer=200,   # maximum number of iterations in solving each sub-problem            tol::Real=1.0e-6,          # tolerance of changes on W and H upon convergence            tolg::Real=1.0e-4,         # tolerable gradient norm in sub-problem (first-order optimality)            verbose::Bool=false)       # whether to show procedural information</code></p></li></ul><h2><a class="nav-anchor" id="Examples-1" href="#Examples-1">Examples</a></h2><p>Here are examples that demonstrate how to use this package to factorize a non-negative dense matrix.</p><h4><a class="nav-anchor" id="Use-High-level-Function:-nnmf-1" href="#Use-High-level-Function:-nnmf-1">Use High-level Function: nnmf</a></h4><pre><code class="language-julia">... # prepare input matrix X

r = nnmf(X, k; alg=:multmse, maxiter=30, tol=1.0e-4)

W = r.W
H = r.H</code></pre><h4><a class="nav-anchor" id="Use-Multiplicative-Update-1" href="#Use-Multiplicative-Update-1">Use Multiplicative Update</a></h4><pre><code class="language-julia">import NMF

 # initialize
W, H = NMF.randinit(X, 5)

 # optimize 
NMF.solve!(NMF.MultUpdate(obj=:mse,maxiter=100), X, W, H)</code></pre><h4><a class="nav-anchor" id="Use-Naive-ALS-1" href="#Use-Naive-ALS-1">Use Naive ALS</a></h4><pre><code class="language-julia">import NMF

 # initialize
W, H = NMF.randinit(X, 5)

 # optimize 
NMF.solve!(NMF.ProjectedALS(maxiter=50), X, W, H)</code></pre><h4><a class="nav-anchor" id="Use-ALS-with-Projected-Gradient-Descent-1" href="#Use-ALS-with-Projected-Gradient-Descent-1">Use ALS with Projected Gradient Descent</a></h4><pre><code class="language-julia">import NMF

 # initialize
W, H = NMF.nndsvdar(X, 5)

 # optimize 
NMF.solve!(NMF.ALSPGrad(maxiter=50, tolg=1.0e-6), X, W, H)</code></pre><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
