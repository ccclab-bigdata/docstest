<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Intrinsics · CUDAnative.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link href="../../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>CUDAnative.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../../">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="../../../man/usage/">Usage</a></li><li><a class="toctext" href="../../../man/troubleshooting/">Troubleshooting</a></li><li><a class="toctext" href="../../../man/performance/">Performance</a></li><li><a class="toctext" href="../../../man/hacking/">Hacking</a></li></ul></li><li><span class="toctext">Library</span><ul><li><a class="toctext" href="../../compilation/">Compilation &amp; Execution</a></li><li><a class="toctext" href="../../reflection/">Reflection</a></li><li><span class="toctext">Device Code</span><ul><li class="current"><a class="toctext" href>Intrinsics</a><ul class="internal"><li><a class="toctext" href="#Indexing-and-Dimensions-1">Indexing and Dimensions</a></li><li><a class="toctext" href="#Memory-Types-1">Memory Types</a></li><li><a class="toctext" href="#Synchronization-1">Synchronization</a></li><li><a class="toctext" href="#Warp-Vote-1">Warp Vote</a></li><li><a class="toctext" href="#Warp-Shuffle-1">Warp Shuffle</a></li><li><a class="toctext" href="#Formatted-Output-1">Formatted Output</a></li><li><a class="toctext" href="#Assertions-1">Assertions</a></li></ul></li><li><a class="toctext" href="../array/">Arrays</a></li><li><a class="toctext" href="../libdevice/">libdevice</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Library</li><li>Device Code</li><li><a href>Intrinsics</a></li></ul></nav><hr/><div id="topbar"><span>Intrinsics</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Intrinsics-1" href="#Intrinsics-1">Intrinsics</a></h1><p>This section lists the package&#39;s public functionality that corresponds to special CUDA functions to be used in device code. It is loosely organized according to the <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#c-language-extensions">C language extensions</a> appendix from the CUDA C programming guide. For more information about certain intrinsics, refer to the aforementioned NVIDIA documentation.</p><h2><a class="nav-anchor" id="Indexing-and-Dimensions-1" href="#Indexing-and-Dimensions-1">Indexing and Dimensions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.gridDim" href="#CUDAnative.gridDim"><code>CUDAnative.gridDim</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">gridDim()::CuDim3</code></pre><p>Returns the dimensions of the grid.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.blockIdx" href="#CUDAnative.blockIdx"><code>CUDAnative.blockIdx</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">blockIdx()::CuDim3</code></pre><p>Returns the block index within the grid.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.blockDim" href="#CUDAnative.blockDim"><code>CUDAnative.blockDim</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">blockDim()::CuDim3</code></pre><p>Returns the dimensions of the block.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.threadIdx" href="#CUDAnative.threadIdx"><code>CUDAnative.threadIdx</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">threadIdx()::CuDim3</code></pre><p>Returns the thread index within the block. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.warpsize" href="#CUDAnative.warpsize"><code>CUDAnative.warpsize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">warpsize()::UInt32</code></pre><p>Returns the warp size (in threads).</p></div></div></section><h2><a class="nav-anchor" id="Memory-Types-1" href="#Memory-Types-1">Memory Types</a></h2><h3><a class="nav-anchor" id="Shared-Memory-1" href="#Shared-Memory-1">Shared Memory</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuStaticSharedMem" href="#CUDAnative.@cuStaticSharedMem"><code>CUDAnative.@cuStaticSharedMem</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@cuStaticSharedMem(T::Type, dims) -&gt; CuDeviceArray{T,AS.Shared}</code></pre><p>Get an array of type <code>T</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a statically-allocated piece of shared memory. The type should be statically inferable and the dimensions should be constant, or an error will be thrown and the generator function will be called dynamically.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuDynamicSharedMem" href="#CUDAnative.@cuDynamicSharedMem"><code>CUDAnative.@cuDynamicSharedMem</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@cuDynamicSharedMem(T::Type, dims, offset::Integer=0) -&gt; CuDeviceArray{T,AS.Shared}</code></pre><p>Get an array of type <code>T</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a dynamically-allocated piece of shared memory. The type should be statically inferable or an error will be thrown and the generator function will be called dynamically.</p><p>Note that the amount of dynamic shared memory needs to specified when launching the kernel.</p><p>Optionally, an offset parameter indicating how many bytes to add to the base shared memory pointer can be specified. This is useful when dealing with a heterogeneous buffer of dynamic shared memory; in the case of a homogeneous multi-part buffer it is preferred to use <code>view</code>.</p></div></div></section><h2><a class="nav-anchor" id="Synchronization-1" href="#Synchronization-1">Synchronization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.sync_threads" href="#CUDAnative.sync_threads"><code>CUDAnative.sync_threads</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">sync_threads()</code></pre><p>Waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <code>sync_threads()</code> are visible to all threads in the block.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.sync_warp" href="#CUDAnative.sync_warp"><code>CUDAnative.sync_warp</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">sync_warp(mask::Integer=0xffffffff)</code></pre><p>Waits threads in the warp, selected by means of the bitmask <code>mask</code>, have reached this point and all global and shared memory accesses made by these threads prior to <code>sync_warp()</code> are visible to those threads in the warp. The default value for <code>mask</code> selects all threads in the warp.</p></div></div></section><h2><a class="nav-anchor" id="Warp-Vote-1" href="#Warp-Vote-1">Warp Vote</a></h2><p>The warp vote functions allow the threads of a given warp to perform a reduction-and-broadcast operation. These functions take as input a boolean predicate from each thread in the warp and evaluate it. The results of that evaluation are combined (reduced) across the active threads of the warp in one different ways, broadcasting a single return value to each participating thread.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_all" href="#CUDAnative.vote_all"><code>CUDAnative.vote_all</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">vote_all(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return non-zero if and only if <code>predicate</code> evaluates to non-zero for all of them.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_any" href="#CUDAnative.vote_any"><code>CUDAnative.vote_any</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">vote_any(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return non-zero if and only if <code>predicate</code> evaluates to non-zero for any of them.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.vote_ballot" href="#CUDAnative.vote_ballot"><code>CUDAnative.vote_ballot</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">vote_ballot(predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return an integer whose Nth bit is set if and only if <code>predicate</code> evaluates to non-zero for the Nth thread of the warp and the Nth thread is active.</p></div></div></section><h2><a class="nav-anchor" id="Warp-Shuffle-1" href="#Warp-Shuffle-1">Warp Shuffle</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl" href="#CUDAnative.shfl"><code>CUDAnative.shfl</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl(val, lane::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a directly indexed lane <code>lane</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_up" href="#CUDAnative.shfl_up"><code>CUDAnative.shfl_up</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_up(val, delta::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with lower ID relative to caller.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_down" href="#CUDAnative.shfl_down"><code>CUDAnative.shfl_down</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_down(val, delta::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with higher ID relative to caller.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_xor" href="#CUDAnative.shfl_xor"><code>CUDAnative.shfl_xor</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_xor(val, mask::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane based on bitwise XOR of own lane ID with <code>mask</code>.</p></div></div></section><p>If using CUDA 9.0, and PTX ISA 6.0 is supported, synchronizing versions of these intrinsics are available as well:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_sync" href="#CUDAnative.shfl_sync"><code>CUDAnative.shfl_sync</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_sync(val, lane::Integer, width::Integer=32, threadmask::UInt32=0xffffffff)</code></pre><p>Shuffle a value from a directly indexed lane <code>lane</code>. The default value for <code>threadmask</code> performs the shuffle on all threads in the warp.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_up_sync" href="#CUDAnative.shfl_up_sync"><code>CUDAnative.shfl_up_sync</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_up_sync(val, delta::Integer, width::Integer=32, threadmask::UInt32=0xffffffff)</code></pre><p>Shuffle a value from a lane with lower ID relative to caller. The default value for <code>threadmask</code> performs the shuffle on all threads in the warp.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_down_sync" href="#CUDAnative.shfl_down_sync"><code>CUDAnative.shfl_down_sync</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_down_sync(val, delta::Integer, width::Integer=32, threadmask::UInt32=0xffffffff)</code></pre><p>Shuffle a value from a lane with higher ID relative to caller. The default value for <code>threadmask</code> performs the shuffle on all threads in the warp.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.shfl_xor_sync" href="#CUDAnative.shfl_xor_sync"><code>CUDAnative.shfl_xor_sync</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">shfl_xor_sync(val, mask::Integer, width::Integer=32, threadmask::UInt32=0xffffffff)</code></pre><p>Shuffle a value from a lane based on bitwise XOR of own lane ID with <code>mask</code>. The default value for <code>threadmask</code> performs the shuffle on all threads in the warp.</p></div></div></section><h2><a class="nav-anchor" id="Formatted-Output-1" href="#Formatted-Output-1">Formatted Output</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuprintf" href="#CUDAnative.@cuprintf"><code>CUDAnative.@cuprintf</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><p>Print a formatted string in device context on the host standard output:</p><pre><code class="language-none">@cuprintf(&quot;%Fmt&quot;, args...)</code></pre><p>Note that this is not a fully C-compliant <code>printf</code> implementation; see the CUDA documentation for supported options and inputs.</p><p>Also beware that it is an untyped, and unforgiving <code>printf</code> implementation. Type widths need to match, eg. printing a 64-bit Julia integer requires the <code>%ld</code> formatting string.</p></div></div></section><h2><a class="nav-anchor" id="Assertions-1" href="#Assertions-1">Assertions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@cuassert" href="#CUDAnative.@cuassert"><code>CUDAnative.@cuassert</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@assert cond [text]</code></pre><p>Signal assertion failure to the CUDA driver if <code>cond</code> is <code>false</code>. Preferred syntax for writing assertions, mimicking <code>Base.@assert</code>. Message <code>text</code> is optionally displayed upon assertion failure.</p><div class="admonition warning"><div class="admonition-title">Warning</div><div class="admonition-text"><p>A failed assertion will crash the GPU, so use sparingly as a debugging tool. Furthermore, the assertion might be disabled at various optimization levels, and thus should not cause any side-effects.</p></div></div></div></div></section><footer><hr/><a class="previous" href="../../reflection/"><span class="direction">Previous</span><span class="title">Reflection</span></a><a class="next" href="../array/"><span class="direction">Next</span><span class="title">Arrays</span></a></footer></article></body></html>
