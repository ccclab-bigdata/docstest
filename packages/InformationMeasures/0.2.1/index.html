<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· InformationMeasures.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>InformationMeasures.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Performance-1">Performance</a></li><li><a class="toctext" href="#Basic-usage-1">Basic usage</a></li><li><a class="toctext" href="#Config-options-1">Config options</a></li><li><a class="toctext" href="#Discretization-1">Discretization</a></li><li><a class="toctext" href="#Advanced-usage-1">Advanced usage</a></li><li><a class="toctext" href="#Contributing-1">Contributing</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="InformationMeasures-1" href="#InformationMeasures-1">InformationMeasures</a></h1><p><em>Release version:</em></p><p><a href="http://pkg.julialang.org/?pkg=InformationMeasures"><img src="http://pkg.julialang.org/badges/InformationMeasures_0.4.svg" alt="InformationMeasures"/></a> <a href="http://pkg.julialang.org/?pkg=InformationMeasures"><img src="http://pkg.julialang.org/badges/InformationMeasures_0.5.svg" alt="InformationMeasures"/></a> <a href="http://pkg.julialang.org/?pkg=InformationMeasures"><img src="http://pkg.julialang.org/badges/InformationMeasures_0.6.svg" alt="InformationMeasures"/></a></p><p><em>Development version:</em></p><p><a href="https://travis-ci.org/Tchanders/InformationMeasures.jl"><img src="https://travis-ci.org/Tchanders/InformationMeasures.jl.svg?branch=master" alt="Build Status"/></a> <a href="http://codecov.io/github/Tchanders/InformationMeasures.jl?branch=master"><img src="http://codecov.io/github/Tchanders/InformationMeasures.jl/coverage.svg?branch=master" alt="codecov.io"/></a></p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p><code>Pkg.add(&quot;InformationMeasures&quot;)</code></p><h2><a class="nav-anchor" id="Performance-1" href="#Performance-1">Performance</a></h2><p>In cases where optimal performance is needed, the latest version of InformationMeasures is recommended, with Julia 0.6. See also <a href="#advanced-usage">Advanced usage</a>.</p><h2><a class="nav-anchor" id="Basic-usage-1" href="#Basic-usage-1">Basic usage</a></h2><p>Currently information measures on three or fewer variables are supported. The basic use case is to pass data arrays for each variable into each function. These will be discretized.</p><p>It is also possible to pass in frequencies (if the data has already been discretized), or probabilities (if the probabilities are already known or have already been estimated) - see below.</p><pre><code class="language-none">using InformationMeasures

data_1 = rand(100)
data_2 = rand(100)
data_3 = rand(100)

# Entropy
ent_1 = get_entropy(data_1)
ent_12 = get_entropy(data_1, data_2)
ent_123 = get_entropy(data_1, data_2, data_3)

# Conditional entropy
ce_1_on_2 = get_conditional_entropy(data_1, data_2)

# Mutual information
mi_12 = get_mutual_information(data_1, data_2)

# Conditional mutual information
cmi_12_on_3 = get_conditional_mutual_information(data_1, data_2, data_3)

# Interaction information
ii_123 = get_interaction_information(data_1, data_2, data_3)

# Total correlation
tc_123 = get_total_correlation(data_1, data_2, data_3)

# Partial information decomposition
pid_123 = get_partial_information_decomposition(data_1, data_2, data_3)</code></pre><h2><a class="nav-anchor" id="Config-options-1" href="#Config-options-1">Config options</a></h2><p>The following keyword arguments can be passed in to each function:</p><p><strong>estimator</strong> (String) Estimator for estimating the probability distribution</p><ul><li><code>&quot;maximum_likelihood&quot;</code> (default)</li><li><code>&quot;miller_madow&quot;</code></li><li><code>&quot;dirichlet&quot;</code></li><li><code>&quot;shrinkage&quot;</code></li></ul><p><strong>base</strong> (Number) Base of the logarithm, i.e. the units for entropy</p><ul><li><code>2</code> (default)</li></ul><p><strong>mode</strong> (String) Method for discretizing</p><ul><li><code>&quot;uniform_width&quot;</code> (default)</li><li><code>&quot;uniform_count&quot;</code></li><li><code>&quot;bayesian_blocks&quot;</code></li></ul><p><strong>number<em>of</em>bins</strong> (Integer)</p><ul><li><code>0</code> (default)</li></ul><p><strong>get<em>number</em>of_bins</strong> (Function) Customized function for calculating the number of bins (will only be used if <code>number_of_bins</code> is <code>0</code>)</p><ul><li><code>get_root_n</code> (default)</li></ul><h4><a class="nav-anchor" id="Estimator-specific-config-options-1" href="#Estimator-specific-config-options-1">Estimator-specific config options</a></h4><p><strong>lambda</strong> (Void or Number) Shrinkage intensity (if left as <code>nothing</code>, will be calculated automatically)</p><ul><li><code>nothing</code> (default)</li></ul><p><strong>prior</strong> (Number) Dirichlet prior (if left as <code>0</code>, Dirichlet estimator is equivalent to maximum likelihood)</p><ul><li><code>0</code> (default)</li></ul><h4><a class="nav-anchor" id="Values,-frequencies,-or-probabilities-1" href="#Values,-frequencies,-or-probabilities-1">Values, frequencies, or probabilities</a></h4><p>The information measures can be calculated from raw data values, frequencies (if the data has already been discretized), or probabilities (if the probabilities are already known or have already been estimated).</p><p>To calculate entropy from frequencies, call <code>get_entropy</code> with the keyword argument <code>discretized = true</code></p><p>For all other information measures, simply pass in a single array of frequencies or probabilities (2D for conditional entropy and mutual information or 3D for conditional mutual information, mutual information and total correlation). If they are probabilities, include the keyword argument <code>probabilities = true</code>, otherwise they will be treated as frequencies.</p><h2><a class="nav-anchor" id="Discretization-1" href="#Discretization-1">Discretization</a></h2><p>Although discretization is taken care of when the information measures are calculated, it is possible to discretize raw values directly, for example to investigate how different discretization algorithms and bin numbers affect the discretization.</p><pre><code class="language-none">data = rand(100)
disc_val = discretize_values(data)</code></pre><p>NB <code>discretize_values</code> returns the frequencies for each bin in order, rather than the discretized values. An example of how to get the discretized values is discussed below.</p><h2><a class="nav-anchor" id="Advanced-usage-1" href="#Advanced-usage-1">Advanced usage</a></h2><p>Functions such as <code>get_entropy</code> and <code>get_mutual_information</code> are designed to be flexible and easy to use with different types of input and config options. In some cases it may be quicker to bypass these functions.</p><h3><a class="nav-anchor" id="Example-1" href="#Example-1">Example</a></h3><p>When calculating the mutual information between every pair of data vectors from a large dataset, simply calling <code>get_mutual_information</code> on each pair of vectors will result in each vector being discretized multiple times.</p><p>Currently, discretization for multiple variables works by discretizing the marginals independently, then reconstructing the higher dimensional frequencies from these discretized marginals. Therefore discretizing each variable once in advance will not affect the results, but will be much quicker. Joint frequencies cannot be reconstructed from the bin frequencies; instead the discretized values should be stored. <code>get_bin_ids!</code> should therefore be used, instead of <code>discretize_values</code>:</p><pre><code class="language-none">data_1 = rand(100)
data_2 = rand(100)
data_3 = rand(100)

number_of_bins = 10
mi_base = 2

bin_ids_1 = zeros(Int, length(data_1))
get_bin_ids!(data_1, &quot;uniform_width&quot;, number_of_bins, bin_ids_1)

bin_ids_2 = zeros(Int, length(data_2))
get_bin_ids!(data_2, &quot;uniform_width&quot;, number_of_bins, bin_ids_2)

bin_ids_3 = zeros(Int, length(data_3))
get_bin_ids!(data_3, &quot;uniform_width&quot;, number_of_bins, bin_ids_3)

f_12 = get_frequencies_from_bin_ids(bin_ids_1, bin_ids_2, number_of_bins, number_of_bins)
p_12 = get_probabilities(&quot;maximum_likelihood&quot;, f_12)
mi_12 = apply_mutual_information_formula(p_12, sum(p_12, 2), sum(p_12, 1), mi_base)

f_13 = get_frequencies_from_bin_ids(bin_ids_1, bin_ids_3, number_of_bins, number_of_bins)
p_13 = get_probabilities(&quot;maximum_likelihood&quot;, f_13)
mi_13 = apply_mutual_information_formula(p_13, sum(p_13, 2), sum(p_13, 1), mi_base)

# And so on...</code></pre><p>Note that the probability distribution is estimated from the joint frequencies rather than the marginals, meaning that, for most estimators, <code>sum(p_12, 2)</code> may be different from <code>sum(p_13, 2)</code>, even though both represent the estimated probability distribution for <code>data_1</code>. (This is not the case for the &quot;maximum likelihood&quot; estimator, which just divides the bin frequencies by the total frequency. For this estimator, the marginal probabilities could be stored in advance to avoid calculating them as they are passed into <code>apply_entropy_formula</code>. The best performance in that case may depend on the cost of storage vs calculations.)</p><p>Here are two full examples of the &quot;quick&quot; vs the &quot;easy&quot; way to estimate the mutual information between all pairs of a set of variables.</p><pre><code class="language-none">data = rand(100, 100)

function mi_quick(data; discretizer = &quot;uniform_width&quot;, estimator = &quot;maximum_likelihood&quot;, mi_base = 2)

	nvals, nvars = size(data)

	bin_ids = zeros(Int, (nvals, nvars))
	nbins = Int(round(sqrt(nvals)))
	mis = zeros(binomial(nvars, 2))

	for i in 1 : nvars
		get_bin_ids!(data[1:nvals, i:i], discretizer, nbins, view(bin_ids, 1:nvals, i:i))
	end

	index = 1
	for i in 1 : nvars, j in i+1 : nvars
		f = get_frequencies_from_bin_ids(bin_ids[1:end, i:i], bin_ids[1:end, j:j], nbins, nbins)
		p = get_probabilities(estimator, f)
		mis[index] = apply_mutual_information_formula(p, sum(p, 1), sum(p, 2), mi_base)
		index += 1
	end

	return mis

end

function mi_easy(data; discretizer = &quot;uniform_width&quot;, estimator = &quot;maximum_likelihood&quot;, mi_base = 2)
	nvals, nvars = size(data)
	mis = zeros(binomial(nvars, 2))

	index = 1
	for i in 1 : nvars, j in i+1 : nvars
		mis[index] = get_mutual_information(data[1:nvals, i:i], data[1:nvals, j:j]; mode = discretizer, estimator = estimator, base = mi_base)
		index += 1
	end

	return mis
end</code></pre><h2><a class="nav-anchor" id="Contributing-1" href="#Contributing-1">Contributing</a></h2><p>Contributions and bug reports are welcome!</p><footer><hr/></footer></article></body></html>
