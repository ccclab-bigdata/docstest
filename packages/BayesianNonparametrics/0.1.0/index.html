<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · BayesianNonparametrics.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>BayesianNonparametrics.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Documentation-1">Documentation</a></li><li><a class="toctext" href="#Example-1">Example</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="BayesianNonparametrics.jl-1" href="#BayesianNonparametrics.jl-1">BayesianNonparametrics.jl</a></h1><p><a href="https://travis-ci.org/OFAI/BayesianNonparametrics.jl"><img src="https://travis-ci.org/OFAI/BayesianNonparametrics.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/OFAI/BayesianNonparametrics.jl?branch=master"><img src="https://coveralls.io/repos/github/OFAI/BayesianNonparametrics.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><p><em>BayesianNonparametrics</em> is a Julia package implementing state-of-the-art Bayesian nonparametric models for medium-sized unsupervised problems. The software package brings Bayesian nonparametrics to non-specialists allowing the widespread use of Bayesian nonparametric models. Emphasis is put on consistency, performance and ease of use allowing easy access to Bayesian nonparametric models inside Julia.</p><p><em>BayesianNonparametrics</em> allows you to</p><ul><li>explain discrete or continous data using: Dirichlet Process Mixtures or Hierarchical Dirichlet Process Mixtures</li><li>analyse variable dependencies using: Variable Clustering Model</li><li>fit multivariate or univariate distributions for discrete or continous data with conjugate priors</li><li>compute point estimtates of Dirichlet Process Mixtures posterior samples</li></ul><h4><a class="nav-anchor" id="News-1" href="#News-1">News</a></h4><p><em>BayesianNonparametrics</em> is Julia 0.7  / 1.0 compatible</p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>You can install the package into your running Julia installation using Julia&#39;s package manager, i.e.</p><pre><code class="language-julia">pkg&gt; add BayesianNonparametrics</code></pre><h2><a class="nav-anchor" id="Documentation-1" href="#Documentation-1">Documentation</a></h2><p>Documentation is available in Markdown: <a href="docs/README.md">documentation</a></p><h2><a class="nav-anchor" id="Example-1" href="#Example-1">Example</a></h2><p>The following example illustrates the use of <em>BayesianNonparametrics</em> for clustering of continuous observations using a Dirichlet Process Mixture of Gaussians. </p><p>After loading the package:</p><pre><code class="language-julia">using BayesianNonparametrics</code></pre><p>we can generate a 2D synthetic dataset (or use a multivariate continuous dataset of interest)</p><pre><code class="language-julia">(X, Y) = bloobs(randomize = false)</code></pre><p>and construct the parameters of our base distribution:</p><pre><code class="language-julia">μ0 = vec(mean(X, dims = 1))
κ0 = 5.0
ν0 = 9.0
Σ0 = cov(X)
H = WishartGaussian(μ0, κ0, ν0, Σ0)</code></pre><p>After defining the base distribution we can specify the model:</p><pre><code class="language-julia">model = DPM(H)</code></pre><p>which is in this case a Dirichlet Process Mixture. Each model has to be initialised, one possible initialisation approach for Dirichlet Process Mixtures is a k-Means initialisation:</p><pre><code class="language-julia">modelBuffer = init(X, model, KMeansInitialisation(k = 10))</code></pre><p>The resulting buffer object can now be used to apply posterior inference on the model given <code>X</code>. In the following we apply Gibbs sampling for 500 iterations without burn in or thining:</p><pre><code class="language-julia">models = train(modelBuffer, DPMHyperparam(), Gibbs(maxiter = 500))</code></pre><p>You shoud see the progress of the sampling process in the command line. After applying Gibbs sampling, it is possible explore the posterior based on their posterior densities,</p><pre><code class="language-julia">densities = map(m -&gt; m.energy, models)</code></pre><p>number of active components</p><pre><code class="language-julia">activeComponents = map(m -&gt; sum(m.weights .&gt; 0), models)</code></pre><p>or the groupings of the observations:</p><pre><code class="language-julia">assignments = map(m -&gt; m.assignments, models)</code></pre><p>The following animation illustrates posterior samples obtained by a Dirichlet Process Mixture: </p><p><img src="posteriorSamples.gif &quot;Posterior Sample&quot;" alt="alt text"/></p><p>Alternatively, one can compute a point estimate based on the posterior similarity matrix:</p><pre><code class="language-julia">A = reduce(hcat, assignments)
(N, D) = size(X)
PSM = ones(N, N)
M = size(A, 2)
for i in 1:N
  for j in 1:i-1
    PSM[i, j] = sum(A[i,:] .== A[j,:]) / M
    PSM[j, i] = PSM[i, j]
  end
end</code></pre><p>and find the optimal partition which minimizes the lower bound of the variation of information:</p><pre><code class="language-julia">mink = minimum(length(m.weights) for m in models)
maxk = maximum(length(m.weights) for m in models)
(peassignments, _) = pointestimate(PSM, method = :average, mink = mink, maxk = maxk)</code></pre><p>The grouping wich minimizes the lower bound of the variation of information is illustrated in the following image: <img src="pointestimate.png &quot;Point Estimate&quot;" alt="alt text"/></p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
