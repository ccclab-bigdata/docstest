<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · Hyperopt.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Hyperopt.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#Usage-1">Usage</a></li><li class="toplevel"><a class="toctext" href="#Categorical-variables-1">Categorical variables</a></li><li class="toplevel"><a class="toctext" href="#Which-sampler-to-use?-1">Which sampler to use?</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Hyperopt-1" href="#Hyperopt-1">Hyperopt</a></h1><p><a href="https://travis-ci.org/baggepinnen/Hyperopt.jl"><img src="https://travis-ci.org/baggepinnen/Hyperopt.jl.svg?branch=master" alt="Build Status"/></a></p><p><a href="https://coveralls.io/github/baggepinnen/Hyperopt.jl?branch=master"><img src="https://coveralls.io/repos/baggepinnen/Hyperopt.jl/badge.svg?branch=master&amp;service=github" alt="Coverage Status"/></a></p><p><a href="http://codecov.io/github/baggepinnen/Hyperopt.jl?branch=master"><img src="http://codecov.io/github/baggepinnen/Hyperopt.jl/coverage.svg?branch=master" alt="codecov.io"/></a></p><p>A package to perform hyperparameter optimization. Currently supports random search, blue noise search, decision tree and random forest samplers, but currently only random and blue noise performs satisfactorily.</p><h1><a class="nav-anchor" id="Usage-1" href="#Usage-1">Usage</a></h1><pre><code class="language-julia">using Hyperopt

f(x,a,b=true;c=10) = sum(@. x + (a-3)^2 + (b ? 10 : 20) + (c-100)^2) # Function to minimize

# Main macro. The first argument to the for loop is always interpreted as the number of iterations
julia&gt; ho = @hyperopt for i=50, sampler = TreeSampler(random_init=5,samples_per_leaf=3,n_tries=20), a = linspace(1,5,1000), b = [true, false], c = logspace(-1,3,1000)
           print(i, &quot;\t&quot;, a, &quot;\t&quot;, b, &quot;\t&quot;, c, &quot;   \t&quot;)
           x = 100
           @show f(x,a,b,c=c)
       end
       1   3.910910910910911   false   0.15282140360258697     f(x, a, b, c=c) = 10090.288832348499
       2   3.930930930930931   true    6.1629662551329405      f(x, a, b, c=c) = 8916.255534433481
       3   2.7617617617617616  true    146.94918006248173      f(x, a, b, c=c) = 2314.282265997491
       4   3.6666666666666665  false   0.3165924111983522      f(x, a, b, c=c) = 10057.226192959602
       5   4.783783783783784   true    34.55719936762139       f(x, a, b, c=c) = 4395.942039196544
       6   2.5895895895895897  true    4.985373463873895       f(x, a, b, c=c) = 9137.947692504491
       7   1.6206206206206206  false   301.6334347259197       f(x, a, b, c=c) = 40777.94468684398
       8   1.012012012012012   true    33.00034791125285       f(x, a, b, c=c) = 4602.905476253546
       9   3.3583583583583585  true    193.7703337477989       f(x, a, b, c=c) = 8903.003911886599
       10  4.903903903903904   true    144.26439512181574      f(x, a, b, c=c) = 2072.9615255755252
       11  2.2332332332332334  false   119.97177354358843      f(x, a, b, c=c) = 519.4596697509966
       12  2.369369369369369   false   117.77987011971193      f(x, a, b, c=c) = 436.52147646611473
       13  3.2182182182182184  false   105.44427935261685      f(x, a, b, c=c) = 149.68779686009242
⋮

Hyperopt.Hyperoptimizer
  iterations: Int64 50
  params: Tuple{Symbol,Symbol,Symbol}
  candidates: Array{AbstractArray{T,1} where T}((3,))
  history: Array{Any}((50,))
  results: Array{Any}((50,))
  sampler: Hyperopt.TreeSampler


julia&gt; best_params, min_f = minimum(ho)
(Real[1.62062, true, 100.694], 112.38413353985818)

julia&gt; printmin(ho)
a = 1.62062
b = true
c = 100.694</code></pre><p>The macro <code>@hyperopt</code> takes a for-loop with an initial argument determining the number of samples to draw (<code>i</code> above) The sampel strategy can be specified by specifying the special keyword <code>sampler = Sampler(opts...)</code>. Available options are <code>RandomSampler</code>, <code>BlueNoiseSampler</code>, <code>TreeSampler</code> and <code>ForestSampler</code>. The subsequent arguments to the for-loop specifies names and candidate values for different hyper parameters (<code>a = linspace(1,2,1000), b = [true, false], c = logspace(-1,3,1000)</code> above). Currently uniform random sampling from the candidate values is the only supported optimizer. Log-uniform sampling is achieved with uniform sampling of a logarithmically spaced vector, e.g. <code>c = logspace(-1,3,1000)</code>. The parameters <code>i,a,b,c</code> can be used within the expression sent to the macro and they will hold a new value sampled from the corresponding candidate vector each iteration.</p><p>The resulting object <code>ho::Hyperoptimizer</code> holds all the sampled parameters and function values and can be queried for <code>minimum/maximum</code>, which returns the best parameters and function value found. It can also be plotted using <code>plot(ho)</code> (uses <code>Plots.jl</code>).</p><p>The type <code>Hyperoptimizer</code> is iterable, it iterates for the specified number of iterations, each iteration providing a sample of the parameter vector, e.g.</p><pre><code class="language-julia">ho = Hyperoptimizer(10, a = linspace(1,2), b = [true, false], c = randn(100))
for (i,a,b,c) in ho
    println(i, &quot;\t&quot;, a, &quot;\t&quot;, b, &quot;\t&quot;, c)
end

1   1.2244897959183674  false   0.8179751164732062
2   1.7142857142857142  true    0.6536272580487854
3   1.4285714285714286  true    -0.2737451706680355
4   1.6734693877551021  false   -0.12313108128547606
5   1.9795918367346939  false   -0.4350837079334295
6   1.0612244897959184  true    -0.2025613848798039
7   1.469387755102041   false   0.7464858339748051
8   1.8571428571428572  true    -0.9269021128132274
9   1.163265306122449   true    2.6554272337516966
10  1.4081632653061225  true    1.112896676939024</code></pre><p>If uesd in this way, the hyperoptimizer <strong>can not</strong> keep track of the function values like it did when <code>@hyperopt</code> was used.</p><h1><a class="nav-anchor" id="Categorical-variables-1" href="#Categorical-variables-1">Categorical variables</a></h1><p>Currently, only <code>RandomSampler</code> and <code>BlueNoiseSampler</code> support categorical variables which do not have a natural floating point representation, such as functions:</p><pre><code class="language-julia">@hyperopt for i=20, fun = [tanh, σ, relu]
    train_network(fun)
end</code></pre><p>Caveat for <code>BlueNoiseSampler</code>: see below.</p><h1><a class="nav-anchor" id="Which-sampler-to-use?-1" href="#Which-sampler-to-use?-1">Which sampler to use?</a></h1><p>Random is a good baseline and the default if none is chosen.</p><p>If number of iterations is smaller than, say, 200, <code>BlueNoiseSampler</code> works better than random. Caveat: <code>BlueNoiseSampler</code> needs all candidate vectors to be of equal length, i.e.,</p><pre><code class="language-julia">hob = @hyperopt for i=100, sampler=BlueNoiseSampler(), a = range(1,stop=5, length=100), b = repeat([true, false],50), c = exp10.(range(-1,stop=3, length=100))
    # println(i, &quot;\t&quot;, a, &quot;\t&quot;, b, &quot;\t&quot;, c)
    # print(i, &quot; &quot;)
    f(a,b,c=c)
end</code></pre><p>where all candidate vectors are of length 100. The candidates for <code>b</code> thus had to be repeated 50 times.</p><p><code>BlueNoiseSampler</code> performs an optimization problem in the beginning to spread out samples so as to sample the space as evenly as possible, both as measured in the full dimensional space, and in each dimension separately. Inspiration for this sampler comes from <a href="http://resources.mpi-inf.mpg.de/ProjectiveBlueNoise/ProjectiveBlueNoise.pdf">Projective Blue-Noise Sampling</a> but the implemented algorithm is not the same. The result of the blue noise optimization in 2 dimensions is visualized below. Initial values to the left and optimized to the right. <img src="bluenoise.png" alt="window"/></p><p>If the number of iterations is very large, the optimization problem might take long time to run in comparison to the runtime of a single experiment and random sampling will end up more effective.</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
