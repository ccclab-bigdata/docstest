<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · DifferentialDynamicProgramming.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DifferentialDynamicProgramming.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">DifferentialDynamicProgramming.@end_backward_pass</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.@setupQTIC</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.ADAMOptimizer</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.DEBUG</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.DifferentialDynamicProgramming</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.EmptyMat2</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.EmptyMat3</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.GaussianPolicy" href="#DifferentialDynamicProgramming.GaussianPolicy"><code>DifferentialDynamicProgramming.GaussianPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">`GaussianPolicy{P}`</code></pre><p><strong>Fileds:</strong></p><pre><code class="language-none">T::Int          # number of time steps
n::Int          # State dimension
m::Int          # Number of control inputs
K::Array{P,3}   # Time-varying feedback gain ∈ R(n,m,T)
k::Array{P,2}   # Open loop control signal  ∈ R(m,T)
Σ::Array{P,3}   # Time-varying controller covariance  ∈ R(m,m,T)
Σi::Array{P,3}  # The inverses of Σ</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.KLmv" href="#DifferentialDynamicProgramming.KLmv"><code>DifferentialDynamicProgramming.KLmv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">This is the inverse of Σₓᵤ</code></pre></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.QPTrace</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.Trace</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.__init__</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.back_pass</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.back_pass_gps</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.boxQP" href="#DifferentialDynamicProgramming.boxQP"><code>DifferentialDynamicProgramming.boxQP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Minimize <code>0.5*x&#39;*H*x + x&#39;*g</code>  s.t. lower&lt;=x&lt;=upper</p><p>inputs:      <code>H</code>            - positive definite matrix   (n * n)      <code>g</code>            - bias vector                (n)      <code>lower</code>        - lower bounds               (n)      <code>upper</code>        - upper bounds               (n)</p><p>optional inputs:      <code>x0</code>           - initial state              (n)      <code>options</code>      - see below                  (7)</p><p>outputs:      <code>x</code>            - solution                   (n)      <code>result</code>       - result type (roughly, higher is better, see below)      <code>Hfree</code>        - subspace cholesky factor   (n<em>free * n</em>free)      <code>free</code>         - set of free dimensions     (n)</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.calc_η" href="#DifferentialDynamicProgramming.calc_η"><code>DifferentialDynamicProgramming.calc_η</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>new<em>η, satisfied, divergence = calc</em>η(xnew,xold,sigmanew,η, traj<em>new, traj</em>prev, kl_step) This Function caluculates the step size</p></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.care</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.choleskyvectens</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.debug</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.demoQP</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.demo_linear</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.demo_linear_kl</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.demo_pendcart" href="#DifferentialDynamicProgramming.demo_pendcart"><code>DifferentialDynamicProgramming.demo_pendcart</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">demo_pendcart(;kwargs...)</code></pre><p>Run the iLQG Function to find an optimal trajectory For the &quot;pendulum on a cart system&quot;. Requires package ControlSystems.jl</p><p><strong>Arguments</strong></p><p><code>x0     = [π-0.6,0,0,0]</code> <code>goal   = [π,0,0,0]</code> <code>Q      = Diagonal([10,1,2,1])</code> : State weight matrix <code>R      = 1</code>                    : Control weight matrix <code>lims   = 5.0*[-1 1]</code>           : control limits, <code>T      = 600</code>                  : Number of time steps <code>doplot = true</code>                 : Plot results</p></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.dir</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.emptyMat2</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.emptyMat3</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.entropy</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.eval</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.eye</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.forward_covariance</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.forward_pass" href="#DifferentialDynamicProgramming.forward_pass"><code>DifferentialDynamicProgramming.forward_pass</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">xnew,unew,cnew,sigmanew = forward_pass(traj_new, x0,u,x,α,f,costfun,lims,diff)</code></pre><p><strong>Arguments</strong></p><ul><li>α: step size (αk is applied to old trajectory)</li><li>diff: function to determine difference <code>diff(xnew[:,i], x[:,i])</code></li><li>f: forward dynamics <code>x(k+1)  = f(x(k), u(k), k)</code></li><li><code>cnew = costfun(xnew, unew)</code></li></ul></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.geom</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.graphics</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.iLQG" href="#DifferentialDynamicProgramming.iLQG"><code>DifferentialDynamicProgramming.iLQG</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>iLQG - solve the deterministic finite-horizon optimal control problem.</p><p>minimize sum_i cost(x[:,i],u[:,i]) + cost(x[:,end]) s.t.  x[:,i+1] = f(x[:,i],u[:,i])</p><p><strong>Inputs</strong></p><p><code>f, costfun, df</code></p><ol><li>step:</li></ol><p><code>xnew = f(x,u,i)</code> is called during the forward pass. Here the state x and control u are vectors: size(x)==(n,), size(u)==(m,). The time index <code>i</code> is a scalar.</p><ol><li>cost:</li></ol><p><code>cost = costfun(x,u)</code> is called in the forward pass to compute the cost per time-step along the trajectory <code>x,u</code>.</p><ol><li>derivatives:</li></ol><p><code>fx,fu,fxx,fxu,fuu,cx,cu,cxx,cxu,cuu = df(x,u)</code> computes the derivatives along a trajectory. In this case size(x)==(n, N) where N is the trajectory length. size(u)==(m, N). The time indexes are I=(1:N). Dimensions match the variable names e.g. size(fxu)==(n, n, m, N) If cost function or system is time invariant, the dimension of the corresponding derivatives can be reduced by dropping the time dimension</p><p><code>x0</code> - The initial state from which to solve the control problem. Should be a column vector. If a pre-rolled trajectory is available then size(x0)==(n, N) can be provided and cost set accordingly.</p><p><code>u0</code> - The initial control sequence. A matrix of size(u0)==(m, N) where m is the dimension of the control and N is the number of state transitions.</p><p><strong>Outputs</strong></p><p><code>x</code> - the optimal state trajectory found by the algorithm. size(x)==(n, N)</p><p><code>u</code> - the optimal open-loop control sequence. size(u)==(m, N)</p><p><code>traj_new</code> - A new <code>GaussianPolicy</code> object containing feedforward control trajectory and feedback-gains, these gains multiply the deviation of a simulated trajectory from the nominal trajectory x. See <code>?GaussianPolicy</code> for more help.</p><p><code>Vx</code> - the gradient of the cost-to-go. size(Vx)==(n, N)</p><p><code>Vxx</code> - the Hessian of the cost-to-go. size(Vxx)==(n, n N)</p><p><code>cost</code> - the costs along the trajectory. size(cost)==(1, N) the cost-to-go is V = fliplr(cumsum(fliplr(cost)))</p><p><code>trace</code> - a trace of various convergence-related values. One row for each iteration, the columns of trace are <code>[iter λ α g_norm Δcost z sum(cost) dλ]</code> see below for details.</p><p><strong>Keyword arguments</strong></p><p><code>lims</code>,           [],            control limits</p><p><code>α</code>,              logspace(0,-3,11), backtracking coefficients</p><p><code>tol_fun</code>,         1e-7,          reduction exit criterion</p><p><code>tol_grad</code>,        1e-4,          gradient exit criterion</p><p><code>max_iter</code>,        500,           maximum iterations</p><p><code>λ</code>,         1,             initial value for λ</p><p><code>dλ</code>,        1,             initial value for dλ</p><p><code>λfactor</code>,   1.6,           λ scaling factor</p><p><code>λmax</code>,      1e10,          λ maximum value</p><p><code>λmin</code>,      1e-6,          below this value λ = 0</p><p><code>regType</code>,        1,             regularization type 1: q<em>uu+λ*I 2: V</em>xx+λ*I</p><p><code>reduce_ratio_min</code>,           0,             minimal accepted reduction ratio</p><p><code>diff_fun</code>,         -,             user-defined diff for sub-space optimization</p><p><code>plot</code>,           1,             0: no  k&gt;0: every k iters k&lt;0: every k iters, with derivs window</p><p><code>verbosity</code>,      2,             0: no  1: final 2: iter 3: iter, detailed</p><p><code>plot_fun</code>,         x-&gt;0,          user-defined graphics callback</p><p><code>cost</code>,           [],            initial cost for pre-rolled trajectory</p><p>This code consists of a port and extension of a MATLAB library provided by the autors of <code>INPROCEEDINGS{author={Tassa, Y. and Mansard, N. and Todorov, E.}, booktitle={Robotics and Automation (ICRA), 2014 IEEE International Conference on}, title={Control-Limited Differential Dynamic Programming}, year={2014}, month={May}, doi={10.1109/ICRA.2014.6907001}}</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.iLQGkl" href="#DifferentialDynamicProgramming.iLQGkl"><code>DifferentialDynamicProgramming.iLQGkl</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">`x, u, traj_new, Vx, Vxx, cost, trace = iLQGkl(dynamics,costfun,derivs, x0, traj_prev, model;
    constrain_per_step = false,
    kl_step            = 0,
    lims               = [],                    # Control signal limits ::Matrix ∈ R(m,2)
    tol_fun            = 1e-7,
    tol_grad           = 1e-4,
    max_iter           = 50,
    print_head         = 10,                    # Print headers this often
    print_period       = 1,                     # Print this often
    reduce_ratio_min   = 0,                     # Not used ATM
    diff_fun           = -,
    verbosity          = 2,                     # ∈ (0,3)
    plot_fun           = x-&gt;0,                  # Not used
    cost               = [],                    # Supply if pre-rolled trajectory supplied
    ηbracket           = [1e-8,1,1e16],         # dual variable bracket [min_η, η, max_η]
    del0               = 0.0001,                # Start of dual variable increase
    gd_alpha           = 0.01                   # Step size in GD (ADAMOptimizer) when constrain_per_step is true
    )`</code></pre><p>Solves the iLQG problem with constraints on control signals <code>lims</code> and bound on the KL-divergence <code>kl_step</code> from the old trajectory distribution <code>traj_prev::GaussianPolicy</code>.</p><p>To solve the maximum entropy problem, use controller <code>controller(xi,i)  = u[:,i] + K[:,:,i]*(xi-x[:,i]) + chol(Σ)*randn(m)</code> where <code>K</code> comes from <code>traj_new</code>. Note that multiplying the cost by a constant changes the relative weight between the cost term and the entropy term, i.e., higher cost produces less noise through chol(Σ) since (Σ = Qᵤᵤ⁻¹).</p></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.include</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.kl_div" href="#DifferentialDynamicProgramming.kl_div"><code>DifferentialDynamicProgramming.kl_div</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">This function produces lots of negative values which are clipped by the max(0,kl)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.kl_div_wiki" href="#DifferentialDynamicProgramming.kl_div_wiki"><code>DifferentialDynamicProgramming.kl_div_wiki</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>This version seems to be symmetric and positive</p></div></div></section><pre><code class="language-none">DifferentialDynamicProgramming.lqr</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.plotstuff_linear</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.plotstuff_pendcart</code></pre><pre><code class="language-none">DifferentialDynamicProgramming.print_timing</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DifferentialDynamicProgramming.∇kl" href="#DifferentialDynamicProgramming.∇kl"><code>DifferentialDynamicProgramming.∇kl</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Calculate the Q terms related to the KL-constraint. (Actually, only related to log(p̂(τ)) since the constraint is rewritten as Entropy term and other term dissapears into expectation under p(τ).) Qtt is [Qxx Qxu; Qux Quu] Qt is [Qx; Qu] These terms should be added to the Q terms calculated in the backwards pass to produce the final Q terms. This Function should be called from within the backwards_pass Function or just prior to it to adjust the cost derivative matrices.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
