<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· Divergences.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Divergences.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Supported-divergences-1">Supported divergences</a></li><li><a class="toctext" href="#Basic-usage-1">Basic usage</a></li><li><a class="toctext" href="#List-of-divergences-1">List of divergences</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Divergences.jl-1" href="#Divergences.jl-1">Divergences.jl</a></h1><p><a href="https://travis-ci.org/gragusa/Divergences.jl"><img src="https://travis-ci.org/gragusa/Divergences.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://codecov.io/gh/gragusa/Divergences.jl"><img src="https://codecov.io/gh/gragusa/Divergences.jl/branch/master/graph/badge.svg" alt="codecov"/></a></p><p><code>Divergences.jl</code> is a Julia package that makes it easy to evaluate divergence measures between two vectors. The package allows calculating the <em>gradient</em>  and the diagonal of the <em>Hessian</em> of several divergences. These divergences are used to good effect by the  <a href="http://github.com/gragusa/MomentBasedEstimators.jl/git">MomentBasedEstimators</a> package.</p><h2><a class="nav-anchor" id="Supported-divergences-1" href="#Supported-divergences-1">Supported divergences</a></h2><p>The package defines an abstract <code>Divergence</code> type with the following suptypes:</p><ul><li>Kullback-Leibler divergence <code>KullbackLeibler</code></li><li>Chi-square distance <code>ChiSquared</code></li><li>Reverse Kullback-Leibler divergence <code>ReverseKullbackLeibler</code></li><li>Cressie-Read divergences <code>CressieRead</code></li></ul><p>These divergences differ from the equivalent ones defined in the <code>Distances</code> package because they are normalized. Also, the package provides methods for calculating their gradient and the (diagonal elements of the) Hessian matrix.</p><p>The constructors for the types above are straightforward</p><pre><code class="language-julia">KullbackLeibler()
ChiSqaured()
ReverseKullbackLeibler()</code></pre><p>The <code>CressieRead</code> type define a family of divergences indexed by a parameter <code>alpha</code>. The constructor for <code>CressieRead</code> is</p><pre><code class="language-julia">CR(::Real)</code></pre><p>The Hellinger divergence is obtained by <code>CR(-1/2)</code>. For a certain value of <code>alpha</code>, <code>CressieRead</code> correspond to a divergence that has a specific type defined. For instance <code>CR(1)</code> is equivalent to <code>ChiSquared</code> although the underlying code for evaluation and calculation of the gradient and Hessian are different. </p><p>Three versions of each divergence in the above list are implemented currently. A <em>vanilla</em> version, a modified version, and a fully modified version. These modifications extend the domain of the divergence.</p><p>The <strong>modified</strong> version takes an additional argument that specifies the point at which the divergence is modified by a convex extension. </p><pre><code class="language-julia">ModifiedKullbackLeibler(theta::Real)
ModifiedReverseKullbackLeibler(theta::Real)
ModifiedCressieRead(alpha::Real, theta::Real)</code></pre><p>Similarly, the <strong>fully modified</strong> version takes <strong>two</strong> additional arguments that specify the points at which the divergence is modified by a convex extensions.</p><pre><code class="language-julia">FullyModifiedKullbackLeibler(phi::Real, theta::Real)
FullyModifiedReverseKullbackLeibler(phi::Real, theta::Real)
FullyModifiedCressieRead(alpha::Real, phi::Real, theta::Real)</code></pre><h2><a class="nav-anchor" id="Basic-usage-1" href="#Basic-usage-1">Basic usage</a></h2><h3><a class="nav-anchor" id="Divergence-between-two-vectors-1" href="#Divergence-between-two-vectors-1">Divergence between two vectors</a></h3><p>Each divergence corresponds to a <em>divergence type</em>. You can always compute a certain divergence between two vectors using the following syntax</p><pre><code class="language-julia">d = evaluate(div, x, y)</code></pre><p>Here, <code>div</code> is an instance of a divergence type. For example, the type for Kullback Leibler divergence is <span>$KullbackLeibler$</span> (more divergence types are described in some details in what follows), then the Kullback Leibler divergence between <span>$x$</span> and <span>$y$</span> can be computed</p><pre><code class="language-julia">d = evaluate(KullbackLeibler(), x, y)</code></pre><p>We can also calculate the diverge between the vector <span>$x$</span> and the unit vector</p><pre><code class="language-julia">r = evaluate(KullbackLeibler(), x)</code></pre><p>The <code>Divergence</code> type is a subtype of <code>PreMetric</code> defined in the <code>Distances</code> package. As such, the divergences can be evaluated row-wise and column-wise for <code>X::Matrix</code> and <code>Y::Matrix</code>. </p><pre><code class="language-julia">rowise(div, X, Y)</code></pre><pre><code class="language-julia">colwise(div, X, Y)</code></pre><h3><a class="nav-anchor" id="Gradient-of-the-divergence-1" href="#Gradient-of-the-divergence-1">Gradient of the divergence</a></h3><p>To calculate the gradient of  <code>div::Divergence</code> with respect to <span>$x::AbstractArray{Float64, 1}$</span> the <code>gradient</code> method can be used</p><pre><code class="language-julia">g = gradient(div, x, y)</code></pre><p>or through its in-place version</p><pre><code class="language-julia">gradient!(Array(Float64, size(x)), div, x, y)</code></pre><h3><a class="nav-anchor" id="Hessian-of-the-divergence-1" href="#Hessian-of-the-divergence-1">Hessian of the divergence</a></h3><p>The <code>hessian</code> method calculate the Hessian of the divergence with respect to <span>$x$</span> </p><pre><code class="language-julia">h = hessian(div, x, y)</code></pre><p>Its in-place variant is also defined</p><pre><code class="language-julia">hessian!(Array(Float64, size(x)), div, x, y)</code></pre><p>Notice that the Hessian of a divergence is sparse, where the diagonal entries are the only ones different from zero. For this reason, <code>hessian(div, x, y)</code> return an <code>Array{Float64,1}</code> with the diagonal entries of the hessian.</p><h2><a class="nav-anchor" id="List-of-divergences-1" href="#List-of-divergences-1">List of divergences</a></h2><p>[To be added]</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
