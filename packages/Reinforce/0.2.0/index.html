<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · Reinforce.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Reinforce.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Environment-Interface-1">Environment Interface</a></li><li><a class="toctext" href="#Policy-Interface-1">Policy Interface</a></li><li><a class="toctext" href="#Episode-Iterator-1">Episode Iterator</a></li><li><a class="toctext" href="#Author:-[Tom-Breloff](https://github.com/tbreloff)-1">Author: Tom Breloff</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Reinforce-1" href="#Reinforce-1">Reinforce</a></h1><p><a href="https://travis-ci.org/JuliaML/Reinforce.jl"><img src="https://travis-ci.org/JuliaML/Reinforce.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://gitter.im/reinforcejl/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge"><img src="https://badges.gitter.im/reinforcejl/Lobby.svg" alt="Gitter"/></a></p><p>Reinforce.jl is an interface for Reinforcement Learning.  It is intended to connect modular environments, policies, and solvers with a simple interface.</p><p><img src="https://cloud.githubusercontent.com/assets/933338/17670982/8923a2f6-62e2-11e6-943f-bd0a2a7b5c1f.gif" alt/> <img src="https://cloud.githubusercontent.com/assets/933338/17703784/f3e18414-63a0-11e6-9f9e-f531278216f9.gif" alt/></p><hr/><p>Packages which build on Reinforce:</p><ul><li><a href="https://github.com/JuliaML/AtariAlgos.jl">AtariAlgos</a>: Environment which wraps Atari games using <a href="https://github.com/nowozin/ArcadeLearningEnvironment.jl">ArcadeLearningEnvironment</a></li><li><a href="https://github.com/JuliaML/OpenAIGym.jl">OpenAIGym</a>: Wrapper for OpenAI&#39;s python package: gym</li></ul><h2><a class="nav-anchor" id="Environment-Interface-1" href="#Environment-Interface-1">Environment Interface</a></h2><p>New environments are created by subtyping <code>AbstractEnvironment</code> and implementing a few methods:</p><ul><li><code>reset!(env) -&gt; env</code></li><li><code>actions(env, s) -&gt; A</code></li><li><code>step!(env, s, a) -&gt; (r, s′)</code></li><li><code>finished(env, s′) -&gt; Bool</code></li></ul><p>and optional overrides:</p><ul><li><code>state(env) -&gt; s</code></li><li><code>reward(env) -&gt; r</code></li></ul><p>which map to <code>env.state</code> and <code>env.reward</code> respectively when unset.</p><ul><li><code>ismdp(env) -&gt; Bool</code></li></ul><p>An environment may be fully observable (MDP) or partially observable (POMDP). In the case of a partially observable environment, the state <code>s</code> is really an observation <code>o</code>.  To maintain consistency, we call everything a state, and assume that an environment is free to maintain additional (unobserved) internal state.  The <code>ismdp</code> query returns true when the environment is MDP, and false otherwise.</p><ul><li><code>maxsteps(env) -&gt; Int</code></li></ul><p>The terminating condition of an episode is control by <code>maxsteps() || finished()</code>. It&#39;s default value is <code>0</code>, indicates unlimited.</p><hr/><p>An minimal example for testing purpose is <code>test/foo.jl</code>.</p><p>TODO: more details and examples</p><h2><a class="nav-anchor" id="Policy-Interface-1" href="#Policy-Interface-1">Policy Interface</a></h2><p>Agents/policies are created by subtyping <code>AbstractPolicy</code> and implementing <code>action</code>. The built-in random policy is a short example:</p><pre><code class="language-julia">struct RandomPolicy &lt;: AbstractPolicy end
action(π::RandomPolicy, r, s, A) = rand(A)</code></pre><p>Where <code>A</code> is the action space. The <code>action</code> method maps the last reward and current state to the next chosen action: <code>(r, s) -&gt; a</code>.</p><ul><li><code>reset!(π::AbstractPolicy) -&gt; π</code></li></ul><h2><a class="nav-anchor" id="Episode-Iterator-1" href="#Episode-Iterator-1">Episode Iterator</a></h2><p>Iterate through episodes using the <code>Episode</code> iterator. A 4-tuple <code>(s,a,r,s′)</code> is returned from each step of the episode:</p><pre><code class="language-julia">ep = Episode(env, π)
for (s, a, r, s′) in ep
    # do some custom processing of the sars-tuple
end
R = ep.total_reward
T = ep.niter</code></pre><p>There is also a convenience method <code>run_episode</code>. The following is an equivalent method to the last example:</p><pre><code class="language-julia">R = run_episode(env, π) do
    # anything you want... this section is called after each step
end</code></pre><hr/><h2><a class="nav-anchor" id="Author:-[Tom-Breloff](https://github.com/tbreloff)-1" href="#Author:-[Tom-Breloff](https://github.com/tbreloff)-1">Author: <a href="https://github.com/tbreloff">Tom Breloff</a></a></h2><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
