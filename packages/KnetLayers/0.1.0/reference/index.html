<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · KnetLayers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>KnetLayers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Function Documentation</span><ul><li class="current"><a class="toctext" href>Reference</a><ul class="internal"><li><a class="toctext" href="#Core-Layers-1">Core Layers</a></li><li><a class="toctext" href="#Nonlinearities-1">Nonlinearities</a></li><li><a class="toctext" href="#Loss-Functions-1">Loss Functions</a></li><li><a class="toctext" href="#Convolutional-Layers-1">Convolutional Layers</a></li><li><a class="toctext" href="#Recurrent-Layers-1">Recurrent Layers</a></li><li><a class="toctext" href="#Special-Layers-1">Special Layers</a></li><li><a class="toctext" href="#Function-Index-1">Function Index</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Function Documentation</li><li><a href>Reference</a></li></ul></nav><hr/><div id="topbar"><span>Reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><p><strong>Contents</strong></p><ul><li><a href="#Reference-1">Reference</a></li><ul><li><a href="#Core-Layers-1">Core Layers</a></li><li><a href="#Nonlinearities-1">Nonlinearities</a></li><li><a href="#Loss-Functions-1">Loss Functions</a></li><li><a href="#Convolutional-Layers-1">Convolutional Layers</a></li><li><a href="#Recurrent-Layers-1">Recurrent Layers</a></li><li><a href="#Special-Layers-1">Special Layers</a></li><li><a href="#Function-Index-1">Function Index</a></li></ul></ul><h2><a class="nav-anchor" id="Core-Layers-1" href="#Core-Layers-1">Core Layers</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Multiply" href="#KnetLayers.Multiply"><code>KnetLayers.Multiply</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Multiply(input=inputDimension, output=outputDimension, winit=xavier, atype=KnetLayers.arrtype)</code></pre><p>Creates a matrix multiplication layer based on <code>inputDimension</code> and <code>outputDimension</code>.     (m::Multiply) = m.w * x</p><p>By default parameters initialized with xavier, you may change it with <code>winit</code> argument.</p><p><strong>Keywords</strong></p><ul><li><code>input=inputDimension</code>: input dimension</li><li><code>output=outputDimension</code>: output dimension</li><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>atype=KnetLayers.arrtype</code> : array type for parameters.  Default value is KnetArray{Float32} if you have gpu device. Otherwise it is Array{Float32}</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Embed" href="#KnetLayers.Embed"><code>KnetLayers.Embed</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Embed(input=inputSize, output=embedSize, winit=xavier, atype=KnetLayers.arrtype)</code></pre><p>Creates an embedding layer according to given <code>inputSize</code> and <code>embedSize</code> where <code>inputSize</code> is your number of unique items you want to embed, and <code>embedSize</code> is the size of output vectors. By default parameters initialized with xavier, you yam change it with <code>winit</code> argument.</p><pre><code class="language-none">(m::Embed)(x::Array{T}) where T&lt;:Integer
(m::Embed)(x; keepsize=true)</code></pre><p>Embed objects are callable with an input which is either and integer array (one hot encoding) or an N-dimensional matrix. For N-dimensional matrix, <code>size(x,1)==inputSize</code></p><p><strong>Keywords</strong></p><ul><li><code>input=inputDimension</code>: input dimension</li><li><code>output=embeddingDimension</code>: output dimension</li><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>atype=KnetLayers.arrtype</code> : array type for parameters.  Default value is KnetArray{Float32} if you have gpu device. Otherwise it is Array{Float32}</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Linear" href="#KnetLayers.Linear"><code>KnetLayers.Linear</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Linear(input=inputSize, output=outputSize, winit=xavier, binit=zeros, atype=KnetLayers.arrtype)</code></pre><p>Creates and linear layer according to given <code>inputSize</code> and <code>outputSize</code>.</p><p><strong>Keywords</strong></p><ul><li><code>input=inputSize</code>   input dimension</li><li><code>output=outputSize</code> output dimension</li><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>atype=KnetLayers.arrtype</code> : array type for parameters.  Default value is KnetArray{Float32} if you have gpu device. Otherwise it is Array{Float32}</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Dense" href="#KnetLayers.Dense"><code>KnetLayers.Dense</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Dense(input=inputSize, output=outputSize, activation=ReLU(), winit=xavier, binit=zeros, atype=KnetLayers.arrtype)</code></pre><p>Creates and deense layer according to given <code>input=inputSize</code> and <code>output=outputSize</code>. If activation is <code>nothing</code>, it acts like a <code>Linear</code> Layer.</p><p><strong>Keywords</strong></p><ul><li><code>input=inputSize</code>   input dimension</li><li><code>output=outputSize</code> output dimension</li><li><code>winit=xaiver</code>: weight initialization distribution</li><li><code>bias=zeros</code>:   bias initialization distribution</li><li><code>activation=ReLU()</code>  activation function(it does not broadcast) or an  activation layer</li><li><code>atype=KnetLayers.arrtype</code> : array type for parameters.  Default value is KnetArray{Float32} if you have gpu device. Otherwise it is Array{Float32}</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.BatchNorm" href="#KnetLayers.BatchNorm"><code>KnetLayers.BatchNorm</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">BatchNorm(channels:Int;options...)
(m::BatchNorm)(x;training=false) #forward run</code></pre><p><strong>Options</strong></p><ul><li><code>momentum=0.1</code>: A real number between 0 and 1 to be used as the scale of</li></ul><p>last mean and variance. The existing running mean or variance is multiplied by  (1-momentum).</p><ul><li>`mean=nothing&#39;: The running mean.</li><li><code>var=nothing</code>: The running variance.</li><li><code>meaninit=zeros</code>: The function used for initialize the running mean. Should either be <code>nothing</code> or</li></ul><p>of the form <code>(eltype, dims...)-&gt;data</code>. <code>zeros</code> is a good option.</p><ul><li><code>varinit=ones</code>: The function used for initialize the run</li><li><code>dataType=eltype(KnetLayers.arrtype)</code> : element type ∈ {Float32,Float64} for parameters. Default value is <code>eltype(KnetLayers.arrtype)</code></li><li><code>usegpu=KnetLayers.arrtype &lt;: KnetArray</code> :</li></ul><p><strong>Keywords</strong></p><ul><li><code>training</code>=nothing: When training is true, the mean and variance of x are used and moments</li></ul><p>argument is modified if it is provided. When training is false, mean and variance  stored in the moments argument are used. Default value is true when at least one  of x and params is AutoGrad.Value, false otherwise.</p></div></div></section><h2><a class="nav-anchor" id="Nonlinearities-1" href="#Nonlinearities-1">Nonlinearities</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.ReLU" href="#KnetLayers.ReLU"><code>KnetLayers.ReLU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ReLU()
(l::ReLU)(x) = max(0,x)</code></pre><p>Rectified Linear Unit function.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Sigm" href="#KnetLayers.Sigm"><code>KnetLayers.Sigm</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Sigm()
(l::Sigm)(x) = sigm(x)</code></pre><p>Sigmoid function</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Tanh" href="#KnetLayers.Tanh"><code>KnetLayers.Tanh</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Tanh()
(l::Tanh)(x) = tanh(x)</code></pre><p>Tangent hyperbolic function</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.ELU" href="#KnetLayers.ELU"><code>KnetLayers.ELU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ELU()
(l::ELU)(x) = elu(x) -&gt; Computes x &lt; 0 ? exp(x) - 1 : x</code></pre><p>Exponential Linear Unit nonlineariy.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.LeakyReLU" href="#KnetLayers.LeakyReLU"><code>KnetLayers.LeakyReLU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LeakyReLU(α=0.2)
(l::LeakyReLU)(x) -&gt; Computes x &lt; 0 ? α*x : x</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Dropout" href="#KnetLayers.Dropout"><code>KnetLayers.Dropout</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Dropout(p=0)</code></pre><p>Dropout Layer. <code>p</code> is the droput probability.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.SoftMax" href="#KnetLayers.SoftMax"><code>KnetLayers.SoftMax</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SoftMax(dims=:)
(l::SoftMax)(x)</code></pre><p>Treat entries in x as as unnormalized scores and return softmax probabilities.</p><p>dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions. In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.LogSoftMax" href="#KnetLayers.LogSoftMax"><code>KnetLayers.LogSoftMax</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LogSoftMax(dims=:)
(l::LogSoftMax)(x)</code></pre><p>Treat entries in x as as unnormalized log probabilities and return normalized log probabilities.</p><p>dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions. In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.LogSumExp" href="#KnetLayers.LogSumExp"><code>KnetLayers.LogSumExp</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LogSumExp(dims=:)
(l::LogSumExp)(x)</code></pre><p>Compute log(sum(exp(x);dims)) in a numerically stable manner.</p><p>dims is an optional argument, if not specified the summation is over the whole x, otherwise the summation is performed over the given dimensions. In particular if x   is a matrix, dims=1 sums columns of x and dims=2 sums rows of x.</p></div></div></section><h2><a class="nav-anchor" id="Loss-Functions-1" href="#Loss-Functions-1">Loss Functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.CrossEntropyLoss" href="#KnetLayers.CrossEntropyLoss"><code>KnetLayers.CrossEntropyLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">CrossEntropyLoss(dims=1)
(l::CrossEntropyLoss)(scores, answers; average=true)</code></pre><p>Calculates negative log likelihood error on your predicted scores. <code>answers</code> should be integers corresponding to correct class indices. If an answer is 0, loss from that answer will not be included. This is usefull feature when you are working with unequal length sequences.</p><p>if dims==1</p><ul><li>size(scores) = C,[B,T1,T2,...]</li><li>size(answers)= [B,T1,T2,...]</li></ul><p>elseif dims==2</p><ul><li>size(scores) = [B,T1,T2,...],C</li><li>size(answers)= [B,T1,T2,...]</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.BCELoss" href="#KnetLayers.BCELoss"><code>KnetLayers.BCELoss</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">BCELoss(average=true)
(l::BCELoss)(scores, answers)
Computes binary cross entropy given scores(predicted values) and answer labels. answer values should be {0,1}, then it returns negative of
mean|sum(answers * log(p) + (1-answers)*log(1-p)) where p is equal to 1/(1 + exp.(scores)). See also LogisticLoss.</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.LogisticLoss" href="#KnetLayers.LogisticLoss"><code>KnetLayers.LogisticLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LogisticLoss(average=true)
(l::LogisticLoss)(scores, answers)
Computes logistic loss given scores(predicted values) and answer labels. answer values should be {-1,1}, then it returns mean|sum(log(1 +
exp(-answers*scores))). See also `BCELoss`.</code></pre></div></div></section><h2><a class="nav-anchor" id="Convolutional-Layers-1" href="#Convolutional-Layers-1">Convolutional Layers</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Conv" href="#KnetLayers.Conv"><code>KnetLayers.Conv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Conv(;height=filterHeight, width=filterWidth, inout = 1 =&gt; 1, kwargs...)</code></pre><p>Creates and convolutional layer <code>Filtering{typeof(conv4)}</code> according to given filter dimensions.</p><pre><code class="language-none">(m::Filtering{typeof(conv4)})(x) #forward run</code></pre><p>If <code>m.w</code> has dimensions <code>(W1,W2,...,I,O)</code> and <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,O,N)</code> where</p><pre><code class="language-none">Yi=1+floor((Xi+2*padding[i]-Wi)/stride[i])</code></pre><p>Here <code>I</code> is the number of input channels, <code>O</code> is the number of output channels, <code>N</code> is the number of instances, and <code>Wi,Xi,Yi</code> are spatial dimensions. <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an tuple with entries for each spatial dimension.</p><p><strong>Keywords</strong></p><ul><li><code>inout=input_channels =&gt; output_channels</code></li><li><code>activation=identity</code>: nonlinear function applied after convolution</li><li><code>pool=nothing</code>: Pooling layer or window size of pooling</li><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>upscale=1</code>: upscale factor for each dimension.</li><li><code>mode=0</code>: 0 for convolution and 1 for cross-correlation.</li><li><code>alpha=1</code>: can be used to scale the result.</li><li><code>handle</code>: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.DeConv" href="#KnetLayers.DeConv"><code>KnetLayers.DeConv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">DeConv(;height=filterHeight, width=filterWidth, inout=1=&gt;1, kwargs...)</code></pre><p>Creates and deconvolutional layer <code>Filtering{typeof(deconv4)}</code>  according to given filter dimensions.</p><pre><code class="language-none">(m::Filtering{typeof(deconv4)})(x) #forward run</code></pre><p>If <code>m.w</code> has dimensions <code>(W1,W2,...,I,O)</code> and <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,O,N)</code> where</p><pre><code class="language-none">Yi = Wi+stride[i](Xi-1)-2padding[i]</code></pre><p>Here <code>I</code> is the number of input channels, <code>O</code> is the number of output channels, <code>N</code> is the number of instances, and <code>Wi,Xi,Yi</code> are spatial dimensions. <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an tuple with entries for each spatial dimension.</p><p><strong>Keywords</strong></p><ul><li><code>inout=input_channels =&gt; output_channels</code></li><li><code>activation=identity</code>: nonlinear function applied after convolution</li><li><code>unpool=nothing</code>: Unpooling layer or window size of unpooling</li><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>upscale=1</code>: upscale factor for each dimension.</li><li><code>mode=0</code>: 0 for convolution and 1 for cross-correlation.</li><li><code>alpha=1</code>: can be used to scale the result.</li><li><code>handle</code>: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.Pool" href="#KnetLayers.Pool"><code>KnetLayers.Pool</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Pool(kwargs...)
(::Sampling{typeof(pool)})(x)</code></pre><p>Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width.</p><p>Currently 4 or 5 dimensional KnetArrays with Float32 or Float64 entries are supported. If x has dimensions (X1,X2,...,I,N), the result y will have dimensions (Y1,Y2,...,I,N) where</p><p>Yi=1+floor((Xi+2*padding[i]-window[i])/stride[i])</p><p>Here I is the number of input channels, N is the number of instances, and Xi,Yi are spatial dimensions. window, padding and stride are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.</p><p>Keywords:</p><ul><li><p>window=2: the pooling window size for each dimension.</p></li><li><p>padding=0: the number of extra zeros implicitly concatenated at the</p></li></ul><p>start and at the end of each dimension.</p><ul><li>stride=window: the number of elements to slide to reach the next pooling</li></ul><p>window.</p><ul><li>mode=0: 0 for max, 1 for average including padded values, 2 for average</li></ul><p>excluding padded values.</p><ul><li>maxpoolingNanOpt=0: Nan numbers are not propagated if 0, they are</li></ul><p>propagated if 1.</p><ul><li>alpha=1: can be used to scale the result.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.UnPool" href="#KnetLayers.UnPool"><code>KnetLayers.UnPool</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">UnPool(kwargs...)
(::Sampling{typeof(unpool)})(x)

Reverse of pooling. It has same kwargs with Pool

x == pool(unpool(x;o...); o...)</code></pre></div></div></section><h2><a class="nav-anchor" id="Recurrent-Layers-1" href="#Recurrent-Layers-1">Recurrent Layers</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.AbstractRNN" href="#KnetLayers.AbstractRNN"><code>KnetLayers.AbstractRNN</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SRNN(;input=inputSize, hidden=hiddenSize, activation=:relu, options...)
LSTM(;input=inputSize, hidden=hiddenSize, options...)
GRU(;input=inputSize, hidden=hiddenSize, options...)

(1) (l::T)(x; kwargs...) where T&lt;:AbstractRNN
(2) (l::T)(x::Array{Int}; batchSizes=nothing, kwargs...) where T&lt;:AbstractRNN
(3) (l::T)(x::Vector{Vector{Int}}; sorted=false, kwargs...) where T&lt;:AbstractRNN</code></pre><p>All RNN layers has above forward run(1,2,3) functionalities.</p><p>(1) <code>x</code> is an input array with size equals d,[B,T]</p><p>(2) For this You need to have an RNN with embedding layer. <code>x</code> is an integer array and inputs coressponds one hot vector indices. You can give 2D array for minibatching as rows corresponds to one instance. You can give 1D array with minibatching by specifying batch batchSizes argument. Checkout <code>Knet.rnnforw</code> for this.</p><p>(3) For this You need to have an RNN with embedding layer. <code>x</code> is an vector of integer vectors. Every integer vector corresponds to an instance. It automatically batches inputs. It is better to give inputs as sorted. If your inputs sorted you can make <code>sorted</code> argument true to increase performance.</p><p>see RNNOutput</p><p><strong>options</strong></p><ul><li><code>embed=nothing</code>: embedding size or and embedding layer</li><li><code>numLayers=1</code>: Number of RNN layers.</li><li><code>bidirectional=false</code>: Create a bidirectional RNN if <code>true</code>.</li><li><code>dropout=0</code>: Dropout probability. Ignored if <code>numLayers==1</code>.</li><li><code>skipInput=false</code>: Do not multiply the input with a matrix if <code>true</code>.</li><li><code>dataType=eltype(KnetLayers.arrtype)</code>: Data type to use for weights. Default is Float32.</li><li><code>algo=0</code>: Algorithm to use, see CUDNN docs for details.</li><li><code>seed=0</code>: Random number seed for dropout. Uses <code>time()</code> if 0.</li><li><code>winit=xavier</code>: Weight initialization method for matrices.</li><li><code>binit=zeros</code>: Weight initialization method for bias vectors.</li><li><code>usegpu=(KnetLayers.arrtype &lt;: KnetArray)</code>: GPU used by default if one exists.</li></ul><p><strong>Keywords</strong></p><ul><li>hx=nothing : initial hidden states</li><li>cx=nothing : initial memory cells</li><li>hy=false   : if true returns h</li><li>cy=false   : if true returns c</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.RNNOutput" href="#KnetLayers.RNNOutput"><code>KnetLayers.RNNOutput</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct RNNOutput
    y
    hidden
    memory
    indices
end</code></pre><p>Outputs of the RNN models are always <code>RNNOutput</code> <code>hidden</code>,<code>memory</code> and <code>indices</code> may be nothing depending on the kwargs you used in forward.</p><p><code>y</code> is last hidden states of each layer. <code>size(y)=(H/2H,[B,T])</code>. If you use unequal length instances in a batch input, <code>y</code> becomes 2D array <code>size(y)=(H/2H,sum_of_sequence_lengths)</code>. See <code>indices</code> and <code>PadRNNOutput</code> to get correct time outputs for a specific instance or to pad whole output.</p><p><code>h</code> is the hidden states in each timesstep. <code>size(h) = h,B,L/2L</code></p><p><code>c</code> is the hidden states in each timesstep. <code>size(h) = h,B,L/2L</code></p><p><code>indices</code> is corresponding instace indices for your <code>RNNOutput.y</code>. You may call <code>yi = y[:,indices[i]]</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.PadSequenceArray" href="#KnetLayers.PadSequenceArray"><code>KnetLayers.PadSequenceArray</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">PadSequenceArray(batch::Vector{Vector{T}}) where T&lt;:Integer</code></pre><p>Pads a batch of integer arrays with zeros</p><p><code>julia&gt; PadSequenceArray([[1,2,3],[1,2],[1]]) 3×3 Array{Int64,2}:  1  2  3  1  2  0  1  0  0</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.PadRNNOutput" href="#KnetLayers.PadRNNOutput"><code>KnetLayers.PadRNNOutput</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">PadRNNOutput(s::RNNOutput)</code></pre><p>Pads a rnn output if it is produces by unequal length batches <code>size(s.y)=(H/2H,sum_of_sequence_lengths)</code> becomes <code>(H/2H,B,Tmax)</code></p></div></div></section><h2><a class="nav-anchor" id="Special-Layers-1" href="#Special-Layers-1">Special Layers</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KnetLayers.MLP" href="#KnetLayers.MLP"><code>KnetLayers.MLP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MLP(h::Int...;kwargs...)</code></pre><p>Creates a multi layer perceptron according to given hidden states. First hidden state is equal to input size and the last one equal to output size.</p><pre><code class="language-none">(m::MLP)(x;prob=0)</code></pre><p>Runs MLP with given input <code>x</code>. <code>prob</code> is the dropout probability.</p><p><strong>Keywords</strong></p><ul><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>activation=ReLU()</code>: activation layer or function</li><li><code>atype=KnetLayers.arrtype</code> : array type for parameters.  Default value is KnetArray{Float32} if you have gpu device. Otherwise it is Array{Float32}</li></ul></div></div></section><h2><a class="nav-anchor" id="Function-Index-1" href="#Function-Index-1">Function Index</a></h2><ul><li><a href="#KnetLayers.AbstractRNN"><code>KnetLayers.AbstractRNN</code></a></li><li><a href="#KnetLayers.BCELoss"><code>KnetLayers.BCELoss</code></a></li><li><a href="#KnetLayers.BatchNorm"><code>KnetLayers.BatchNorm</code></a></li><li><a href="#KnetLayers.CrossEntropyLoss"><code>KnetLayers.CrossEntropyLoss</code></a></li><li><a href="#KnetLayers.Dense"><code>KnetLayers.Dense</code></a></li><li><a href="#KnetLayers.Dropout"><code>KnetLayers.Dropout</code></a></li><li><a href="#KnetLayers.ELU"><code>KnetLayers.ELU</code></a></li><li><a href="#KnetLayers.Embed"><code>KnetLayers.Embed</code></a></li><li><a href="#KnetLayers.LeakyReLU"><code>KnetLayers.LeakyReLU</code></a></li><li><a href="#KnetLayers.Linear"><code>KnetLayers.Linear</code></a></li><li><a href="#KnetLayers.LogSoftMax"><code>KnetLayers.LogSoftMax</code></a></li><li><a href="#KnetLayers.LogSumExp"><code>KnetLayers.LogSumExp</code></a></li><li><a href="#KnetLayers.LogisticLoss"><code>KnetLayers.LogisticLoss</code></a></li><li><a href="#KnetLayers.MLP"><code>KnetLayers.MLP</code></a></li><li><a href="#KnetLayers.Multiply"><code>KnetLayers.Multiply</code></a></li><li><a href="#KnetLayers.RNNOutput"><code>KnetLayers.RNNOutput</code></a></li><li><a href="#KnetLayers.ReLU"><code>KnetLayers.ReLU</code></a></li><li><a href="#KnetLayers.Sigm"><code>KnetLayers.Sigm</code></a></li><li><a href="#KnetLayers.SoftMax"><code>KnetLayers.SoftMax</code></a></li><li><a href="#KnetLayers.Tanh"><code>KnetLayers.Tanh</code></a></li><li><a href="#KnetLayers.Conv"><code>KnetLayers.Conv</code></a></li><li><a href="#KnetLayers.DeConv"><code>KnetLayers.DeConv</code></a></li><li><a href="#KnetLayers.PadRNNOutput"><code>KnetLayers.PadRNNOutput</code></a></li><li><a href="#KnetLayers.PadSequenceArray"><code>KnetLayers.PadSequenceArray</code></a></li><li><a href="#KnetLayers.Pool"><code>KnetLayers.Pool</code></a></li><li><a href="#KnetLayers.UnPool"><code>KnetLayers.UnPool</code></a></li></ul><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a></footer></article></body></html>
