<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· DecisionTree.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DecisionTree.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Classification-1">Classification</a></li><li><a class="toctext" href="#Regression-1">Regression</a></li><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#ScikitLearn.jl-API-1">ScikitLearn.jl API</a></li><li><a class="toctext" href="#Native-API-1">Native API</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="DecisionTree.jl-1" href="#DecisionTree.jl-1">DecisionTree.jl</a></h1><p><a href="https://travis-ci.org/bensadeghi/DecisionTree.jl"><img src="https://travis-ci.org/bensadeghi/DecisionTree.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/r/bensadeghi/DecisionTree.jl?branch=master"><img src="https://coveralls.io/repos/bensadeghi/DecisionTree.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><p><a href="http://pkg.julialang.org/?pkg=DecisionTree&amp;ver=0.5"><img src="http://pkg.julialang.org/badges/DecisionTree_0.5.svg" alt="DecisionTree"/></a> <a href="http://pkg.julialang.org/?pkg=DecisionTree&amp;ver=0.6"><img src="http://pkg.julialang.org/badges/DecisionTree_0.6.svg" alt="DecisionTree"/></a> <a href="http://pkg.julialang.org/?pkg=DecisionTree&amp;ver=0.7"><img src="http://pkg.julialang.org/badges/DecisionTree_0.7.svg" alt="DecisionTree"/></a></p><p>Julia implementation of Decision Tree and Random Forest algorithms</p><h2><a class="nav-anchor" id="Classification-1" href="#Classification-1">Classification</a></h2><ul><li>pre-pruning (max depth, min leaf size)</li><li>post-pruning (pessimistic pruning)</li><li>parallelized bagging (random forests)</li><li>adaptive boosting (decision stumps)</li><li>cross validation (n-fold)</li><li>support for mixed categorical and numerical data</li></ul><h2><a class="nav-anchor" id="Regression-1" href="#Regression-1">Regression</a></h2><ul><li>pre-pruning (max depth, min leaf size)</li><li>parallelized bagging (random forests)</li><li>cross validation (n-fold)</li><li>support for numerical features</li></ul><p><strong>Note that regression is implied if labels/targets are of type Array{Float}</strong></p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>You can install DecisionTree.jl using Julia&#39;s package manager</p><pre><code class="language-julia">Pkg.add(&quot;DecisionTree&quot;)</code></pre><h2><a class="nav-anchor" id="ScikitLearn.jl-API-1" href="#ScikitLearn.jl-API-1">ScikitLearn.jl API</a></h2><p>DecisionTree.jl supports the <a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a> interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)</p><p>Available models: <code>DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier</code>. See each model&#39;s help (eg. <code>?DecisionTreeRegressor</code> at the REPL) for more information</p><h3><a class="nav-anchor" id="Classification-Example-1" href="#Classification-Example-1">Classification Example</a></h3><p>Load DecisionTree package</p><pre><code class="language-julia">using DecisionTree</code></pre><p>Separate Fisher&#39;s Iris dataset features and labels</p><pre><code class="language-julia">features, labels = load_data(&quot;iris&quot;)    # also see &quot;adult&quot; and &quot;digits&quot; datasets

# the data loaded are of type Array{Any}
# cast them to concrete types for better performance
features = Float64.(features)
labels   = String.(labels)</code></pre><p>Pruned Tree Classifier</p><pre><code class="language-julia"># train depth-truncated classifier
model = DecisionTreeClassifier(max_depth=2)
fit!(model, features, labels)
# pretty print of the tree, to a depth of 5 nodes (optional)
print_tree(model, 5)
# apply learned model
predict(model, [5.9,3.0,5.1,1.9])
# get the probability of each label
predict_proba(model, [5.9,3.0,5.1,1.9])
println(get_classes(model)) # returns the ordering of the columns in predict_proba&#39;s output
# run n-fold cross validation over 3 CV folds
# See ScikitLearn.jl for installation instructions
using ScikitLearn.CrossValidation: cross_val_score
accuracy = cross_val_score(model, features, labels, cv=3)</code></pre><p>Also have a look at these <a href="https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb">classification</a>, and <a href="https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb">regression</a> notebooks.</p><h2><a class="nav-anchor" id="Native-API-1" href="#Native-API-1">Native API</a></h2><h3><a class="nav-anchor" id="Classification-Example-2" href="#Classification-Example-2">Classification Example</a></h3><p>Decision Tree Classifier</p><pre><code class="language-julia"># train full-tree classifier
model = build_tree(labels, features)
# prune tree: merge leaves having &gt;= 90% combined purity (default: 100%)
model = prune_tree(model, 0.9)
# pretty print of the tree, to a depth of 5 nodes (optional)
print_tree(model, 5)
# apply learned model
apply_tree(model, [5.9,3.0,5.1,1.9])
# get the probability of each label
apply_tree_proba(model, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# run 3-fold cross validation of pruned tree,
n_folds=3
accuracy = nfoldCV_tree(labels, features, n_folds)

# set of classification parameters and respective default values
# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)
# max_depth: maximum depth of the decision tree (default: -1, no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# n_subfeatures: number of features to select at random (default: 0, keep all)
n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2
min_purity_increase=0.0; pruning_purity = 1.0

model    =   build_tree(labels, features,
                        n_subfeatures,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase)

accuracy = nfoldCV_tree(labels, features,
                        n_folds,
                        pruning_purity,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase)</code></pre><p>Random Forest Classifier</p><pre><code class="language-julia"># train random forest classifier
# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6
model = build_forest(labels, features, 2, 10, 0.5, 6)
# apply learned model
apply_forest(model, [5.9,3.0,5.1,1.9])
# get the probability of each label
apply_forest_proba(model, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# run 3-fold cross validation for forests, using 2 random features per split
n_folds=3; n_subfeatures=2
accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)

# set of classification parameters and respective default values
# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))
# n_trees: number of trees to train (default: 10)
# partial_sampling: fraction of samples to train each tree on (default: 0.7)
# max_depth: maximum depth of the decision trees (default: no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1
min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0

model    =   build_forest(labels, features,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase)

accuracy = nfoldCV_forest(labels, features,
                          n_folds,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase)</code></pre><p>Adaptive-Boosted Decision Stumps Classifier</p><pre><code class="language-julia"># train adaptive-boosted stumps, using 7 iterations
model, coeffs = build_adaboost_stumps(labels, features, 7);
# apply learned model
apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])
# get the probability of each label
apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# run 3-fold cross validation for boosted stumps, using 7 iterations
n_iterations=7; n_folds=3
accuracy = nfoldCV_stumps(labels, features,
                          n_folds,
                          n_iterations)</code></pre><h3><a class="nav-anchor" id="Regression-Example-1" href="#Regression-Example-1">Regression Example</a></h3><pre><code class="language-julia">n, m = 10^3, 5
features = randn(n, m)
weights = rand(-2:2, m)
labels = features * weights</code></pre><p>Regression Tree</p><pre><code class="language-julia"># train regression tree
model = build_tree(labels, features)
# apply learned model
apply_tree(model, [-0.9,3.0,5.1,1.9,0.0])
# run 3-fold cross validation, returns array of coefficients of determination (R^2)
n_folds = 3
r2 = nfoldCV_tree(labels, features, n_folds)

# set of regression parameters and respective default values
# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)
# max_depth: maximum depth of the decision tree (default: -1, no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# n_subfeatures: number of features to select at random (default: 0, keep all)
n_subfeatures = 0; max_depth = -1; min_samples_leaf = 5
min_samples_split = 2; min_purity_increase = 0.0; pruning_purity = 1.0

model = build_tree(labels, features,
                   n_subfeatures,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase)

r2 =  nfoldCV_tree(labels, features,
                   n_folds,
                   pruning_purity,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase)</code></pre><p>Regression Random Forest</p><pre><code class="language-julia"># train regression forest, using 2 random features, 10 trees,
# averaging of 5 samples per leaf, and 0.7 portion of samples per tree
model = build_forest(labels, features, 2, 10, 0.7, 5)
# apply learned model
apply_forest(model, [-0.9,3.0,5.1,1.9,0.0])
# run 3-fold cross validation on regression forest, using 2 random features per split
n_subfeatures=2; n_folds=3
r2 = nfoldCV_forest(labels, features, n_folds, n_subfeatures)

# set of regression build_forest() parameters and respective default values
# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))
# n_trees: number of trees to train (default: 10)
# partial_sampling: fraction of samples to train each tree on (default: 0.7)
# max_depth: maximum depth of the decision trees (default: no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1
min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0

model = build_forest(labels, features,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase)

r2 =  nfoldCV_forest(labels, features,
                     n_folds,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase)</code></pre><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
