<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · DecisionTree.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DecisionTree.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.AdaBoostStumpClassifier" href="#DecisionTree.AdaBoostStumpClassifier"><code>DecisionTree.AdaBoostStumpClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AdaBoostStumpClassifier(; n_iterations::Int=0)</code></pre><p>Adaboosted decision tree stumps. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_iterations</code>: number of iterations of AdaBoost</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><pre><code class="language-none">DecisionTree.ConfusionMatrix</code></pre><pre><code class="language-none">DecisionTree.DecisionTree</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.DecisionTreeClassifier" href="#DecisionTree.DecisionTreeClassifier"><code>DecisionTree.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DecisionTreeClassifier(; pruning_purity_threshold=0.0,
                       max_depth::Int=-1,
                       min_samples_leaf::Int=1,
                       min_samples_split::Int=2,
                       min_purity_increase::Float=0.0,
                       n_subfeatures::Int=0,
                       rng=Random.GLOBAL_RNG)</code></pre><p>Decision tree classifier. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>pruning_purity_threshold</code>: (post-pruning) merge leaves having <code>&gt;=thresh</code> combined purity (default: no pruning)</li><li><code>max_depth</code>: maximum depth of the decision tree (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 1)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split (default: 2)</li><li><code>min_purity_increase</code>: minimum purity needed for a split (default: 0.0)</li><li><code>n_subfeatures</code>: number of features to select at random (default: keep all)</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.DecisionTreeRegressor" href="#DecisionTree.DecisionTreeRegressor"><code>DecisionTree.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DecisionTreeRegressor(; pruning_purity_threshold=0.0,
                      max_depth::Int-1,
                      min_samples_leaf::Int=5,
                      min_samples_split::Int=2,
                      min_purity_increase::Float=0.0,
                      n_subfeatures::Int=0,
                      rng=Random.GLOBAL_RNG)</code></pre><p>Decision tree regression. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>pruning_purity_threshold</code>: (post-pruning) merge leaves having <code>&gt;=thresh</code> combined purity (default: no pruning)</li><li><code>max_depth</code>: maximum depth of the decision tree (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 5)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split (default: 2)</li><li><code>min_purity_increase</code>: minimum purity needed for a split (default: 0.0)</li><li><code>n_subfeatures</code>: number of features to select at random (default: keep all)</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>get_classes</code></p></div></div></section><pre><code class="language-none">DecisionTree.Ensemble</code></pre><pre><code class="language-none">DecisionTree.Leaf</code></pre><pre><code class="language-none">DecisionTree.LeafOrNode</code></pre><pre><code class="language-none">DecisionTree.Node</code></pre><pre><code class="language-none">DecisionTree.R2</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.RandomForestClassifier" href="#DecisionTree.RandomForestClassifier"><code>DecisionTree.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RandomForestClassifier(; n_subfeatures::Int=-1,
                       n_trees::Int=10,
                       partial_sampling::Float=0.7,
                       max_depth::Int=-1,
                       rng=Random.GLOBAL_RNG)</code></pre><p>Random forest classification. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_subfeatures</code>: number of features to consider at random per split (default: -1, sqrt(# features))</li><li><code>n_trees</code>: number of trees to train (default: 10)</li><li><code>partial_sampling</code>: fraction of samples to train each tree on (default: 0.7)</li><li><code>max_depth</code>: maximum depth of the decision trees (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split</li><li><code>min_purity_increase</code>: minimum purity needed for a split</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.RandomForestRegressor" href="#DecisionTree.RandomForestRegressor"><code>DecisionTree.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RandomForestRegressor(; n_subfeatures::Int=-1,
                      n_trees::Int=10,
                      partial_sampling::Float=0.7,
                      max_depth::Int=-1,
                      min_samples_leaf::Int=5,
                      rng=Random.GLOBAL_RNG)</code></pre><p>Random forest regression. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_subfeatures</code>: number of features to consider at random per split (default: -1, sqrt(# features))</li><li><code>n_trees</code>: number of trees to train (default: 10)</li><li><code>partial_sampling</code>: fraction of samples to train each tree on (default: 0.7)</li><li><code>max_depth</code>: maximum depth of the decision trees (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 5)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split</li><li><code>min_purity_increase</code>: minimum purity needed for a split</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>get_classes</code></p></div></div></section><pre><code class="language-none">DecisionTree._convert</code></pre><pre><code class="language-none">DecisionTree._hist</code></pre><pre><code class="language-none">DecisionTree._hist_add!</code></pre><pre><code class="language-none">DecisionTree._nfoldCV</code></pre><pre><code class="language-none">DecisionTree._weighted_error</code></pre><pre><code class="language-none">DecisionTree.apply_adaboost_stumps</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_adaboost_stumps_proba" href="#DecisionTree.apply_adaboost_stumps_proba"><code>DecisionTree.apply_adaboost_stumps_proba</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">apply_adaboost_stumps_proba(stumps::Ensemble, coeffs, features, labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><pre><code class="language-none">DecisionTree.apply_forest</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_forest_proba" href="#DecisionTree.apply_forest_proba"><code>DecisionTree.apply_forest_proba</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">apply_forest_proba(forest::Ensemble, features, col_labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><pre><code class="language-none">DecisionTree.apply_tree</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_tree_proba" href="#DecisionTree.apply_tree_proba"><code>DecisionTree.apply_tree_proba</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">apply_tree_proba(::Node, features, col_labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><pre><code class="language-none">DecisionTree.build_adaboost_stumps</code></pre><pre><code class="language-none">DecisionTree.build_forest</code></pre><pre><code class="language-none">DecisionTree.build_stump</code></pre><pre><code class="language-none">DecisionTree.build_tree</code></pre><pre><code class="language-none">DecisionTree.compute_probabilities</code></pre><pre><code class="language-none">DecisionTree.confusion_matrix</code></pre><pre><code class="language-none">DecisionTree.depth</code></pre><pre><code class="language-none">DecisionTree.eval</code></pre><pre><code class="language-none">DecisionTree.fit!</code></pre><pre><code class="language-none">DecisionTree.get_classes</code></pre><pre><code class="language-none">DecisionTree.include</code></pre><pre><code class="language-none">DecisionTree.is_leaf</code></pre><pre><code class="language-none">DecisionTree.label_index</code></pre><pre><code class="language-none">DecisionTree.load_data</code></pre><pre><code class="language-none">DecisionTree.majority_vote</code></pre><pre><code class="language-none">DecisionTree.mean_squared_error</code></pre><pre><code class="language-none">DecisionTree.mk_rng</code></pre><pre><code class="language-none">DecisionTree.nfoldCV_forest</code></pre><pre><code class="language-none">DecisionTree.nfoldCV_stumps</code></pre><pre><code class="language-none">DecisionTree.nfoldCV_tree</code></pre><pre><code class="language-none">DecisionTree.predict</code></pre><pre><code class="language-none">DecisionTree.predict_log_proba</code></pre><pre><code class="language-none">DecisionTree.predict_proba</code></pre><pre><code class="language-none">DecisionTree.print_tree</code></pre><pre><code class="language-none">DecisionTree.prune_tree</code></pre><pre><code class="language-none">DecisionTree.stack_function_results</code></pre><pre><code class="language-none">DecisionTree.treeclassifier</code></pre><pre><code class="language-none">DecisionTree.treeregressor</code></pre><pre><code class="language-none">DecisionTree.util</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
