<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · Tokenize.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Tokenize.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tokenize-1" href="#Tokenize-1">Tokenize</a></h1><p><a href="https://travis-ci.org/KristofferC/Tokenize.jl"><img src="https://travis-ci.org/KristofferC/Tokenize.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://ci.appveyor.com/project/KristofferC/tokenize-jl"><img src="https://ci.appveyor.com/api/projects/status/h9d9webkxyhpx790?svg=true" alt="Build status"/></a>  <a href="https://codecov.io/github/KristofferC/Tokenize.jl?branch=master"><img src="https://codecov.io/github/KristofferC/Tokenize.jl/coverage.svg?branch=master" alt="codecov.io"/></a></p><p><code>Tokenize</code> is a Julia package that serves a similar purpose and API as the <a href="https://docs.python.org/3/library/tokenize.html">tokenize module</a> in Python but for Julia. This is to take a string or buffer containing Julia code, perform lexical analysis and return a stream of tokens.</p><p>The goals of this package is to be</p><ul><li>Fast, it currently lexes all of Julia source files in ~0.3 seconds (295 files, 1.16 million Tokens)</li><li>Round trippable, that is, from a stream of tokens the original string should be recoverable exactly.</li><li>Non error throwing. Instead of throwing errors a certain error token is returned.</li></ul><h3><a class="nav-anchor" id="API-1" href="#API-1">API</a></h3><h4><a class="nav-anchor" id="Tokenization-1" href="#Tokenization-1">Tokenization</a></h4><p>The function <code>tokenize</code> is the main entrypoint for generating <code>Token</code>s. It takes a string or a buffer and creates an iterator that will sequentially return the next <code>Token</code> until the end of string or buffer. The argument to <code>tokenize</code> can either be a <code>String</code>, <code>IOBuffer</code> or an <code>IOStream</code>.</p><pre><code class="language-jl">julia&gt; collect(tokenize(&quot;function f(x) end&quot;))
 1,1-1,8          KEYWORD        &quot;function&quot;
 1,9-1,9          WHITESPACE     &quot; &quot;
 1,10-1,10        IDENTIFIER     &quot;f&quot;
 1,11-1,11        LPAREN         &quot;(&quot;
 1,12-1,12        IDENTIFIER     &quot;x&quot;
 1,13-1,13        RPAREN         &quot;)&quot;
 1,14-1,14        WHITESPACE     &quot; &quot;
 1,15-1,17        KEYWORD        &quot;end&quot;
 1,18-1,17        ENDMARKER      &quot;&quot;</code></pre><h4><a class="nav-anchor" id="Tokens-1" href="#Tokens-1"><code>Token</code>s</a></h4><p>Each <code>Token</code> is represented by where it starts and ends, what string it contains and what type it is.</p><p>The API for a <code>Token</code> (non exported from the <code>Tokenize.Tokens</code> module) is.</p><pre><code class="language-julia">startpos(t)::Tuple{Int, Int} # row and column where the token start
endpos(t)::Tuple{Int, Int}   # row and column where the token ends
startbyte(T)::Int            # byte offset where the token start
endbyte(t)::Int              # byte offset where the token ends
untokenize(t)::String        # string representation of the token
kind(t)::Token.Kind          # kind of the token
exactkind(t)::Token.Kind     # exact kind of the token</code></pre><p>The difference between <code>kind</code> and <code>exactkind</code> is that <code>kind</code> returns <code>OP</code> for all operators and <code>KEYWORD</code> for all keywords while <code>exactkind</code> returns a unique kind for all different operators and keywords, ex;</p><pre><code class="language-jl">julia&gt; tok = collect(tokenize(&quot;⇒&quot;))[1];

julia&gt; Tokens.kind(tok)
OP::Tokenize.Tokens.Kind = 90

julia&gt; Tokens.exactkind(tok)
RIGHTWARDS_DOUBLE_ARROW::Tokenize.Tokens.Kind = 128</code></pre><p>All the different <code>Token.Kind</code> can be seen in the <a href="https://github.com/KristofferC/Tokenize.jl/blob/master/src/token_kinds.jl"><code>token_kinds.jl</code> file</a></p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
