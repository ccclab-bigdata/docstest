<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · XGrad.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>XGrad.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Main</a></li><li class="current"><a class="toctext" href>Tutorial</a><ul class="internal"><li><a class="toctext" href="#Expression-differentiation-1">Expression differentiation</a></li><li><a class="toctext" href="#Function-differentiation-1">Function differentiation</a></li><li><a class="toctext" href="#Memory-buffers-1">Memory buffers</a></li><li><a class="toctext" href="#Struct-derivatives-1">Struct derivatives</a></li><li><a class="toctext" href="#How-it-works-1">How it works</a></li><li><a class="toctext" href="#Defining-your-own-primitives-1">Defining your own primitives</a></li></ul></li><li><a class="toctext" href="../codediscovery/">Code Discovery</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tutorial</a></li></ul></nav><hr/><div id="topbar"><span>Tutorial</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h1><p>XGrad.jl is a package for symbolic tensor differentiation that lets you automatically derive gradients of algebraic expressions or Julia functions with such expressions. Let&#39;s start right from examples.</p><h2><a class="nav-anchor" id="Expression-differentiation-1" href="#Expression-differentiation-1">Expression differentiation</a></h2><pre><code class="language-none">using XGrad

xdiff(:(y = sum(W * x .+ b)); W=rand(3,4), x=rand(4), b=rand(3))</code></pre><p>In this code:</p><ul><li><code>:(sum(W * x .+ b))</code> is an expression we want to differentiate</li><li><code>W</code>, <code>x</code> and <code>b</code> are example values, we need them to understand type of variables  in the expression (e.g. matrix vs. vector vs. scalar)</li></ul><p>The result of this call should look something like this:</p><pre><code class="language-none">quote
    tmp692 = @get_or_create(mem, :tmp692, Array(zeros(Float64, (3,))))
    dy!dW = @get_or_create(mem, :dy!dW, Array(zeros(Float64, (3, 4))))
    dy!dx = @get_or_create(mem, :dy!dx, Array(zeros(Float64, (4,))))
    tmp698 = @get_or_create(mem, :tmp698, Array(zeros(Float64, (1, 4))))
    tmp696 = @get_or_create(mem, :tmp696, Array(zeros(Float64, (3,))))
    dy!db = @get_or_create(mem, :dy!db, Array(zeros(Float64, (3,))))
    dy!dy = (Float64)(0.0)
    y = (Float64)(0.0)
    tmp700 = @get_or_create(mem, :tmp700, Array(zeros(Float64, (4, 3))))
    tmp691 = @get_or_create(mem, :tmp691, Array(zeros(Float64, (3,))))
    dy!dy = 1.0
    tmp691 .= W * x
    tmp692 .= tmp691 .+ b
    tmp695 = size(tmp692)
    tmp696 .= ones(tmp695)
    dy!db = tmp696
    dy!dx .= W&#39; * (tmp696 .* dy!dy)
    dy!dW .= dy!db * x&#39;
    y = sum(tmp692)
    tmp702 = (y, dy!dW, dy!dx, dy!db)
end</code></pre><p>First 10 lines (those starting with <code>@get_or_create</code> macro) are variable initialization. Don&#39;t worry about them right now, I will explain them later in this tutorial. The rest of the code calculates gradients of the result variable <span>$y$</span> w.r.t. input arguments, i.e. <span>$\frac{dy}{dW}$</span>, <span>$\frac{dy}{dx}$</span> and <span>$\frac{dy}{db}$</span>. Note that the last expression is a tuple holding both - <code>y</code> and all the gradients. Differentiation requires computing <code>y</code> anyway, but you can use it or dismiss depending on your workflow.</p><p>The generated code is somewhat ugly, but much more efficient than a naive one (which we will demonstrate in the following section). To make it run, you need first to define a <code>mem::Dict</code> variable. Try this:</p><pre><code class="language-none"># since we will evaluate expression in global scope, we need to initialize variables first
W = rand(3,4)
x = rand(4)
b = rand(3)

# save derivative expression to a variable
dex = xdiff(:(y = sum(W * x .+ b)); W=W, x=x, b=b)

# define auxiliary variable `mem`
mem = Dict()
eval(dex)</code></pre><p>which should give us something like:</p><pre><code class="language-none">(4.528510092075925, [0.679471 0.727158 0.505823 0.209988; 0.679471 0.727158 0.505823 0.209988; 0.679471 0.727158 0.505823 0.209988], [0.919339, 1.61009, 1.74046, 1.9212], [1.0, 1.0, 1.0])</code></pre><p>Instead of efficient code you may want to get something more readable. Fortunately, XGrad&#39;s codegens are pluggable and you can easily switch default codegen to e.g. <code>VectorCodeGen</code>:</p><pre><code class="language-none">ctx = Dict(:codegen =&gt; VectorCodeGen())
xdiff(:(y = sum(W * x .+ b)); ctx=ctx, W=rand(3,4), x=rand(4), b=rand(3))</code></pre><p>this produces:</p><pre><code class="language-none">quote
    tmp796 = transpose(W)
    dy!dy = 1.0
    tmp794 = transpose(x)
    tmp787 = W * x
    tmp788 = tmp787 .+ b
    tmp791 = size(tmp788)
    tmp792 = ones(tmp791)
    dy!db = tmp792 .* dy!dy
    dy!dtmp787 = tmp792 .* dy!dy
    dy!dx = tmp796 * dy!dtmp787
    dy!dW = dy!dtmp787 * tmp794
    y = sum(tmp788)
    tmp798 = (y, dy!dW, dy!dx, dy!db)
end</code></pre><p>See more about different kinds of code generators in the corresponding section on the left [TODO].</p><h2><a class="nav-anchor" id="Function-differentiation-1" href="#Function-differentiation-1">Function differentiation</a></h2><p>In most optimization tasks you need not an expression, but a function for calculating derivatives. XGrad provides a convenient wrapper for it as well:</p><pre><code class="language-none"># in file loss.jl
predict(W, b, x) = W * x .+ b

loss(W, b, x, y) = sum((predict(W, b, x) .- y)^2)

# in REPL or another file
include(&quot;loss.jl&quot;)
W = rand(3,4); b = rand(3); x = rand(4); y=rand(3)
dloss = xdiff(loss; W=W, b=b, x=x, y=y)
dloss(W, b, x, y)</code></pre><p>And voilà! We get a value of the same structure as in previous section:</p><pre><code class="language-none">(3.531294775990527, [1.0199 1.09148 0.75925 0.315196; 1.92224 2.05715 1.43099 0.594062; 1.33645 1.43025 0.994905 0.413026], [1.50102, 2.82903, 1.9669], [2.20104, 3.07484, 3.31411, 4.33103], [-1.50102, -2.82903, -1.9669])</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>XGrad works on Julia source code. When differentiating a function, XGrad first tries to read its source code from a file where it was defined (using <a href="https://github.com/SimonDanisch/Sugar.jl">Sugar.jl</a>) and, if failed, to recover code from a lowered AST. The latter doesn&#39;t always work, so if you are working in REPL, it&#39;s a good idea to put functions to differentiate to a separate file and then <code>include(...)</code> it. Also see <a href="../codediscovery/#Code-Discovery-1">Code Discovery</a> for some other rules.</p></div></div><p>Compiling function derivatives beforehand may be tedious, so there&#39;s also a convenient shortcut - <code>xgrad</code> - that compiles derivatives dynamically and caches them for later use. We can rewrite previous example as:</p><pre><code class="language-none">include(&quot;loss.jl&quot;)
W = rand(3,4); b = rand(3); x = rand(4); y=rand(3)
xgrad(loss; W=W, b=b, x=x, y=y)</code></pre><p>This is very convenient when using in a training loop in machine learning, e.g. something like this:</p><pre><code class="language-none">W, b = ...
for (x, y) in batchview((X, Y))
    dW, db, dx, dy = xgrad(loss; W=W, b=b, x=x, y=y)   # compiled once, applied at each iteration
    update_params!(W, b, dW, db)
end</code></pre><h2><a class="nav-anchor" id="Memory-buffers-1" href="#Memory-buffers-1">Memory buffers</a></h2><p>Remember a strange <code>mem</code> variable that we&#39;ve seen in the <a href="#Expression-differentiation-1">Expression differentiation</a> section? It turns out that significant portion of time for computing a derivative (as well as any numeric code with tensors) is spend on memory allocations. The obvious way to fix it is to use memory buffers and in-place functions. This is exactly the default behavior of XGrad.jl: it allocates buffers for all temporary variables in <code>mem</code> dictionary and rewrites expressions using BLAS, broadcasting and in-place assignments. To take advantage of this feature, just add a buffer of type  <code>Dict{Any,Any}()</code> as a last argument to the derivative function:</p><pre><code class="language-none">mem = Dict()
dloss(W, b, x, y, mem)</code></pre><p>If you take a look at the value of <code>mem</code> after this call, you will find a number of keys for each intermediate variable. Here&#39;s a full example:</p><pre><code class="language-none">include(&quot;loss.jl&quot;)
W = rand(1000, 10_000); b = rand(1000); x = rand(10_000, 100); y=rand(1000)
dloss = xdiff(loss; W=W, b=b, x=x, y=y)

using BenchmarkTools

# without mem
@btime dloss(W, b, x, y)
# ==&gt; 155.191 ms (84 allocations: 175.52 MiB)

# with mem
mem = Dict()
@btime dloss(W, b, x, y, mem)
# ==&gt; 100.354 ms (26 allocations: 797.86 KiB)</code></pre><p><code>xgrad</code> supports memory buffers using keyword parameter <code>mem</code>:</p><pre><code class="language-none">@btime xgrad(loss; W=W, b=b, x=x, y=y, mem=mem)
# ==&gt; 100.640 ms (113 allocations: 802.36 KiB)</code></pre><h2><a class="nav-anchor" id="Struct-derivatives-1" href="#Struct-derivatives-1">Struct derivatives</a></h2><p>So far our loss functions were pretty simple taking only a couple of parameters, but in real life machine learning models have many more of them. Copying a dozen of arguments all over the code quickly becomes a pain in the neck. To fight this issue, XGrad supports derivatives of (mutable) structs. Here&#39;s an example:</p><pre><code class="language-none"># in file linear.jl
mutable struct Linear
    W::Matrix{Float64}
    b::Vector{Float64}
end

# we need a default constructor to instantiate a struct
# fields shouldn&#39;t necessary have meaningful values
Linear() = Linear(zeros(1,1), zeros(1))

predict(m::Linear, x) = m.W * x .+ m.b

loss(m::Linear, x, y) = sum((predict(m, x) .- y).^2)


## in REPL or another file
include(&quot;linear.jl&quot;)
m = Linear(randn(3,4), randn(3))
x = rand(4); y = rand(3)
dloss = xdiff(loss; m=m, x=x, y=y)
y_hat, dm, dx, dy = dloss(m, x, y)
# or using `xgrad`
y_hat, dm, dx, dy = xgrad(loss; m=m, x=x, y=y)</code></pre><p>Just like with arrays in previous example, <code>dm</code> has the same type (<code>Linear</code>) and size of its fields (<code>dm.W</code> and <code>dm.b</code>) as original model, but holds gradients of paramaters instead of their values. If you are doing something like SGD on model parameters, you can then update the model like this:</p><pre><code class="language-none">for fld in fieldnames(typeof(m))
    theta = getfield(m, fld)
    theta .-= getfield(dm, fld)
    setfield!(m, fld, theta)
end</code></pre><h2><a class="nav-anchor" id="How-it-works-1" href="#How-it-works-1">How it works</a></h2><p>XGrad works similar to reverse-mode automatic differentiation, but operates on symbolic variables instead of actual values. If you are familiar with AD, you should already know most details, if not - don&#39;t worry, it&#39;s pretty simple. The main idea is to decompose an expression into a chain of some primitive function calls that we already know how to differentiate, assign the deriviative of the result a &quot;seed&quot; value of 1.0 and then propagate derivatives back to the input parameters. Here&#39;s an example.</p><p>Let&#39;s say, you have an expression like this (where <span>$x$</span> is a plain number):</p><div>\[z = exp(sin(x))\]</div><p>It consists of 2 function calls that we write down, adding an intermediate variable <span>$y$</span>:</p><div>\[y = sin(x)\]</div><div>\[z = exp(y)\]</div><p>We aim to go through all variables <span>$v_i$</span> and collect derivatives <span>$\frac{\partial z}{\partial v_i}$</span>. The first variable is <span>$z$</span> itself. Since derivative of a variable w.r.t. itself is 1.0, we set:</p><div>\[\frac{dz}{dz} = 1.0\]</div><p>The next step is to find the derivative of <span>$\frac{\partial z}{\partial y}$</span>. We know that the derivative of <code>exp(u)</code> w.r.t. <span>$u$</span> is also <span>$exp(u)$</span>. If there has been some accomulated derivative from variables later in the chain, we should also multiply by it:</p><div>\[\frac{dz}{dy} = \frac{d(exp(y))}{dy} \cdot \frac{dz}{dz} = exp(y) \cdot \frac{dz}{dz}\]</div><p>Finally, from math classes we know that the derivative of <span>$sin(u)$</span> is <span>$cos(u)$</span>, so we add:</p><div>\[\frac{dz}{dx} = \frac{d(sin(x))}{dx} \cdot \frac{dz}{dy} = cos(x) \cdot \frac{dz}{dy}\]</div><p>The full derivative expression thus looks like:</p><div>\[\frac{dz}{dz} = 1.0\]</div><div>\[\frac{dz}{dy} = exp(y) \cdot \frac{dz}{dz}\]</div><div>\[\frac{dz}{dx} = cos(x) \cdot \frac{dz}{dy}\]</div><p>In case of scalar-valued function of multiple variables (i.e. <span>$R^n \rightarrow R$</span>, common in ML tasks) instead of &quot;derivative&quot; we say &quot;gradient&quot;, but approach stays more or less the same.</p><h2><a class="nav-anchor" id="Defining-your-own-primitives-1" href="#Defining-your-own-primitives-1">Defining your own primitives</a></h2><p>XGrad knows about most common primitive functions such as <code>*</code>, <code>+</code>, <code>exp</code>, etc., but there&#39;s certenly many more of them. Thus the library provides a <code>@diffrule</code> macro that lets you define your own differentiation rules. For example, provided a function for 2D convolution <code>conv2d(x, w)</code> and derivatives <code>conv2d_grad_x(...)</code> and <code>conv2d_grad_w</code>, you can add them like this</p><pre><code class="language-none">@diffrule conv2d(x, w) x conv2d_grad_x(x, w, ds)
@diffrule conv2d(x, w) w conv2d_grad_w(x, w, ds)</code></pre><p>where:</p><ul><li><code>conv2d(x, w)</code> is a target function expression</li><li><code>x</code> and <code>w</code> are variables to differentiate w.r.t.</li><li><code>conv2d_grad_x(...)</code> and <code>conv2d_grad_w(...)</code> are derivative expression</li><li><code>ds</code> is a previous gradient in the chain, e.g. if <code>y = conv2d(x, w)</code> and <code>z</code> is the  last variable of original expression, <code>ds</code> stands for gradient <span>$\frac{dz}{dy}$</span></li></ul><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Main</span></a><a class="next" href="../codediscovery/"><span class="direction">Next</span><span class="title">Code Discovery</span></a></footer></article></body></html>
