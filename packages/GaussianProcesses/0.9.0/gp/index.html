<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian Processes · GaussianProcesses.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GaussianProcesses.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li class="current"><a class="toctext" href>Gaussian Processes</a><ul class="internal"></ul></li><li><a class="toctext" href="../kernels/">Kernels</a></li><li><a class="toctext" href="../mean/">Means</a></li><li><a class="toctext" href="../lik/">Likelihoods</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Gaussian Processes</a></li></ul></nav><hr/><div id="topbar"><span>Gaussian Processes</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Gaussian-Processes-1" href="#Gaussian-Processes-1">Gaussian Processes</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.predict_f-Tuple{GPBase,AbstractArray{T,2} where T}" href="#GaussianProcesses.predict_f-Tuple{GPBase,AbstractArray{T,2} where T}"><code>GaussianProcesses.predict_f</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">predict_f(gp::GPBase, X::Matrix{Float64}[; full_cov::Bool = false])</code></pre><p>Return posterior mean and variance of the Gaussian Process <code>gp</code> at specfic points which are given as columns of matrix <code>X</code>. If <code>full_cov</code> is <code>true</code>, the full covariance matrix is returned instead of only variances.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.GPE-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.Mean,Kernel,Float64,GaussianProcesses.KernelData,PDMats.AbstractPDMat}" href="#GaussianProcesses.GPE-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.Mean,Kernel,Float64,GaussianProcesses.KernelData,PDMats.AbstractPDMat}"><code>GaussianProcesses.GPE</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">GPE(x, y, mean, kernel[, logNoise])</code></pre><p>Fit a Gaussian process to a set of training points. The Gaussian process is defined in terms of its user-defined mean and covariance (kernel) functions. As a default it is assumed that the observations are noise free.</p><p><strong>Arguments:</strong></p><ul><li><code>x::AbstractVecOrMat{Float64}</code>: Input observations</li><li><code>y::AbstractVector{Float64}</code>: Output observations</li><li><code>mean::Mean</code>: Mean function</li><li><code>kernel::Kernel</code>: Covariance function</li><li><code>logNoise::Float64</code>: Natural logarithm of the standard deviation for the observation noise. The default is -2.0, which is equivalent to assuming no observation noise.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.GPE-Tuple{}" href="#GaussianProcesses.GPE-Tuple{}"><code>GaussianProcesses.GPE</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">GPE(; mean::Mean = MeanZero(), kernel::Kernel = SE(0.0, 0.0), logNoise::Float64 = -2.0)</code></pre><p>Construct a <a href="#GaussianProcesses.GPE-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.Mean,Kernel,Float64,GaussianProcesses.KernelData,PDMats.AbstractPDMat}"><code>GPE</code></a> object without observations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.GP" href="#GaussianProcesses.GP"><code>GaussianProcesses.GP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">GP(x, y, mean::Mean, kernel::Kernel[, logNoise::Float64=-2.0])</code></pre><p>Fit a Gaussian process that is defined by its <code>mean</code>, its <code>kernel</code>, and the logarithm <code>logNoise</code> of the standard deviation of its observation noise to a set of training points <code>x</code> and <code>y</code>.</p><p>See also: <a href="#GaussianProcesses.GPE-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.Mean,Kernel,Float64,GaussianProcesses.KernelData,PDMats.AbstractPDMat}"><code>GPE</code></a></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.predict_y-Tuple{GPE,AbstractArray{T,2} where T}" href="#GaussianProcesses.predict_y-Tuple{GPE,AbstractArray{T,2} where T}"><code>GaussianProcesses.predict_y</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">predict_y(gp::GPE, x::Union{Vector{Float64},Matrix{Float64}}[; full_cov::Bool=false])</code></pre><p>Return the predictive mean and variance of Gaussian Process <code>gp</code> at specfic points which are given as columns of matrix <code>x</code>. If <code>full_cov</code> is <code>true</code>, the full covariance matrix is returned instead of only variances.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_target!-Tuple{GPE}" href="#GaussianProcesses.update_target!-Tuple{GPE}"><code>GaussianProcesses.update_target!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_target!(gp::GPE, ...)</code></pre><p>Update the log-posterior</p><div>\[\log p(θ | y) ∝ \log p(y | θ) +  \log p(θ)\]</div><p>of a Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.GPMC-Tuple{AbstractArray{T,2} where T,AbstractArray{#s4145,1} where #s4145&lt;:Real,GaussianProcesses.Mean,Kernel,Likelihood}" href="#GaussianProcesses.GPMC-Tuple{AbstractArray{T,2} where T,AbstractArray{#s4145,1} where #s4145&lt;:Real,GaussianProcesses.Mean,Kernel,Likelihood}"><code>GaussianProcesses.GPMC</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">GPMC(x, y, mean, kernel, lik)</code></pre><p>Fit a Gaussian process to a set of training points. The Gaussian process with non-Gaussian observations is defined in terms of its user-defined likelihood function, mean and covaiance (kernel) functions.</p><p>The non-Gaussian likelihood is handled by a Monte Carlo method. The latent function values are represented by centered (whitened) variables <span>$f(x) = m(x) + Lv$</span> where <span>$v ∼ N(0, I)$</span> and <span>$LLᵀ = K_θ$</span>.</p><p><strong>Arguments:</strong></p><ul><li><code>x::AbstractVecOrMat{Float64}</code>: Input observations</li><li><code>y::AbstractVector{&lt;:Real}</code>: Output observations</li><li><code>mean::Mean</code>: Mean function</li><li><code>kernel::Kernel</code>: Covariance function</li><li><code>lik::Likelihood</code>: Likelihood function</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.GP-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},AbstractArray{#s4145,1} where #s4145&lt;:Real,GaussianProcesses.Mean,Kernel,Likelihood}" href="#GaussianProcesses.GP-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},AbstractArray{#s4145,1} where #s4145&lt;:Real,GaussianProcesses.Mean,Kernel,Likelihood}"><code>GaussianProcesses.GP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">GP(x, y, mean::Mean, kernel::Kernel, lik::Likelihood)</code></pre><p>Fit a Gaussian process that is defined by its <code>mean</code>, its <code>kernel</code>, and its likelihood function <code>lik</code> to a set of training points <code>x</code> and <code>y</code>.</p><p>See also: <a href="#GaussianProcesses.GPMC-Tuple{AbstractArray{T,2} where T,AbstractArray{#s4145,1} where #s4145&lt;:Real,GaussianProcesses.Mean,Kernel,Likelihood}"><code>GPMC</code></a></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.predict_y-Tuple{GPMC,AbstractArray{T,2} where T}" href="#GaussianProcesses.predict_y-Tuple{GPMC,AbstractArray{T,2} where T}"><code>GaussianProcesses.predict_y</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">predict_y(gp::GPMC, x::Union{Vector{Float64},Matrix{Float64}}[; full_cov::Bool=false])</code></pre><p>Return the predictive mean and variance of Gaussian Process <code>gp</code> at specfic points which are given as columns of matrix <code>x</code>. If <code>full_cov</code> is <code>true</code>, the full covariance matrix is returned instead of only variances.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_target!-Tuple{GPMC}" href="#GaussianProcesses.update_target!-Tuple{GPMC}"><code>GaussianProcesses.update_target!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_target!(gp::GPMC, ...)</code></pre><p>Update the log-posterior</p><div>\[\log p(θ, v | y) ∝ \log p(y | v, θ) + \log p(v) + \log p(θ)\]</div><p>of a Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.mcmc-Tuple{GPBase}" href="#GaussianProcesses.mcmc-Tuple{GPBase}"><code>GaussianProcesses.mcmc</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">mcmc(gp::GPBase; kwargs...)</code></pre><p>Run MCMC algorithms provided by the Klara package for estimating the hyperparameters of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.make_posdef!-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.make_posdef!-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.make_posdef!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">make_posdef!(m::Matrix{Float64}, chol_factors::Matrix{Float64})</code></pre><p>Try to encode covariance matrix <code>m</code> as a positive definite matrix. The <code>chol_factors</code> matrix is recycled to store the cholesky decomposition, so as to reduce the number of memory allocations.</p><p>Sometimes covariance matrices of Gaussian processes are positive definite mathematically but have negative eigenvalues numerically. To resolve this issue, small weights are added to the diagonal (and hereby all eigenvalues are raised by that amount mathematically) until all eigenvalues are positive numerically.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.fit!-Union{Tuple{Y}, Tuple{X}, Tuple{GPE{X,Y,M,K,P,D} where D&lt;:KernelData where P&lt;:AbstractPDMat where K&lt;:Kernel where M&lt;:Mean,X,Y}} where Y where X" href="#GaussianProcesses.fit!-Union{Tuple{Y}, Tuple{X}, Tuple{GPE{X,Y,M,K,P,D} where D&lt;:KernelData where P&lt;:AbstractPDMat where K&lt;:Kernel where M&lt;:Mean,X,Y}} where Y where X"><code>GaussianProcesses.fit!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">fit!(gp::GPE{X,Y}, x::X, y::Y)</code></pre><p>Fit Gaussian process <code>GPE</code> to a training data set consisting of input observations <code>x</code> and output observations <code>y</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.get_ααinvcKI!-Tuple{AbstractArray{T,2} where T,PDMats.AbstractPDMat,Array{T,1} where T}" href="#GaussianProcesses.get_ααinvcKI!-Tuple{AbstractArray{T,2} where T,PDMats.AbstractPDMat,Array{T,1} where T}"><code>GaussianProcesses.get_ααinvcKI!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">get_ααinvcKI!(ααinvcKI::Matrix{Float64}, cK::AbstractPDMat, α::Vector)</code></pre><p>Write <code>ααᵀ - cK⁻¹</code> to <code>ααinvcKI</code> avoiding any memory allocation, where <code>cK</code> and <code>α</code> are the covariance matrix and the alpha vector of a Gaussian process, respectively. Hereby <code>α</code> is defined as <code>cK⁻¹ (Y - μ)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.initialise_target!-Tuple{GPE}" href="#GaussianProcesses.initialise_target!-Tuple{GPE}"><code>GaussianProcesses.initialise_target!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">initialise_target!(gp::GPE)</code></pre><p>Initialise the log-posterior</p><div>\[\log p(θ | y) ∝ \log p(y | θ) +  \log p(θ)\]</div><p>of a Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_cK!-Tuple{GPE}" href="#GaussianProcesses.update_cK!-Tuple{GPE}"><code>GaussianProcesses.update_cK!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_cK!(gp::GPE)</code></pre><p>Update the covariance matrix and its Cholesky decomposition of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_dmll!-Tuple{GPE,AbstractArray{T,2} where T}" href="#GaussianProcesses.update_dmll!-Tuple{GPE,AbstractArray{T,2} where T}"><code>GaussianProcesses.update_dmll!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none"> update_dmll!(gp::GPE, ...)</code></pre><p>Update the gradient of the marginal log-likelihood of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_mll_and_dmll!-Tuple{GPE,AbstractArray{T,2} where T}" href="#GaussianProcesses.update_mll_and_dmll!-Tuple{GPE,AbstractArray{T,2} where T}"><code>GaussianProcesses.update_mll_and_dmll!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_mll_and_dmll!(gp::GPE, ...)</code></pre><p>Update the gradient of the marginal log-likelihood of a Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_target_and_dtarget!-Tuple{GPE,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.update_target_and_dtarget!-Tuple{GPE,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.update_target_and_dtarget!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_target_and_dtarget!(gp::GPE, ...)</code></pre><p>Update the log-posterior</p><div>\[\log p(θ | y) ∝ \log p(y | θ) +  \log p(θ)\]</div><p>of a Gaussian process <code>gp</code> and its derivative.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.initialise_ll!-Tuple{GPMC}" href="#GaussianProcesses.initialise_ll!-Tuple{GPMC}"><code>GaussianProcesses.initialise_ll!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">initialise_ll!(gp::GPMC)</code></pre><p>Initialise the log-likelihood of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.initialise_target!-Tuple{GPMC}" href="#GaussianProcesses.initialise_target!-Tuple{GPMC}"><code>GaussianProcesses.initialise_target!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">initialise_target!(gp::GPMC)</code></pre><p>Initialise the log-posterior</p><div>\[\log p(θ, v | y) ∝ \log p(y | v, θ) + \log p(v) + \log p(θ)\]</div><p>of a Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_cK!-Tuple{GPMC}" href="#GaussianProcesses.update_cK!-Tuple{GPMC}"><code>GaussianProcesses.update_cK!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_cK!(gp::GPMC)</code></pre><p>Update the covariance matrix and its Cholesky decomposition of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_dll!-Tuple{GPMC,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.update_dll!-Tuple{GPMC,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.update_dll!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none"> update_dll!(gp::GPMC, ...)</code></pre><p>Update the gradient of the log-likelihood of Gaussian process <code>gp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.update_target_and_dtarget!-Tuple{GPMC,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.update_target_and_dtarget!-Tuple{GPMC,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.update_target_and_dtarget!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">update_target_and_dtarget!(gp::GPMC, ...)</code></pre><p>Update the log-posterior</p><div>\[\log p(θ, v | y) ∝ \log p(y | v, θ) + \log p(v) + \log p(θ)\]</div><p>of a Gaussian process <code>gp</code> and its derivative.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.composite_param_names-Tuple{Any,Any}" href="#GaussianProcesses.composite_param_names-Tuple{Any,Any}"><code>GaussianProcesses.composite_param_names</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">composite_param_names(objects, prefix)</code></pre><p>Call <code>get_param_names</code> on each element of <code>objects</code> and prefix the returned name of the element at index <code>i</code> with <code>prefix * i * &#39;_&#39;</code>.</p><p><strong>Examples</strong></p><pre><code class="language-none">julia&gt; GaussianProcesses.get_param_names(ProdKernel(Mat12Iso(1/2, 1/2), SEArd([0.0, 1.0], 0.0)))
5-element Array{Symbol,1}:
 :pk1_ll
 :pk1_lσ
 :pk2_ll_1
 :pk2_ll_2
 :pk2_lσ</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.map_column_pairs!-Tuple{AbstractArray{T,2} where T,Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.map_column_pairs!-Tuple{AbstractArray{T,2} where T,Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.map_column_pairs!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">map_column_pairs!(D::Matrix{Float64}, f, X::Matrix{Float64}[, Y::Matrix{Float64} = X])</code></pre><p>Like <a href="#GaussianProcesses.map_column_pairs-Tuple{Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>map_column_pairs</code></a>, but stores the result in <code>D</code> rather than a new matrix.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GaussianProcesses.map_column_pairs-Tuple{Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#GaussianProcesses.map_column_pairs-Tuple{Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>GaussianProcesses.map_column_pairs</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">map_column_pairs(f, X::Matrix{Float64}[, Y::Matrix{Float64} = X])</code></pre><p>Create a matrix by applying function <code>f</code> to each pair of columns of input matrices <code>X</code> and <code>Y</code>.</p></div></div></section><footer><hr/><a class="previous" href="../usage/"><span class="direction">Previous</span><span class="title">Usage</span></a><a class="next" href="../kernels/"><span class="direction">Next</span><span class="title">Kernels</span></a></footer></article></body></html>
