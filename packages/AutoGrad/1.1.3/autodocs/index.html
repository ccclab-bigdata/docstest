<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · AutoGrad.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>AutoGrad.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@diff" href="#AutoGrad.@diff"><code>AutoGrad.@diff</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@gcheck" href="#AutoGrad.@gcheck"><code>AutoGrad.@gcheck</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">gcheck(f, x...; kw, o...)
@gcheck f(x...; kw...) (opt1=val1,opt2=val2,...)</code></pre><p>Numerically check the gradient of <code>f(x...; kw...)</code> and return a boolean result.</p><p>Example call: <code>gcheck(nll,model,x,y)</code> or <code>@gcheck nll(model,x,y)</code>. The parameters should be marked as <code>Param</code> arrays in <code>f</code>, <code>x</code>, and/or <code>kw</code>.  Only 10 random entries in each large numeric array are checked by default.  If the output of <code>f</code> is not a number, we check the gradient of <code>sum(f(x...; kw...))</code>. Keyword arguments:</p><ul><li><code>kw=()</code>: keyword arguments to be passed to <code>f</code>, i.e. <code>f(x...; kw...)</code></li><li><code>nsample=10</code>: number of random entries from each param to check</li><li><code>atol=0.01,rtol=0.05</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</li><li><code>delta=0.0001</code>: step size for numerical gradient calculation.</li><li><code>verbose=1</code>: 0 prints nothing, 1 shows failing tests, 2 shows all tests.</li></ul></div></div></section><pre><code class="language-none">AutoGrad.@gs</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@primitive" href="#AutoGrad.@primitive"><code>AutoGrad.@primitive</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@primitive  fx g1 g2...</code></pre><p>Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as <code>sign</code>, and non-numeric functions such as <code>size</code> should be defined using the @zerograd macro instead.</p><p><strong>Examples</strong></p><pre><code class="language-none">@primitive sin(x::Number)
@primitive hypot(x1,x2),dy,y

@primitive sin(x::Number),dy  (dy.*cos(x))
@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)</code></pre><p>The first example shows that <code>fx</code> is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.</p><p>The second example specifies variable names for the output gradient <code>dy</code> and the output <code>y</code> after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in <code>f(a::Int,b,c...;d=1)</code>.  Parametric methods such as <code>f(x::T) where {T&lt;:Number}</code> cannot be used.</p><p>The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.</p><p><strong>Under the hood</strong></p><p>The @primitive macro turns the first example into:</p><pre><code class="language-none">sin(x::Value{T}) where {T&lt;:Number} = forw(sin, x)</code></pre><p>This will cause calls to <code>sin</code> with a boxed argument (<code>Value{T&lt;:Number}</code>) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:</p><pre><code class="language-none">hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)
hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)
hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)</code></pre><p>We want the forw method to be called if any one of the arguments is a boxed <code>Value</code>.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.</p><p>In AutoGrad, gradients are defined using gradient methods that have the following pattern:</p><pre><code class="language-none">back(f,Arg{i},dy,y,x...) =&gt; dx[i]</code></pre><p>For the third example here is the generated gradient method:</p><pre><code class="language-none">back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>For the last example a different gradient method is generated for each argument:</p><pre><code class="language-none">back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y
back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y</code></pre><p>In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.</p><p><strong>Broadcasting</strong></p><p>Broadcasting is handled by extra <code>forw</code> and <code>back</code> methods. <code>@primitive</code> defines the following  so that broadcasting of a primitive function with a boxed value triggers <code>forw</code> and <code>back</code>.</p><pre><code class="language-none">broadcasted(::typeof(sin), x::Value{T}) where {T&lt;:Number} = forw(broadcasted,sin,x)
back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>If you do not want the broadcasting methods, you can use the <code>@primitive1</code> macro. If you only want the broadcasting methods use <code>@primitive2</code>. As a motivating example, here is how <code>*</code> is defined for non-scalars:</p><pre><code class="language-none">@primitive1 *(x1,x2),dy  (dy*x2&#39;)  (x1&#39;*dy)
@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)</code></pre><p>Regular <code>*</code> is matrix multiplication, broadcasted <code>*</code> is elementwise multiplication and the two have different gradients as defined above. <code>unbroadcast(a,b)</code> reduces <code>b</code> to the same shape as <code>a</code> by performing the necessary summations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@primitive1" href="#AutoGrad.@primitive1"><code>AutoGrad.@primitive1</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@primitive  fx g1 g2...</code></pre><p>Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as <code>sign</code>, and non-numeric functions such as <code>size</code> should be defined using the @zerograd macro instead.</p><p><strong>Examples</strong></p><pre><code class="language-none">@primitive sin(x::Number)
@primitive hypot(x1,x2),dy,y

@primitive sin(x::Number),dy  (dy.*cos(x))
@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)</code></pre><p>The first example shows that <code>fx</code> is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.</p><p>The second example specifies variable names for the output gradient <code>dy</code> and the output <code>y</code> after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in <code>f(a::Int,b,c...;d=1)</code>.  Parametric methods such as <code>f(x::T) where {T&lt;:Number}</code> cannot be used.</p><p>The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.</p><p><strong>Under the hood</strong></p><p>The @primitive macro turns the first example into:</p><pre><code class="language-none">sin(x::Value{T}) where {T&lt;:Number} = forw(sin, x)</code></pre><p>This will cause calls to <code>sin</code> with a boxed argument (<code>Value{T&lt;:Number}</code>) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:</p><pre><code class="language-none">hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)
hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)
hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)</code></pre><p>We want the forw method to be called if any one of the arguments is a boxed <code>Value</code>.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.</p><p>In AutoGrad, gradients are defined using gradient methods that have the following pattern:</p><pre><code class="language-none">back(f,Arg{i},dy,y,x...) =&gt; dx[i]</code></pre><p>For the third example here is the generated gradient method:</p><pre><code class="language-none">back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>For the last example a different gradient method is generated for each argument:</p><pre><code class="language-none">back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y
back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y</code></pre><p>In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.</p><p><strong>Broadcasting</strong></p><p>Broadcasting is handled by extra <code>forw</code> and <code>back</code> methods. <code>@primitive</code> defines the following  so that broadcasting of a primitive function with a boxed value triggers <code>forw</code> and <code>back</code>.</p><pre><code class="language-none">broadcasted(::typeof(sin), x::Value{T}) where {T&lt;:Number} = forw(broadcasted,sin,x)
back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>If you do not want the broadcasting methods, you can use the <code>@primitive1</code> macro. If you only want the broadcasting methods use <code>@primitive2</code>. As a motivating example, here is how <code>*</code> is defined for non-scalars:</p><pre><code class="language-none">@primitive1 *(x1,x2),dy  (dy*x2&#39;)  (x1&#39;*dy)
@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)</code></pre><p>Regular <code>*</code> is matrix multiplication, broadcasted <code>*</code> is elementwise multiplication and the two have different gradients as defined above. <code>unbroadcast(a,b)</code> reduces <code>b</code> to the same shape as <code>a</code> by performing the necessary summations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@primitive2" href="#AutoGrad.@primitive2"><code>AutoGrad.@primitive2</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@primitive  fx g1 g2...</code></pre><p>Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as <code>sign</code>, and non-numeric functions such as <code>size</code> should be defined using the @zerograd macro instead.</p><p><strong>Examples</strong></p><pre><code class="language-none">@primitive sin(x::Number)
@primitive hypot(x1,x2),dy,y

@primitive sin(x::Number),dy  (dy.*cos(x))
@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)</code></pre><p>The first example shows that <code>fx</code> is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.</p><p>The second example specifies variable names for the output gradient <code>dy</code> and the output <code>y</code> after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in <code>f(a::Int,b,c...;d=1)</code>.  Parametric methods such as <code>f(x::T) where {T&lt;:Number}</code> cannot be used.</p><p>The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.</p><p><strong>Under the hood</strong></p><p>The @primitive macro turns the first example into:</p><pre><code class="language-none">sin(x::Value{T}) where {T&lt;:Number} = forw(sin, x)</code></pre><p>This will cause calls to <code>sin</code> with a boxed argument (<code>Value{T&lt;:Number}</code>) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:</p><pre><code class="language-none">hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)
hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)
hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)</code></pre><p>We want the forw method to be called if any one of the arguments is a boxed <code>Value</code>.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.</p><p>In AutoGrad, gradients are defined using gradient methods that have the following pattern:</p><pre><code class="language-none">back(f,Arg{i},dy,y,x...) =&gt; dx[i]</code></pre><p>For the third example here is the generated gradient method:</p><pre><code class="language-none">back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>For the last example a different gradient method is generated for each argument:</p><pre><code class="language-none">back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y
back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y</code></pre><p>In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.</p><p><strong>Broadcasting</strong></p><p>Broadcasting is handled by extra <code>forw</code> and <code>back</code> methods. <code>@primitive</code> defines the following  so that broadcasting of a primitive function with a boxed value triggers <code>forw</code> and <code>back</code>.</p><pre><code class="language-none">broadcasted(::typeof(sin), x::Value{T}) where {T&lt;:Number} = forw(broadcasted,sin,x)
back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T&lt;:Number} = dy .* cos(x)</code></pre><p>If you do not want the broadcasting methods, you can use the <code>@primitive1</code> macro. If you only want the broadcasting methods use <code>@primitive2</code>. As a motivating example, here is how <code>*</code> is defined for non-scalars:</p><pre><code class="language-none">@primitive1 *(x1,x2),dy  (dy*x2&#39;)  (x1&#39;*dy)
@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)</code></pre><p>Regular <code>*</code> is matrix multiplication, broadcasted <code>*</code> is elementwise multiplication and the two have different gradients as defined above. <code>unbroadcast(a,b)</code> reduces <code>b</code> to the same shape as <code>a</code> by performing the necessary summations.</p></div></div></section><pre><code class="language-none">AutoGrad.@timer</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@zerograd" href="#AutoGrad.@zerograd"><code>AutoGrad.@zerograd</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@zerograd f(args...; kwargs...)</code></pre><p>Define <code>f</code> as an AutoGrad primitive operation with zero gradient.</p><p><strong>Example:</strong></p><pre><code class="language-none">@zerograd  floor(x::Float32)</code></pre><p><code>@zerograd</code> allows <code>f</code> to handle boxed <code>Value</code> inputs by unboxing them like a <code>@primitive</code>, but unlike <code>@primitive</code> it does not record its actions or return a boxed <code>Value</code> result. Some functions, like <code>sign()</code>, have zero gradient.  Others, like <code>length()</code> have discrete or constant outputs.  These need to handle <code>Value</code> inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program. Use the <code>@zerograd</code> macro for those.  Use the <code>@zerograd1</code> variant if you don&#39;t want to define the broadcasting version and <code>@zerograd2</code> if you only want to define the broadcasting version. Note that <code>kwargs</code> are NOT unboxed.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@zerograd1" href="#AutoGrad.@zerograd1"><code>AutoGrad.@zerograd1</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@zerograd f(args...; kwargs...)</code></pre><p>Define <code>f</code> as an AutoGrad primitive operation with zero gradient.</p><p><strong>Example:</strong></p><pre><code class="language-none">@zerograd  floor(x::Float32)</code></pre><p><code>@zerograd</code> allows <code>f</code> to handle boxed <code>Value</code> inputs by unboxing them like a <code>@primitive</code>, but unlike <code>@primitive</code> it does not record its actions or return a boxed <code>Value</code> result. Some functions, like <code>sign()</code>, have zero gradient.  Others, like <code>length()</code> have discrete or constant outputs.  These need to handle <code>Value</code> inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program. Use the <code>@zerograd</code> macro for those.  Use the <code>@zerograd1</code> variant if you don&#39;t want to define the broadcasting version and <code>@zerograd2</code> if you only want to define the broadcasting version. Note that <code>kwargs</code> are NOT unboxed.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@zerograd2" href="#AutoGrad.@zerograd2"><code>AutoGrad.@zerograd2</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@zerograd f(args...; kwargs...)</code></pre><p>Define <code>f</code> as an AutoGrad primitive operation with zero gradient.</p><p><strong>Example:</strong></p><pre><code class="language-none">@zerograd  floor(x::Float32)</code></pre><p><code>@zerograd</code> allows <code>f</code> to handle boxed <code>Value</code> inputs by unboxing them like a <code>@primitive</code>, but unlike <code>@primitive</code> it does not record its actions or return a boxed <code>Value</code> result. Some functions, like <code>sign()</code>, have zero gradient.  Others, like <code>length()</code> have discrete or constant outputs.  These need to handle <code>Value</code> inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program. Use the <code>@zerograd</code> macro for those.  Use the <code>@zerograd1</code> variant if you don&#39;t want to define the broadcasting version and <code>@zerograd2</code> if you only want to define the broadcasting version. Note that <code>kwargs</code> are NOT unboxed.</p></div></div></section><pre><code class="language-none">AutoGrad.Arg</code></pre><pre><code class="language-none">AutoGrad.ArrayValue</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.AutoGrad" href="#AutoGrad.AutoGrad"><code>AutoGrad.AutoGrad</code></a> — <span class="docstring-category">Module</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><pre><code class="language-none">AutoGrad.Bcasted</code></pre><pre><code class="language-none">AutoGrad.NA</code></pre><pre><code class="language-none">AutoGrad.NAR</code></pre><pre><code class="language-none">AutoGrad.Node</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.Param" href="#AutoGrad.Param"><code>AutoGrad.Param</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><pre><code class="language-none">AutoGrad.Rec</code></pre><pre><code class="language-none">AutoGrad.Result</code></pre><pre><code class="language-none">AutoGrad.TIMER</code></pre><pre><code class="language-none">AutoGrad.Tape</code></pre><pre><code class="language-none">AutoGrad.Tracked</code></pre><pre><code class="language-none">AutoGrad.UngetIndex</code></pre><pre><code class="language-none">AutoGrad.Value</code></pre><pre><code class="language-none">AutoGrad._cat1d</code></pre><pre><code class="language-none">AutoGrad._kron</code></pre><pre><code class="language-none">AutoGrad._params_array_t</code></pre><pre><code class="language-none">AutoGrad._tapes</code></pre><pre><code class="language-none">AutoGrad._valstr</code></pre><pre><code class="language-none">AutoGrad.back</code></pre><pre><code class="language-none">AutoGrad.bsig</code></pre><pre><code class="language-none">AutoGrad.btimer</code></pre><pre><code class="language-none">AutoGrad.bzcall</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.cat1d" href="#AutoGrad.cat1d"><code>AutoGrad.cat1d</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cat1d(args...)</code></pre><p>Return <code>vcat(vec.(args)...)</code> but possibly more efficiently. Can be used to concatenate the contents of arrays with different shapes and sizes.</p></div></div></section><pre><code class="language-none">AutoGrad.chol_back</code></pre><pre><code class="language-none">AutoGrad.default_gc</code></pre><pre><code class="language-none">AutoGrad.differentiate</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.dir" href="#AutoGrad.dir"><code>AutoGrad.dir</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Use <code>AutoGrad.dir(path...)</code> to construct paths relative to AutoGrad root.</p></div></div></section><pre><code class="language-none">AutoGrad.dxndx</code></pre><pre><code class="language-none">AutoGrad.eval</code></pre><pre><code class="language-none">AutoGrad.f2b</code></pre><pre><code class="language-none">AutoGrad.fcall</code></pre><pre><code class="language-none">AutoGrad.findresult</code></pre><pre><code class="language-none">AutoGrad.forw</code></pre><pre><code class="language-none">AutoGrad.forwargs</code></pre><pre><code class="language-none">AutoGrad.fparse</code></pre><pre><code class="language-none">AutoGrad.fsigs</code></pre><pre><code class="language-none">AutoGrad.ftimer</code></pre><pre><code class="language-none">AutoGrad.full</code></pre><pre><code class="language-none">AutoGrad.gcget</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gcheck" href="#AutoGrad.gcheck"><code>AutoGrad.gcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">gcheck(f, x...; kw, o...)
@gcheck f(x...; kw...) (opt1=val1,opt2=val2,...)</code></pre><p>Numerically check the gradient of <code>f(x...; kw...)</code> and return a boolean result.</p><p>Example call: <code>gcheck(nll,model,x,y)</code> or <code>@gcheck nll(model,x,y)</code>. The parameters should be marked as <code>Param</code> arrays in <code>f</code>, <code>x</code>, and/or <code>kw</code>.  Only 10 random entries in each large numeric array are checked by default.  If the output of <code>f</code> is not a number, we check the gradient of <code>sum(f(x...; kw...))</code>. Keyword arguments:</p><ul><li><code>kw=()</code>: keyword arguments to be passed to <code>f</code>, i.e. <code>f(x...; kw...)</code></li><li><code>nsample=10</code>: number of random entries from each param to check</li><li><code>atol=0.01,rtol=0.05</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</li><li><code>delta=0.0001</code>: step size for numerical gradient calculation.</li><li><code>verbose=1</code>: 0 prints nothing, 1 shows failing tests, 2 shows all tests.</li></ul></div></div></section><pre><code class="language-none">AutoGrad.gcnode</code></pre><pre><code class="language-none">AutoGrad.gcsum</code></pre><pre><code class="language-none">AutoGrad.gcwalk</code></pre><pre><code class="language-none">AutoGrad.getval</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.grad" href="#AutoGrad.grad"><code>AutoGrad.grad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradcheck" href="#AutoGrad.gradcheck"><code>AutoGrad.gradcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">gradcheck(f, x...; kwargs...)</code></pre><p>Numerically check the gradient of <code>f(x...)</code> and return a boolean result.</p><p>Each argument can be a Number, Array, Tuple or Dict which in turn can contain other Arrays etc.  Only 10 random entries in each large numeric array are checked by default.  If the output of <code>f</code> is not a number, we check the gradient of <code>sum(f(x...))</code>. See also <code>gcheck</code> for a different take on marking parameters.</p><p><strong>Keywords</strong></p><ul><li><p><code>args=:</code>: the argument indices to check gradients with respect to. Could be an array or range of indices or a single index. By default all arguments that have a <code>length</code> method are checked.</p></li><li><p><code>kw=()</code>: keyword arguments to be passed to <code>f</code>.</p></li><li><p><code>nsample=10</code>: number of random entries from each numeric array in gradient <code>dw=(grad(f))(w,x...;o...)</code> compared to their numerical estimates.</p></li><li><p><code>atol=rtol=0.01</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</p></li><li><p><code>delta=0.0001</code>: step size for numerical gradient calculation.</p></li><li><p><code>verbose=1</code>: 0 prints nothing, 1 shows failing tests, 2 shows all tests.</p></li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradloss" href="#AutoGrad.gradloss"><code>AutoGrad.gradloss</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><pre><code class="language-none">AutoGrad.gsig</code></pre><pre><code class="language-none">AutoGrad.include</code></pre><pre><code class="language-none">AutoGrad.lq_back</code></pre><pre><code class="language-none">AutoGrad.normback</code></pre><pre><code class="language-none">AutoGrad.notypes</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.params" href="#AutoGrad.params"><code>AutoGrad.params</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><pre><code class="language-none">AutoGrad.params_internal</code></pre><pre><code class="language-none">AutoGrad.qr_back</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.randcheck" href="#AutoGrad.randcheck"><code>AutoGrad.randcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Test a numeric function with Float32/64 randn scalars and randn arrays, possibly transforming the input to match the domain</p></div></div></section><pre><code class="language-none">AutoGrad.recording</code></pre><pre><code class="language-none">AutoGrad.set_gc_function</code></pre><pre><code class="language-none">AutoGrad.sum_outgrads</code></pre><pre><code class="language-none">AutoGrad.sum_outgrads_array</code></pre><pre><code class="language-none">AutoGrad.sum_outgrads_single</code></pre><pre><code class="language-none">AutoGrad.svd_back</code></pre><pre><code class="language-none">AutoGrad.to</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.unbroadcast" href="#AutoGrad.unbroadcast"><code>AutoGrad.unbroadcast</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">unbroadcast(x,dx)</code></pre><p>Bring dx to x&#39;s size via unbroadcasting (reduction). This is needed when defining gradients of multi-argument broadcasting functions where the arguments and the result may be of different sizes.</p></div></div></section><pre><code class="language-none">AutoGrad.uncat</code></pre><pre><code class="language-none">AutoGrad.uncat1</code></pre><pre><code class="language-none">AutoGrad.ungetindex</code></pre><pre><code class="language-none">AutoGrad.v2b</code></pre><pre><code class="language-none">AutoGrad.valstr</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.value" href="#AutoGrad.value"><code>AutoGrad.value</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><pre><code class="language-none">AutoGrad.zcall</code></pre><pre><code class="language-none">AutoGrad.zeroslike</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
