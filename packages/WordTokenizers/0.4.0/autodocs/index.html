<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · WordTokenizers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>WordTokenizers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.MERGESYMBOL" href="#WordTokenizers.MERGESYMBOL"><code>WordTokenizers.MERGESYMBOL</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>A simple reversible tokenizer</p><pre><code class="language-none">tokenized = rev_tokenize(instring)
de_tokenized = rev_detokenize(token)</code></pre><p>The rev<em>tokenize tokenizer splits into token based on space, punctuations and special symbols and in  addition it leaves some merge-symbols (<code>&#39;&#39;</code>) for the tokens to be re-arranged when needed  using the rev</em>detokenize. It uses a character based approach for splitting and re-merging.</p><p>Parameters:</p><ul><li>instring: Input string to be tokenized </li><li>token: Collection to tokens i.e String Array</li></ul></div></div></section><pre><code class="language-none">WordTokenizers.Sentences</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.TokenBuffer" href="#WordTokenizers.TokenBuffer"><code>WordTokenizers.TokenBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">TokenBuffer(&quot;foo bar&quot;)</code></pre><p>Turns a string into a readable stream, used for building tokenisers. Utility parsers such as <code>spaces</code> and <code>number</code> read characters from the stream and into an array of tokens.</p><p>Parsers return <code>true</code> or <code>false</code> to indicate whether they matched anything in the input stream. They can therefore be combined easily, e.g.</p><pre><code class="language-none">spacesornumber(ts) = spaces(ts) || number(ts)</code></pre><p>either skips whitespace or parses a number token, if possible.</p><p>The simplest possible tokeniser accepts any <code>character</code> with no token breaks:</p><pre><code class="language-none">function tokenise(input)
  ts = TokenBuffer(input)
  while !isdone(ts)
    character(ts)
  end
  return ts.tokens
end

tokenise(&quot;foo bar baz&quot;) # [&quot;foo bar baz&quot;]</code></pre><p>The second simplest splits only on spaces:</p><pre><code class="language-none">function tokenise(input)
  ts = TokenBuffer(input)
  while !isdone(ts)
    spaces(ts) || character(ts)
  end
  return ts.tokens
end

tokenise(&quot;foo bar baz&quot;) # [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;]</code></pre><p>See <code>nltk_word_tokenize</code> for a more advanced example.</p></div></div></section><pre><code class="language-none">WordTokenizers.WordTokenizers</code></pre><pre><code class="language-none">WordTokenizers.Words</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.atoms" href="#WordTokenizers.atoms"><code>WordTokenizers.atoms</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">atoms(::TokenBuffer, [&quot;--&quot;, &quot;...&quot;, ...])</code></pre><p>Matches a set of atomic tokens, such as <code>...</code>, which should always be treated as a single token, regardless of word boundaries.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.character" href="#WordTokenizers.character"><code>WordTokenizers.character</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">character(::TokenBuffer)</code></pre><p>Push the next character in the input into the buffer&#39;s current token.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.closingquote" href="#WordTokenizers.closingquote"><code>WordTokenizers.closingquote</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">openquote(::TokenBuffer)</code></pre><p>Matches &quot; used as a closing quote, and tokenises it as &#39;&#39;.</p></div></div></section><pre><code class="language-none">WordTokenizers.eval</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.flush!" href="#WordTokenizers.flush!"><code>WordTokenizers.flush!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">flush!(::TokenBuffer, tokens...)</code></pre><p>TokenBuffer builds the current token as characters are read from the input. When the end of the current token is detected, call <code>flush!</code> to finish it and append it to the token stream. Optionally, give additional tokens to be added to the stream after the current one.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.generate_tokenizer_from_sed" href="#WordTokenizers.generate_tokenizer_from_sed"><code>WordTokenizers.generate_tokenizer_from_sed</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">generate_tokenizer_from_sed(sed_script_path)</code></pre><p>This returns Julia code, that is the translation of a simple sed script for tokenizing. This doesn&#39;t fully cover all the functionality of sed, but it covers enough for many purposes.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.improved_penn_tokenize" href="#WordTokenizers.improved_penn_tokenize"><code>WordTokenizers.improved_penn_tokenize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">improved_penn_tokenize(input::AbstractString)</code></pre><p>The Improved Penn Treebank tokenizer. This is a port of NLTK&#39;s modified Penn Tokenizer. It has a bundle of minor changes, that I don&#39;t think are actually documented anywhere. But things like <code>cannot cannot</code> become <code>can not can not</code> where as the original would produce <code>can not cannot</code>.</p><p>The tokenizer still seperates out contractions: &quot;shouldn&#39;t&quot; becomes [&quot;should&quot;, &quot;n&#39;t&quot;]</p><p>The input should be a single sentence; but again it will likely be relatively fine if it isn&#39;t. Depends exactly what you want it for.</p><p>This matches NLTK&#39;s <code>nltk.tokenize.TreeBankWordTokenizer.tokenize</code></p></div></div></section><pre><code class="language-none">WordTokenizers.include</code></pre><pre><code class="language-none">WordTokenizers.is_weird</code></pre><pre><code class="language-none">WordTokenizers.isdone</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.lookahead" href="#WordTokenizers.lookahead"><code>WordTokenizers.lookahead</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">lookahead(::TokenBuffer, s; boundary = false)</code></pre><p>Peek at the input to see if <code>s</code> is coming up next. <code>boundary</code> specifies whether a word boundary should follow <code>s</code>.</p><pre><code class="language-none">julia&gt; lookahead(TokenBuffer(&quot;foo bar&quot;), &quot;foo&quot;)
true

julia&gt; lookahead(TokenBuffer(&quot;foo bar&quot;), &quot;bar&quot;)
false

julia&gt; lookahead(TokenBuffer(&quot;foo bar&quot;), &quot;foo&quot;, boundary = true)
true

julia&gt; lookahead(TokenBuffer(&quot;foobar&quot;), &quot;foo&quot;, boundary = true)
false</code></pre></div></div></section><pre><code class="language-none">WordTokenizers.nltk_atoms</code></pre><pre><code class="language-none">WordTokenizers.nltk_splits</code></pre><pre><code class="language-none">WordTokenizers.nltk_suffixes</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.nltk_word_tokenize" href="#WordTokenizers.nltk_word_tokenize"><code>WordTokenizers.nltk_word_tokenize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">nltk_word_tokenize(input)</code></pre><p>NLTK&#39;s word tokenizer. It is an extension on the Punctuation Preserving Penn Treebank tokenizer, mostly to better handle unicode.</p><p>Punctuation is still preserved as its own token. This includes periods which will be stripped from words.</p><p>The tokenizer still seperates out contractions: &quot;shouldn&#39;t&quot; becomes [&quot;should&quot;, &quot;n&#39;t&quot;]</p><p>The input should be a single sentence; but again it will likely be relatively fine if it isn&#39;t. Depends exactly what you want it for.</p><p>This matches to the most commonly used <code>nltk.word_tokenize</code>, minus the sentence tokenizing step.</p></div></div></section><pre><code class="language-none">WordTokenizers.nth_ind</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.number" href="#WordTokenizers.number"><code>WordTokenizers.number</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">number(::TokenBuffer)</code></pre><p>Matches numbers such as <code>10,000.5</code>, preserving formatting.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.openquote" href="#WordTokenizers.openquote"><code>WordTokenizers.openquote</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">openquote(::TokenBuffer)</code></pre><p>Matches &quot; used as an opening quote, and tokenises it as ``.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.penn_tokenize" href="#WordTokenizers.penn_tokenize"><code>WordTokenizers.penn_tokenize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">penn_tokenize(input::AbstractString)</code></pre><p>&quot;... to produce Penn Treebank tokenization on arbitrary raw text. Yeah, sure&quot; quote Robert MacIntyre</p><p>Tokenization does a number of things like seperate out contractions: &quot;shouldn&#39;t&quot; becomes [&quot;should&quot;, &quot;n&#39;t&quot;] Most other punctuation becomes &amp;&#39;s. Exception is periods which are not touched. The input should be a single sentence; but it will likely be relatively fine if it isn&#39;t. Depends exactly what you want it for.</p><p>This is a direct (automatic) translation of the original sed script.</p><p>If you want to mess with exactly what it does it is actually really easy. copy the penn.sed file from this repo, modify it to your hearts content. There are some lines you can uncomment out. You can generate a new tokenizer using:</p><pre><code class="language-none">@generated function custom_tokenizer(input::AbstractString)
    generate_tokenizer_from_sed(joinpath(@__DIR__, &quot;custom.sed&quot;))
end</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.poormans_tokenize" href="#WordTokenizers.poormans_tokenize"><code>WordTokenizers.poormans_tokenize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">poormans_tokenize</code></pre><p>Tokenizes by removing punctuation and splitting on spaces</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.postproc_splits" href="#WordTokenizers.postproc_splits"><code>WordTokenizers.postproc_splits</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none"> postproc_splits(sentences)</code></pre><p>Applies heuristic rules to repair sentence splitting errors. Developed for use as postprocessing for the GENIA sentence splitter on PubMed abstracts, with minor tweaks for full-text documents.</p><p><code>sentences</code> should be a string, with line breaks on sentence boundaries. Returns a similar string, but more correct.</p><p>Based on https://github.com/ninjin/geniass/blob/master/geniass-postproc.pl Which is (c) 2010 Sampo Pyysalo. No rights reserved, i.e. do whatever you like with this. Which draws in part on heuristics included in Yoshimasa Tsuruoka&#39;s medss.pl script.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.punctuation_space_tokenize" href="#WordTokenizers.punctuation_space_tokenize"><code>WordTokenizers.punctuation_space_tokenize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">punctuation_space_tokenize</code></pre><p>Tokenizes by removing punctuation, unless it occurs inside of a word.</p></div></div></section><pre><code class="language-none">WordTokenizers.replace_til_no_change</code></pre><pre><code class="language-none">WordTokenizers.rev_detokenize</code></pre><pre><code class="language-none">WordTokenizers.rev_tokenize</code></pre><pre><code class="language-none">WordTokenizers.rulebased_split_sentences</code></pre><pre><code class="language-none">WordTokenizers.sentence_splitters</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.set_sentence_splitter" href="#WordTokenizers.set_sentence_splitter"><code>WordTokenizers.set_sentence_splitter</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">set_sentence_splitter(fun)</code></pre><p>Call this to set the default sentence splitter to invoke the passed in function <code>fun</code> It will be used by <code>split_sentences</code>. Calling this will trigger recompilation of any functions that use <code>split_sentences</code>.</p><p>Calling <code>set_sentence_splitter</code>  will give method overwritten warnings. They are expected, be worried if they do not occur</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.set_tokenizer" href="#WordTokenizers.set_tokenizer"><code>WordTokenizers.set_tokenizer</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">set_tokenizer(fun)</code></pre><p>Call this to set the default tokenizer to invoke the passed in function <code>fun</code> It will be used by <code>tokenize</code>. Calling this will trigger recompilation of any functions that use <code>tokenize</code>.</p><p>Calling <code>set_tokenizer</code>  will give method overwritten warnings. They are expected, be worried if they do not occur</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.spaces" href="#WordTokenizers.spaces"><code>WordTokenizers.spaces</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">spaces(::TokenBuffer)</code></pre><p>If there is whitespace in the input, skip it, and flush the current token.</p></div></div></section><pre><code class="language-none">WordTokenizers.split_sentences</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.splits" href="#WordTokenizers.splits"><code>WordTokenizers.splits</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">splits(::TokenBuffer, [(&quot;cannot&quot;, 3), (&quot;gimme&quot;, 3), ...])</code></pre><p>Matches tokens that should be split at the given index. For example, <code>cannot</code> would be split into <code>can</code> and <code>not</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="WordTokenizers.suffixes" href="#WordTokenizers.suffixes"><code>WordTokenizers.suffixes</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">suffixes(::TokenBuffer, [&quot;&#39;ll&quot;, &quot;&#39;re&quot;, ...])</code></pre><p>Matches tokens with suffixes, such as <code>you&#39;re</code>, that should be treated as separate tokens.</p></div></div></section><pre><code class="language-none">WordTokenizers.tokenize</code></pre><pre><code class="language-none">WordTokenizers.tokenizers</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
