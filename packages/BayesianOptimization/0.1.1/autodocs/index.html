<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · BayesianOptimization.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>BayesianOptimization.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">BayesianOptimization.@mytimeit</code></pre><pre><code class="language-none">BayesianOptimization.AbstractAcquisition</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.BOpt" href="#BayesianOptimization.BOpt"><code>BayesianOptimization.BOpt</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">BOpt(func, model, acquisition, modeloptimizer, lowerbounds, upperbounds;
          sense = Max, maxiterations = 10^4, maxduration = Inf,
          acquisitionoptions = NamedTuple(), repetitions = 1,
          verbosity = Progress, lhs_iterations = 5*length(lowerbounds))</code></pre></div></div></section><pre><code class="language-none">BayesianOptimization.BayesianOptimization</code></pre><pre><code class="language-none">BayesianOptimization.BetaScaling</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.BrochuBetaScaling" href="#BayesianOptimization.BrochuBetaScaling"><code>BayesianOptimization.BrochuBetaScaling</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Scales <code>βt</code> of <code>UpperConfidenceBound</code> as</p><pre><code class="language-none">βt = √(2 * log(t^(D/2 + 2) * π^2/(3δ)))</code></pre><p>where <code>t</code> is the number of observations, <code>D</code> is the dimensionality of the input data points and δ is a small constant (default δ = 0.1).</p><p>See Brochu E., Cora V. M., de Freitas N. (2010), &quot;A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning&quot;, https://arxiv.org/abs/1012.2599v1 page 16.</p></div></div></section><pre><code class="language-none">BayesianOptimization.DurationCounter</code></pre><pre><code class="language-none">BayesianOptimization.ENABLE_TIMINGS</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.ExpectedImprovement" href="#BayesianOptimization.ExpectedImprovement"><code>BayesianOptimization.ExpectedImprovement</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>The expected improvement measures the expected improvement <code>x - τ</code> of a point <code>x</code> upon an incumbent target <code>τ</code>. For Gaussian distributions it is given by</p><pre><code class="language-none">(μ(x) - τ) * ϕ[(μ(x) - τ)/σ(x)] + σ(x) * Φ[(μ(x) - τ)/σ(x)]</code></pre><p>where <code>ϕ</code> is the standard normal distribution function and <code>Φ</code> is the standard normal cumulative function, and <code>μ(x)</code>, <code>σ(x)</code> are mean and standard deviation of the distribution at point <code>x</code>.</p></div></div></section><pre><code class="language-none">BayesianOptimization.GP</code></pre><pre><code class="language-none">BayesianOptimization.IterationCounter</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.MLGPOptimizer" href="#BayesianOptimization.MLGPOptimizer"><code>BayesianOptimization.MLGPOptimizer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MLGPOptimizer(; every = 10, kwargs...)</code></pre><p>Set the GP hyperparameters to the maximum likelihood estimate <code>every</code> number of steps.</p></div></div></section><pre><code class="language-none">BayesianOptimization.Max</code></pre><pre><code class="language-none">BayesianOptimization.MaxMean</code></pre><pre><code class="language-none">BayesianOptimization.Min</code></pre><pre><code class="language-none">BayesianOptimization.ModelOptimizer</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.MutualInformation" href="#BayesianOptimization.MutualInformation"><code>BayesianOptimization.MutualInformation</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>The mutual information measures the amount of information gained by querying at x. The parameter γ̂ gives a lower bound for the information on f from the queries {x}. For a Gaussian this is     γ̂ = ∑σ²(x) and the mutual information at x is     μ(x) + √(α)*(√(σ²(x)+γ̂) - √(γ̂))</p><p>where <code>μ(x)</code>, <code>σ(x)</code> are mean and standard deviation of the distribution at point <code>x</code>.</p><p>See Contal E., Perchet V., Vayatis N. (2014), &quot;Gaussian Process Optimization with Mutual Information&quot; http://proceedings.mlr.press/v32/contal14.pdf</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.NoBetaScaling" href="#BayesianOptimization.NoBetaScaling"><code>BayesianOptimization.NoBetaScaling</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Applies no scaling to <code>βt</code> of <code>UpperConfidenceBound</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.NoModelOptimizer" href="#BayesianOptimization.NoModelOptimizer"><code>BayesianOptimization.NoModelOptimizer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Don&#39;t optimize the model ever.</p></div></div></section><pre><code class="language-none">BayesianOptimization.NoOptimizer</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.ProbabilityOfImprovement" href="#BayesianOptimization.ProbabilityOfImprovement"><code>BayesianOptimization.ProbabilityOfImprovement</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>The probability of improvement measures the probability that a point <code>x</code> leads to an improvement upon an incumbent target <code>τ</code>. For Gaussian distributions it is given by</p><pre><code class="language-none">Φ[(μ(x) - τ)/σ(x)]</code></pre><p>where <code>Φ</code> is the standard normal cumulative distribution function and <code>μ(x)</code>, <code>σ(x)</code> are mean and standard deviation of the distribution at point <code>x</code>.</p></div></div></section><pre><code class="language-none">BayesianOptimization.Progress</code></pre><pre><code class="language-none">BayesianOptimization.Sense</code></pre><pre><code class="language-none">BayesianOptimization.Silent</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.ThompsonSamplingSimple" href="#BayesianOptimization.ThompsonSamplingSimple"><code>BayesianOptimization.ThompsonSamplingSimple</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>The acquisition function associated with <code>ThompsonSamplingSimple</code> draws independent samples for each input <code>x</code> a function value from the model. Together with a gradient-free optimization method this leads to proposal points that might be similarly distributed as the maxima of true Thompson samples from GPs. True Thompson samples from a GP are simply functions from a GP. Maximizing these samples can be tricky, see e.g. http://hildobijl.com/Downloads/GPRT.pdf chapter 6.</p></div></div></section><pre><code class="language-none">BayesianOptimization.Timings</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.UpperConfidenceBound" href="#BayesianOptimization.UpperConfidenceBound"><code>BayesianOptimization.UpperConfidenceBound</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>For Gaussian distributions the upper confidence bound at <code>x</code> is given by     μ(x) + βt * σ(x)</p><p>where <code>βt</code> is a fixed parameter in the case of <code>NoBetaScaling</code> or an observation size dependent parameter in the case of e.g. <code>BrochuBetaScaling</code>.</p></div></div></section><pre><code class="language-none">BayesianOptimization.Verbosity</code></pre><pre><code class="language-none">BayesianOptimization.acquire_max</code></pre><pre><code class="language-none">BayesianOptimization.acquire_model_max</code></pre><pre><code class="language-none">BayesianOptimization.acquisitionfunction</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BayesianOptimization.boptimize!" href="#BayesianOptimization.boptimize!"><code>BayesianOptimization.boptimize!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">boptimize!(o::BOpt)</code></pre></div></div></section><pre><code class="language-none">BayesianOptimization.defaultoptions</code></pre><pre><code class="language-none">BayesianOptimization.dims</code></pre><pre><code class="language-none">BayesianOptimization.eval</code></pre><pre><code class="language-none">BayesianOptimization.include</code></pre><pre><code class="language-none">BayesianOptimization.init!</code></pre><pre><code class="language-none">BayesianOptimization.initialise_model!</code></pre><pre><code class="language-none">BayesianOptimization.isdone</code></pre><pre><code class="language-none">BayesianOptimization.latin_hypercube_sampling</code></pre><pre><code class="language-none">BayesianOptimization.maxy</code></pre><pre><code class="language-none">BayesianOptimization.mean_var</code></pre><pre><code class="language-none">BayesianOptimization.nlopt_setup</code></pre><pre><code class="language-none">BayesianOptimization.normal_cdf</code></pre><pre><code class="language-none">BayesianOptimization.normal_pdf</code></pre><pre><code class="language-none">BayesianOptimization.optimizemodel!</code></pre><pre><code class="language-none">BayesianOptimization.sample</code></pre><pre><code class="language-none">BayesianOptimization.setparams!</code></pre><pre><code class="language-none">BayesianOptimization.step!</code></pre><pre><code class="language-none">BayesianOptimization.update!</code></pre><pre><code class="language-none">BayesianOptimization.wrap_dummygradient</code></pre><pre><code class="language-none">BayesianOptimization.wrap_gradient</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
