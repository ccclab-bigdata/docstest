<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · NLPModels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NLPModels.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">NLPModels.@lencheck</code></pre><pre><code class="language-none">NLPModels.@rangecheck</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.ADNLPModel" href="#NLPModels.ADNLPModel"><code>NLPModels.ADNLPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>ADNLPModel is an AbstractNLPModel using ForwardDiff to compute the derivatives. In this interface, the objective function <span>$f$</span> and an initial estimate are required. If there are constraints, the function <span>$c:\mathbb{R}^n\rightarrow\mathbb{R}^m$</span>  and the vectors <span>$c_L$</span> and <span>$c_U$</span> also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided.</p><pre><code class="language-none">ADNLPModel(f, x0; lvar = [-∞,…,-∞], uvar = [∞,…,∞], y0 = zeros,
  c = NotImplemented, lcon = [-∞,…,-∞], ucon = [∞,…,∞], name = &quot;Generic&quot;)</code></pre><ul><li><code>f :: Function</code> - The objective function <span>$f$</span>;</li><li><code>x0 :: AbstractVector</code> - The initial point of the problem;</li><li><code>lvar :: AbstractVector</code> - <span>$\ell$</span>, the lower bound of the variables;</li><li><code>uvar :: AbstractVector</code> - <span>$u$</span>, the upper bound of the variables;</li><li><code>c :: Function</code> - The constraints function <span>$c$</span>;</li><li><code>y0 :: AbstractVector</code> - The initial value of the Lagrangian estimates;</li><li><code>lcon :: AbstractVector</code> - <span>$c_L$</span>, the lower bounds of the constraints function;</li><li><code>ucon :: AbstractVector</code> - <span>$c_U$</span>, the upper bounds of the constraints function;</li><li><code>name :: String</code> - A name for the model.</li></ul><p>The functions follow the same restrictions of ForwardDiff functions, summarised here:</p><ul><li>The function can only be composed of generic Julia functions;</li><li>The function must accept only one argument;</li><li>The function&#39;s argument must accept a subtype of AbstractVector;</li><li>The function should be type-stable.</li></ul><p>For contrained problems, the function <span>$c$</span> is required, and it must return an array even when m = 1, and <span>$c_L$</span> and <span>$c_U$</span> should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of <span>$c_L$</span> and <span>$c_U$</span> should be the same.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.ADNLSModel" href="#NLPModels.ADNLSModel"><code>NLPModels.ADNLSModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>ADNLSModel is an Nonlinear Least Squares model using ForwardDiff to compute the derivatives.</p><pre><code class="language-none">ADNLSModel(F, x0, m; lvar = [-∞,…,-∞], uvar = [∞,…,∞], y0 = zeros,
  c = NotImplemented, lcon = [-∞,…,-∞], ucon = [∞,…,∞], name = &quot;Generic&quot;)</code></pre><ul><li><code>F :: Function</code> - The residual function <span>$F$</span>;</li><li><code>x0 :: AbstractVector</code> - The initial point of the problem;</li><li><code>m :: Int</code> - The dimension of <span>$F(x)$</span>, i.e., the number of</li></ul><p>equations in the nonlinear system.</p><p>The other parameters are as in <code>ADNLPModel</code>.</p></div></div></section><pre><code class="language-none">NLPModels.AbstractNLPModel</code></pre><pre><code class="language-none">NLPModels.AbstractNLPModelMeta</code></pre><pre><code class="language-none">NLPModels.AbstractNLSModel</code></pre><pre><code class="language-none">NLPModels.Counters</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.FeasibilityFormNLS" href="#NLPModels.FeasibilityFormNLS"><code>NLPModels.FeasibilityFormNLS</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Converts a nonlinear least-squares problem with residual <code>F(x)</code> to a nonlinear optimization problem with constraints <code>F(x) = r</code> and objective <code>¹/₂‖r‖²</code>. In other words, converts</p><pre><code class="language-none">min ¹/₂‖F(x)‖²
s.t  cₗ ≤ c(x) ≤ cᵤ
      ℓ ≤   x  ≤ u</code></pre><p>to</p><pre><code class="language-none">min ¹/₂‖r‖²
s.t   F(x) - r = 0
     cₗ ≤ c(x) ≤ cᵤ
      ℓ ≤   x  ≤ u</code></pre><p>If you rather have the first problem, the <code>nls</code> model already works as an NLPModel of that format.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.FeasibilityResidual" href="#NLPModels.FeasibilityResidual"><code>NLPModels.FeasibilityResidual</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>A feasibility residual model is created from a NLPModel of the form</p><pre><code class="language-none">min f(x)
s.t cℓ ≤ c(x) ≤ cu
    bℓ ≤   x  ≤ bu</code></pre><p>by creating slack variables s and defining F(x,s) = c(x) - s. The resulting NLS problem is</p><pre><code class="language-none">min ¹/₂‖c(x) - s‖²
    bℓ ≤ x ≤ bu
    cℓ ≤ s ≤ bu</code></pre><p>This is done using SlackModel first, and then defining the NLS. Notice that if bℓᵢ = buᵢ, no slack variable is created.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.LBFGSModel" href="#NLPModels.LBFGSModel"><code>NLPModels.LBFGSModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Construct a <code>LBFGSModel</code> from another type of model.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.LLSModel" href="#NLPModels.LLSModel"><code>NLPModels.LLSModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">nls = LLSModel(A, b; lvar, uvar, C, lcon, ucon)</code></pre><p>Creates a Linear Least Squares model ½‖Ax - b‖² with optional bounds <code>lvar ≦ x ≦ y</code> and optional linear constraints <code>lcon ≦ Cx ≦ ucon</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.LSR1Model" href="#NLPModels.LSR1Model"><code>NLPModels.LSR1Model</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Construct a <code>LSR1Model</code> from another type of nlp.</p></div></div></section><pre><code class="language-none">NLPModels.NLPModelMeta</code></pre><pre><code class="language-none">NLPModels.NLPModels</code></pre><pre><code class="language-none">NLPModels.NLSCounters</code></pre><pre><code class="language-none">NLPModels.NLSMeta</code></pre><pre><code class="language-none">NLPModels.NotImplemented</code></pre><pre><code class="language-none">NLPModels.NotImplementedError</code></pre><pre><code class="language-none">NLPModels.QuasiNewtonModel</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.SimpleNLPModel" href="#NLPModels.SimpleNLPModel"><code>NLPModels.SimpleNLPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>SimpleNLPModel is an AbstractNLPModel that uses only user-defined functions. In this interface, the objective function <span>$f$</span> and an initial estimate are required. If the user wants to use derivatives, they need to be passed. The same goes for the Hessian and Hessian-AbstractVector product. For constraints, <span>$c:\mathbb{R}^n\rightarrow\mathbb{R}^m$</span>  and the vectors <span>$c_L$</span> and <span>$c_U$</span> also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided. The user can also pass the Jacobian and the Lagrangian Hessian and Hessian-AbstractVector product.</p><pre><code class="language-none">SimpleNLPModel(f, x0; lvar = [-∞,…,-∞], uvar = [∞,…,∞], y0=zeros,
  lcon = [-∞,…,-∞], ucon = [∞,…,∞], name = &quot;Generic&quot;,
  [list of functions])</code></pre><ul><li><code>f :: Function</code> - The objective function <span>$f$</span>;</li><li><code>x0 :: AbstractVector</code> - The initial point of the problem;</li><li><code>lvar :: AbstractVector</code> - <span>$\ell$</span>, the lower bound of the variables;</li><li><code>uvar :: AbstractVector</code> - <span>$u$</span>, the upper bound of the variables;</li><li><code>y0 :: AbstractVector</code> - The initial value of the Lagrangian estimates;</li><li><code>lcon :: AbstractVector</code> - <span>$c_L$</span>, the lower bounds of the constraints function;</li><li><code>ucon :: AbstractVector</code> - <span>$c_U$</span>, the upper bounds of the constraints function;</li><li><code>name :: String</code> - A name for the model.</li></ul><p>All functions passed have a direct correlation with a NLP function. You don&#39;t have to define any more than you need, but calling an undefined function will throw a <code>NotImplementedError</code>. The list is</p><ul><li><p><code>g</code> and <code>g!</code>: <span>$\nabla f(x)$</span>, the gradient of the objective function;</p><pre><code class="language-none">gx = g(x)
gx = g!(x, gx)</code></pre></li><li><p><code>H</code>: The lower triangle of the Hessian of the objective function or of the Lagrangian;</p><pre><code class="language-none">Hx = H(x; obj_weight=1.0) # if the problem is unconstrained
Hx = H(x; obj_weight=1.0, y=zeros) # if the problem is constrained</code></pre></li><li><p><code>Hcoord</code> - The lower triangle of the Hessian of the objective function or of the Lagrangian, in triplet format;</p><pre><code class="language-none">(rows,cols,vals) = Hcoord(x; obj_weight=1.0) # if the problem is unconstrained
(rows,cols,vals) = Hcoord(x; obj_weight=1.0, y=zeros) # if the problem is constrained</code></pre></li><li><p><code>Hp</code> and <code>Hp!</code> - The product of the Hessian of the objective function or of the Lagrangian by a vector;</p><pre><code class="language-none">Hv = Hp(x, v, obj_weight=1.0) # if the problem is unconstrained
Hv = Hp!(x, v, Hv, obj_weight=1.0) # if the problem is unconstrained
Hv = Hp(x, v, obj_weight=1.0, y=zeros) # if the problem is constrained
Hv = Hp!(x, v, Hv, obj_weight=1.0, y=zeros) # if the problem is constrained</code></pre></li><li><p><code>c</code> and <code>c!</code> - <span>$c(x)$</span>, the constraints function;</p><pre><code class="language-none">cx = c(x)
cx = c!(x, cx)</code></pre></li><li><p><code>J</code> - <span>$J(x)$</span>, the Jacobian of the constraints;</p><pre><code class="language-none">Jx = J(x)</code></pre></li><li><p><code>Jcoord</code> - <span>$J(x)$</span>, the Jacobian of the constraints, in triplet format;</p><pre><code class="language-none">(rows,cols,vals) = Jcoord(x)</code></pre></li><li><p><code>Jp</code> and <code>Jp!</code> - The Jacobian-vector product;</p><pre><code class="language-none">Jv = Jp(x, v)
Jv = Jp!(x, v, Jv)</code></pre></li><li><p><code>Jtp</code> and <code>Jtp!</code> - The Jacobian-transposed-vector product;</p><pre><code class="language-none">Jtv = Jtp(x, v)
Jtv = Jtp!(x, v, Jtv)</code></pre></li></ul><p>For contrained problems, the function <span>$c$</span> is required, and it must return an array even when m = 1, and <span>$c_L$</span> and <span>$c_U$</span> should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of <span>$c_L$</span> and <span>$c_U$</span> should be the same.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.SimpleNLSModel" href="#NLPModels.SimpleNLSModel"><code>NLPModels.SimpleNLSModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">nls = SimpleNLSModel(n;  F=F, F! =F!, JF=JF, JFp=JFp, JFp! =JFp!,
JFtp=JFtp, JFtp! =JFtp!)
nls = SimpleNLSModel(x0; F=F, F! =F!, JF=JF, JFp=JFp, JFp! =JFp!,
JFtp=JFtp, JFtp! =JFtp!)</code></pre><p>Creates a Nonlinear Linear Least Squares model to minimize ‖F(x)‖². If JF = JF(x) is passed, the Jacobian is available.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.SlackModel" href="#NLPModels.SlackModel"><code>NLPModels.SlackModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>A model whose only inequality constraints are bounds.</p><p>Given a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form</p><p>\[ \min f(x)  \mbox{ s. t. }  c<em>L \leq c(x) \leq c</em>U \mbox{ and } \ell \leq x \leq u, \]</p><p>the new model appears to the user as</p><p>\[ \min f(X)  \mbox{ s. t. }  g(X) = 0 \mbox{ and } L \leq X \leq U. \]</p><p>The unknowns <span>$X = (x, s)$</span> contain the original variables and slack variables <span>$s$</span>. The latter are such that the new model has the general form</p><p>\[ \min f(x)  \mbox{ s. t. }  c(x) - s = 0, c<em>L \leq s \leq c</em>U \mbox{ and } \ell \leq x \leq u, \]</p><p>although no slack variables are introduced for equality constraints.</p><p>The slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where <code>low</code>, <code>upp</code> and <code>rng</code> represent the indices of the constraints of the form <span>$c_L \leq c(x) &lt; \infty$</span>, <span>$-\infty &lt; c(x) \leq c_U$</span> and <span>$c_L \leq c(x) \leq c_U$</span>, respectively.</p></div></div></section><pre><code class="language-none">NLPModels.SlackModels</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.SlackNLSModel" href="#NLPModels.SlackNLSModel"><code>NLPModels.SlackNLSModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Like <code>SlackModel</code>, this model converts inequalities into equalities and bounds.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.bound_constrained" href="#NLPModels.bound_constrained"><code>NLPModels.bound_constrained</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">bound_constrained(nlp)
bound_constrained(meta)</code></pre><p>Returns whether the problem has bounds on the variables and no other constraints.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.cons" href="#NLPModels.cons"><code>NLPModels.cons</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>c = cons(nlp, x)</code></p><p>Evaluate <span>$c(x)$</span>, the constraints at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.cons!" href="#NLPModels.cons!"><code>NLPModels.cons!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>c = cons!(nlp, x, c)</code></p><p>Evaluate <span>$c(x)$</span>, the constraints at <code>x</code> in place.</p></div></div></section><pre><code class="language-none">NLPModels.conscale</code></pre><pre><code class="language-none">NLPModels.counters</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.equality_constrained" href="#NLPModels.equality_constrained"><code>NLPModels.equality_constrained</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">equality_constrained(nlp)
equality_constrained(meta)</code></pre><p>Returns whether the problem&#39;s constraints are all equalities. Unconstrained problems return false.</p></div></div></section><pre><code class="language-none">NLPModels.eval</code></pre><pre><code class="language-none">NLPModels.ghjvprod</code></pre><pre><code class="language-none">NLPModels.ghjvprod!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.grad" href="#NLPModels.grad"><code>NLPModels.grad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>g = grad(nlp, x)</code></p><p>Evaluate <span>$\nabla f(x)$</span>, the gradient of the objective function at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.grad!" href="#NLPModels.grad!"><code>NLPModels.grad!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>g = grad!(nlp, x, g)</code></p><p>Evaluate <span>$\nabla f(x)$</span>, the gradient of the objective function at <code>x</code> in place.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.gradient_check" href="#NLPModels.gradient_check"><code>NLPModels.gradient_check</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Check the first derivatives of the objective at <code>x</code> against centered finite differences.</p><p>This function returns a dictionary indexed by components of the gradient for which the relative error exceeds <code>rtol</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.has_bounds" href="#NLPModels.has_bounds"><code>NLPModels.has_bounds</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">has_bounds(nlp)
has_bounds(meta)</code></pre><p>Returns whether the problem has bounds on the variables.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess" href="#NLPModels.hess"><code>NLPModels.hess</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Hx = hess(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the Lagrangian Hessian at <code>(x,y)</code> as a sparse matrix, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight. Only the lower triangle is returned.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_coord" href="#NLPModels.hess_coord"><code>NLPModels.hess_coord</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>(rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the Lagrangian Hessian at <code>(x,y)</code> in sparse coordinate format, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight. Only the lower triangle is returned.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_coord_residual" href="#NLPModels.hess_coord_residual"><code>NLPModels.hess_coord_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(rows,cols,vals) = hess_coord_residual(nls, x, v)</code></pre><p>Computes the linear combination of the Hessians of the residuals at <code>x</code> with coefficients <code>v</code> in sparse coordinate format.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op" href="#NLPModels.hess_op"><code>NLPModels.hess_op</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>H = hess_op(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Return the Lagrangian Hessian at <code>(x,y)</code> with objective function scaled by <code>obj_weight</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>H * v</code>. The linear operator H represents</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op!" href="#NLPModels.hess_op!"><code>NLPModels.hess_op!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>H = hess_op!(nlp, x, Hv; obj_weight=1.0, y=zeros)</code></p><p>Return the Lagrangian Hessian at <code>(x,y)</code> with objective function scaled by <code>obj_weight</code> as a linear operator, and storing the result on <code>Hv</code>. The resulting object may be used as if it were a matrix, e.g., <code>w = H * v</code>. The vector <code>Hv</code> is used as preallocated storage for the operation.  The linear operator H represents</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op_residual" href="#NLPModels.hess_op_residual"><code>NLPModels.hess_op_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Hop = hess_op_residual(nls, x, i)</code></pre><p>Computes the Hessian of the i-th residual at x, in linear operator form.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op_residual!" href="#NLPModels.hess_op_residual!"><code>NLPModels.hess_op_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Hop = hess_op_residual!(nls, x, i, Hiv)</code></pre><p>Computes the Hessian of the i-th residual at x, in linear operator form. The vector <code>Hiv</code> is used as preallocated storage for the operation.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_residual" href="#NLPModels.hess_residual"><code>NLPModels.hess_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">H = hess_residual(nls, x, v)</code></pre><p>Computes the linear combination of the Hessians of the residuals at <code>x</code> with coefficients <code>v</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hessian_check" href="#NLPModels.hessian_check"><code>NLPModels.hessian_check</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Check the second derivatives of the objective and each constraints at <code>x</code> against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.</p><p>The <code>sgn</code> arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) + ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>JuMPNLPModel</code>s, and a negative value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) - ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>AmplModel</code>s. Only the sign of <code>sgn</code> is important.</p><p>This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k &gt; 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds <code>rtol</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hessian_check_from_grad" href="#NLPModels.hessian_check_from_grad"><code>NLPModels.hessian_check_from_grad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Check the second derivatives of the objective and each constraints at <code>x</code> against centered finite differences. This check assumes exactness of the first derivatives.</p><p>The <code>sgn</code> arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) + ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>JuMPNLPModel</code>s, and a negative value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) - ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>AmplModel</code>s. Only the sign of <code>sgn</code> is important.</p><p>This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k &gt; 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds <code>rtol</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod" href="#NLPModels.hprod"><code>NLPModels.hprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Hv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the product of the Lagrangian Hessian at <code>(x,y)</code> with the vector <code>v</code>, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod!" href="#NLPModels.hprod!"><code>NLPModels.hprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Hv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the product of the Lagrangian Hessian at <code>(x,y)</code> with the vector <code>v</code> in place, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum<em>{i=1}^m y</em>i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod_residual" href="#NLPModels.hprod_residual"><code>NLPModels.hprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Hiv = hprod_residual(nls, x, i, v)</code></pre><p>Computes the product of the Hessian of the i-th residual at x, times the vector v.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod_residual!" href="#NLPModels.hprod_residual!"><code>NLPModels.hprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Hiv = hprod_residual!(nls, x, i, v, Hiv)</code></pre><p>Computes the product of the Hessian of the i-th residual at x, times the vector v, and stores it in vector Hiv.</p></div></div></section><pre><code class="language-none">NLPModels.include</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.increment!" href="#NLPModels.increment!"><code>NLPModels.increment!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>increment!(nlp, s)</code></p><p>Increment counter <code>s</code> of problem <code>nlp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.inequality_constrained" href="#NLPModels.inequality_constrained"><code>NLPModels.inequality_constrained</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">inequality_constrained(nlp)
inequality_constrained(meta)</code></pre><p>Returns whether the problem&#39;s constraints are all inequalities. Unconstrained problems return true.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac" href="#NLPModels.jac"><code>NLPModels.jac</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Jx = jac(nlp, x)</code></p><p>Evaluate <span>$\nabla c(x)$</span>, the constraint&#39;s Jacobian at <code>x</code> as a sparse matrix.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_coord" href="#NLPModels.jac_coord"><code>NLPModels.jac_coord</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>(rows,cols,vals) = jac_coord(nlp, x)</code></p><p>Evaluate <span>$\nabla c(x)$</span>, the constraint&#39;s Jacobian at <code>x</code> in sparse coordinate format.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_coord_residual" href="#NLPModels.jac_coord_residual"><code>NLPModels.jac_coord_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(rows,cols,vals) = jac_coord_residual(nls, x)</code></pre><p>Computes the Jacobian of the residual at <code>x</code> in sparse coordinate format.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op" href="#NLPModels.jac_op"><code>NLPModels.jac_op</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>J = jac_op(nlp, x)</code></p><p>Return the Jacobian at <code>x</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>J * v</code> or <code>J&#39; * v</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op!" href="#NLPModels.jac_op!"><code>NLPModels.jac_op!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>J = jac_op!(nlp, x, Jv, Jtv)</code></p><p>Return the Jacobian at <code>x</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>J * v</code> or <code>J&#39; * v</code>. The values <code>Jv</code> and <code>Jtv</code> are used as preallocated storage for the operations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op_residual" href="#NLPModels.jac_op_residual"><code>NLPModels.jac_op_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jx = jac_op_residual(nls, x)</code></pre><p>Computes J(x), the Jacobian of the residual at x, in linear operator form.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op_residual!" href="#NLPModels.jac_op_residual!"><code>NLPModels.jac_op_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jx = jac_op_residual!(nls, x, Jv, Jtv)</code></pre><p>Computes J(x), the Jacobian of the residual at x, in linear operator form. The vectors <code>Jv</code> and <code>Jtv</code> are used as preallocated storage for the operations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_residual" href="#NLPModels.jac_residual"><code>NLPModels.jac_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jx = jac_residual(nls, x)</code></pre><p>Computes J(x), the Jacobian of the residual at x.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jacobian_check" href="#NLPModels.jacobian_check"><code>NLPModels.jacobian_check</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Check the first derivatives of the constraints at <code>x</code> against centered finite differences.</p><p>This function returns a dictionary indexed by (j, i) tuples such that the relative error in the <code>i</code>-th partial derivative of the <code>j</code>-th constraint exceeds <code>rtol</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod" href="#NLPModels.jprod"><code>NLPModels.jprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Jv = jprod(nlp, x, v)</code></p><p>Evaluate <span>$\nabla c(x)v$</span>, the Jacobian-vector product at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod!" href="#NLPModels.jprod!"><code>NLPModels.jprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Jv = jprod!(nlp, x, v, Jv)</code></p><p>Evaluate <span>$\nabla c(x)v$</span>, the Jacobian-vector product at <code>x</code> in place.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod_residual" href="#NLPModels.jprod_residual"><code>NLPModels.jprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jv = jprod_residual(nls, x, v)</code></pre><p>Computes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)*v.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod_residual!" href="#NLPModels.jprod_residual!"><code>NLPModels.jprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jv = jprod_residual!(nls, x, v, Jv)</code></pre><p>Computes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)*v, storing it in <code>Jv</code>.</p></div></div></section><pre><code class="language-none">NLPModels.jth_con</code></pre><pre><code class="language-none">NLPModels.jth_congrad</code></pre><pre><code class="language-none">NLPModels.jth_congrad!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jth_hess_residual" href="#NLPModels.jth_hess_residual"><code>NLPModels.jth_hess_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Hj = jth_hess_residual(nls, x, j)</code></pre><p>Computes the Hessian of the j-th residual at x.</p></div></div></section><pre><code class="language-none">NLPModels.jth_hprod</code></pre><pre><code class="language-none">NLPModels.jth_hprod!</code></pre><pre><code class="language-none">NLPModels.jth_sparse_congrad</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod" href="#NLPModels.jtprod"><code>NLPModels.jtprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Jtv = jtprod(nlp, x, v, Jtv)</code></p><p>Evaluate <span>$\nabla c(x)^Tv$</span>, the transposed-Jacobian-vector product at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod!" href="#NLPModels.jtprod!"><code>NLPModels.jtprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>Jtv = jtprod!(nlp, x, v, Jtv)</code></p><p>Evaluate <span>$\nabla c(x)^Tv$</span>, the transposed-Jacobian-vector product at <code>x</code> in place.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod_residual" href="#NLPModels.jtprod_residual"><code>NLPModels.jtprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jtv = jtprod_residual(nls, x, v)</code></pre><p>Computes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)&#39;*v.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod_residual!" href="#NLPModels.jtprod_residual!"><code>NLPModels.jtprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Jtv = jtprod_residual!(nls, x, v, Jtv)</code></pre><p>Computes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)&#39;*v, storing it in <code>Jtv</code>.</p></div></div></section><pre><code class="language-none">NLPModels.lagscale</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.linearly_constrained" href="#NLPModels.linearly_constrained"><code>NLPModels.linearly_constrained</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">linearly_constrained(nlp)
linearly_constrained(meta)</code></pre><p>Returns whether the problem&#39;s constraints are known to be all linear.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_cons" href="#NLPModels.neval_cons"><code>NLPModels.neval_cons</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_cons(nlp)</code></p><p>Get the number of <code>cons</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_grad" href="#NLPModels.neval_grad"><code>NLPModels.neval_grad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_grad(nlp)</code></p><p>Get the number of <code>grad</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_hess" href="#NLPModels.neval_hess"><code>NLPModels.neval_hess</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_hess(nlp)</code></p><p>Get the number of <code>hess</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_hess_residual" href="#NLPModels.neval_hess_residual"><code>NLPModels.neval_hess_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_hess_residual(nlp)</code></p><p>Get the number of <code>hess</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_hprod" href="#NLPModels.neval_hprod"><code>NLPModels.neval_hprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_hprod(nlp)</code></p><p>Get the number of <code>hprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_hprod_residual" href="#NLPModels.neval_hprod_residual"><code>NLPModels.neval_hprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_hprod_residual(nlp)</code></p><p>Get the number of <code>hprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jac" href="#NLPModels.neval_jac"><code>NLPModels.neval_jac</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jac(nlp)</code></p><p>Get the number of <code>jac</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jac_residual" href="#NLPModels.neval_jac_residual"><code>NLPModels.neval_jac_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jac_residual(nlp)</code></p><p>Get the number of <code>jac</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jcon" href="#NLPModels.neval_jcon"><code>NLPModels.neval_jcon</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jcon(nlp)</code></p><p>Get the number of <code>jcon</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jgrad" href="#NLPModels.neval_jgrad"><code>NLPModels.neval_jgrad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jgrad(nlp)</code></p><p>Get the number of <code>jgrad</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jhess_residual" href="#NLPModels.neval_jhess_residual"><code>NLPModels.neval_jhess_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jhess_residual(nlp)</code></p><p>Get the number of <code>jhess</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jhprod" href="#NLPModels.neval_jhprod"><code>NLPModels.neval_jhprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jhprod(nlp)</code></p><p>Get the number of <code>jhprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jprod" href="#NLPModels.neval_jprod"><code>NLPModels.neval_jprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jprod(nlp)</code></p><p>Get the number of <code>jprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jprod_residual" href="#NLPModels.neval_jprod_residual"><code>NLPModels.neval_jprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jprod_residual(nlp)</code></p><p>Get the number of <code>jprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jtprod" href="#NLPModels.neval_jtprod"><code>NLPModels.neval_jtprod</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jtprod(nlp)</code></p><p>Get the number of <code>jtprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_jtprod_residual" href="#NLPModels.neval_jtprod_residual"><code>NLPModels.neval_jtprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_jtprod_residual(nlp)</code></p><p>Get the number of <code>jtprod</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_obj" href="#NLPModels.neval_obj"><code>NLPModels.neval_obj</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_obj(nlp)</code></p><p>Get the number of <code>obj</code> evaluations.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.neval_residual" href="#NLPModels.neval_residual"><code>NLPModels.neval_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>NLPModels.neval_residual(nlp)</code></p><p>Get the number of <code>residual</code> evaluations.</p></div></div></section><pre><code class="language-none">NLPModels.nls_meta</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.obj" href="#NLPModels.obj"><code>NLPModels.obj</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>f = obj(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span>, the objective function of <code>nlp</code> at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objcons" href="#NLPModels.objcons"><code>NLPModels.objcons</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>f, c = objcons(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$c(x)$</span> at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objcons!" href="#NLPModels.objcons!"><code>NLPModels.objcons!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>f = objcons!(nlp, x, c)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$c(x)$</span> at <code>x</code>. <code>c</code> is overwritten with the value of <span>$c(x)$</span>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objgrad" href="#NLPModels.objgrad"><code>NLPModels.objgrad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>f, g = objgrad(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$\nabla f(x)$</span> at <code>x</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objgrad!" href="#NLPModels.objgrad!"><code>NLPModels.objgrad!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>f, g = objgrad!(nlp, x, g)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$\nabla f(x)$</span> at <code>x</code>. <code>g</code> is overwritten with the value of <span>$\nabla f(x)$</span>.</p></div></div></section><pre><code class="language-none">NLPModels.push!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LinearOperators.reset!" href="#LinearOperators.reset!"><code>LinearOperators.reset!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>reset!(counters)</code></p><p>Reset evaluation counters</p></div></div><div><div><p>`reset!(nlp)</p><p>Reset evaluation count in <code>nlp</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.residual" href="#NLPModels.residual"><code>NLPModels.residual</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Fx = residual(nls, x)</code></pre><p>Computes F(x), the residual at x.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.residual!" href="#NLPModels.residual!"><code>NLPModels.residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Fx = residual!(nls, x, Fx)</code></pre><p>Computes F(x), the residual at x.</p></div></div></section><pre><code class="language-none">NLPModels.slack_meta</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.sum_counters" href="#NLPModels.sum_counters"><code>NLPModels.sum_counters</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>sum_counters(counters)</code></p><p>Sum all counters of <code>counters</code>.</p></div></div><div><div><p><code>sum_counters(nlp)</code></p><p>Sum all counters of problem <code>nlp</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.unconstrained" href="#NLPModels.unconstrained"><code>NLPModels.unconstrained</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">unconstrained(nlp)
unconstrained(meta)</code></pre><p>Returns whether the problem in unconstrained.</p></div></div></section><pre><code class="language-none">NLPModels.varscale</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
