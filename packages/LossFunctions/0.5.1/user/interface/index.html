<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Working with Losses · LossFunctions.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><h1>LossFunctions.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><span class="toctext">Introduction</span><ul><li><a class="toctext" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li class="current"><a class="toctext" href>Working with Losses</a><ul class="internal"><li><a class="toctext" href="#Instantiating-a-Loss-1">Instantiating a Loss</a></li><li><a class="toctext" href="#Computing-the-Values-1">Computing the Values</a></li><li><a class="toctext" href="#Computing-the-1st-Derivatives-1">Computing the 1st Derivatives</a></li><li><a class="toctext" href="#Computing-the-2nd-Derivatives-1">Computing the 2nd Derivatives</a></li><li><a class="toctext" href="#Function-Closures-1">Function Closures</a></li><li><a class="toctext" href="#Properties-of-a-Loss-1">Properties of a Loss</a></li></ul></li><li><a class="toctext" href="../aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="toctext">Available Losses</span><ul><li><a class="toctext" href="../../losses/distance/">Distance-based Losses</a></li><li><a class="toctext" href="../../losses/margin/">Margin-based Losses</a></li></ul></li><li><span class="toctext">Advances Topics</span><ul><li><a class="toctext" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="toctext" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="toctext" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>User&#39;s Guide</li><li><a href>Working with Losses</a></li></ul></nav><hr/><div id="topbar"><span>Working with Losses</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Working-with-Losses-1" href="#Working-with-Losses-1">Working with Losses</a></h1><p>Even though they are called loss &quot;functions&quot;, this package implements them as immutable types instead of true Julia functions. There are good reasons for that. For example it allows us to specify the properties of losse functions explicitly (e.g. <code>isconvex(myloss)</code>). It also makes for a more consistent API when it comes to computing the value or the derivative. Some loss functions even have additional parameters that need to be specified, such as the <span>$\epsilon$</span> in the case of the <span>$\epsilon$</span>-insensitive loss. Here, types allow for member variables to hide that information away from the method signatures.</p><p>In order to avoid potential confusions with true Julia functions, we will refer to &quot;loss functions&quot; as &quot;losses&quot; instead. The available losses share a common interface for the most part. This section will provide an overview of the basic functionality that is available for all the different types of losses. We will discuss how to create a loss, how to compute its value and derivative, and how to query its properties.</p><h2><a class="nav-anchor" id="Instantiating-a-Loss-1" href="#Instantiating-a-Loss-1">Instantiating a Loss</a></h2><p>Losses are immutable types. As such, one has to instantiate one in order to work with it. For most losses, the constructors do not expect any parameters.</p><pre><code class="language-julia-repl">julia&gt; L2DistLoss()
LPDistLoss{2}()

julia&gt; HingeLoss()
L1HingeLoss()</code></pre><p>We just said that we need to instantiate a loss in order to work with it. One could be inclined to belief, that it would be more memory-efficient to &quot;pre-allocate&quot; a loss when using it in more than one place.</p><pre><code class="language-julia-repl">julia&gt; loss = L2DistLoss()
LPDistLoss{2}()

julia&gt; value(loss, 2, 3)
1</code></pre><p>However, that is a common oversimplification. Because all losses are immutable types, they can live on the stack and thus do not come with a heap-allocation overhead.</p><p>Even more interesting in the example above, is that for such losses as <a href="../../losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>, which do not have any constructor parameters or member variables, there is no additional code executed at all. Such singletons are only used for dispatch and don&#39;t even produce any additional code, which you can observe for yourself in the code below. As such they are zero-cost abstractions.</p><pre><code class="language-julia-repl">julia&gt; v1(loss,t,y) = value(loss,t,y)

julia&gt; v2(t,y) = value(L2DistLoss(),t,y)

julia&gt; @code_llvm v1(loss, 2, 3)
define i64 @julia_v1_70944(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}

julia&gt; @code_llvm v2(2, 3)
define i64 @julia_v2_70949(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}</code></pre><p>On the other hand, some types of losses are actually more comparable to whole families of losses instead of just a single one. For example, the immutable type <a href="../../losses/distance/#LossFunctions.L1EpsilonInsLoss"><code>L1EpsilonInsLoss</code></a> has a free parameter <span>$\epsilon$</span>. Each concrete <span>$\epsilon$</span> results in a different concrete loss of the same family of epsilon-insensitive losses.</p><pre><code class="language-julia-repl">julia&gt; L1EpsilonInsLoss(0.5)
L1EpsilonInsLoss{Float64}(0.5)

julia&gt; L1EpsilonInsLoss(1)
L1EpsilonInsLoss{Float64}(1.0)</code></pre><p>For such losses that do have parameters, it can make a slight difference to pre-instantiate a loss. While they will live on the stack, the constructor usually performs some assertions and conversion for the given parameter. This can come at a slight overhead. At the very least it will not produce the same exact code when pre-instantiated. Still, the fact that they are immutable makes them very efficient abstractions with little to no performance overhead, and zero memory allocations on the heap.</p><h2><a class="nav-anchor" id="Computing-the-Values-1" href="#Computing-the-Values-1">Computing the Values</a></h2><p>The first thing we may want to do is compute the loss for some observation (singular). In fact, all losses are implemented on single observations under the hood. The core function to compute the value of a loss is <code>value</code>. We will see throughout the documentation that this function allows for a lot of different method signatures to accomplish a variety of tasks.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.value-Tuple{SupervisedLoss,Number,Number}" href="#LearnBase.value-Tuple{SupervisedLoss,Number,Number}"><code>LearnBase.value</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">value(loss, target::Number, output::Number) -&gt; Number</code></pre><p>Compute the (non-negative) numeric result for the loss-function denoted by the parameter <code>loss</code> and return it. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><div>\[L : Y \times \mathbb{R} \rightarrow [0,\infty)\]</div><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we want to compute the value with.</p></li><li><p><code>target::Number</code>: The ground truth <span>$y \in Y$</span> of the observation.</p></li><li><p><code>output::Number</code>: The predicted output <span>$\hat{y} \in \mathbb{R}$</span> for the observation.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">#               loss        y    ŷ
julia&gt; value(L1DistLoss(), 1.0, 2.0)
1.0

julia&gt; value(L1DistLoss(), 1, 2)
1

julia&gt; value(L1HingeLoss(), -1, 2)
3

julia&gt; value(L1HingeLoss(), -1f0, 2f0)
3.0f0</code></pre></div></section><p>It may be interesting to note, that this function also supports broadcasting and all the syntax benefits that come with it. Thus, it is quite simple to make use of preallocated memory for storing the element-wise results.</p><pre><code class="language-julia-repl">julia&gt; value.(L1DistLoss(), [1,2,3], [2,5,-2])
3-element Array{Int64,1}:
 1
 3
 5

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= value.(L1DistLoss(), [1.,2,3], [2,5,-2])
3-element Array{Float64,1}:
 1.0
 3.0
 5.0</code></pre><p>Furthermore, with the loop fusion changes that were introduced in Julia 0.6, one can also easily weight the influence of each observation without allocating a temporary array.</p><pre><code class="language-julia-repl">julia&gt; buffer .= value.(L1DistLoss(), [1.,2,3], [2,5,-2]) .* [2,1,0.5]
3-element Array{Float64,1}:
 2.0
 3.0
 2.5</code></pre><p>Even though broadcasting is supported, we do expose a vectorized method natively. This is done mainly for API consistency reasons. Internally it even uses broadcast itself, but it does provide the additional benefit of a more reliable type-inference.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.value-Tuple{SupervisedLoss,AbstractArray,AbstractArray}" href="#LearnBase.value-Tuple{SupervisedLoss,AbstractArray,AbstractArray}"><code>LearnBase.value</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">value(loss, targets::AbstractArray, outputs::AbstractArray)</code></pre><p>Compute the result of the loss function for each index-pair in <code>targets</code> and <code>outputs</code> individually and return the result as an array of the appropriate size.</p><p>In the case that the two parameters are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; value(L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
  1.0
  9.0
 25.0
</code></pre></div></div></section><p>We also provide a mutating version for the same reasons. It even utilizes <code>broadcast!</code> underneath.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.value!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}" href="#LearnBase.value!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}"><code>LearnBase.value!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">value!(buffer::AbstractArray, loss, targets::AbstractArray, outputs::AbstractArray) -&gt; buffer</code></pre><p>Compute the result of the loss function for each index-pair in <code>targets</code> and <code>outputs</code> individually, and store them in the preallocated <code>buffer</code>. Note that <code>buffer</code> has to be of the appropriate size.</p><p>In the case that the two parameters, <code>targets</code> and <code>outputs</code>, are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>buffer::AbstractArray</code>: Array to store the computed values in. Old values will be overwritten and lost.</p></li><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; value!(buffer, L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
  1.0
  9.0
 25.0
</code></pre></div></div></section><h2><a class="nav-anchor" id="Computing-the-1st-Derivatives-1" href="#Computing-the-1st-Derivatives-1">Computing the 1st Derivatives</a></h2><p>Maybe the more interesting aspect of loss functions are their derivatives. In fact, most of the popular learning algorithm in Supervised Learning, such as gradient descent, utilize the derivatives of the loss in one way or the other during the training process.</p><p>To compute the derivative of some loss we expose the function <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a>. It supports the same exact method signatures as <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a>. It may be interesting to note explicitly, that we always compute the derivative in respect to the predicted <code>output</code>, since we are interested in deducing in which direction the output should change.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.deriv-Tuple{SupervisedLoss,Number,Number}" href="#LearnBase.deriv-Tuple{SupervisedLoss,Number,Number}"><code>LearnBase.deriv</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">deriv(loss, target::Number, output::Number) -&gt; Number</code></pre><p>Compute the derivative for the loss-function (denoted by the parameter <code>loss</code>) in respect to the <code>output</code>. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we want to compute the derivative with.</p></li><li><p><code>target::Number</code>: The ground truth <span>$y \in Y$</span> of the observation.</p></li><li><p><code>output::Number</code>: The predicted output <span>$\hat{y} \in \mathbb{R}$</span> for the observation.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">#               loss        y    ŷ
julia&gt; deriv(L2DistLoss(), 1.0, 2.0)
2.0

julia&gt; deriv(L2DistLoss(), 1, 2)
2

julia&gt; deriv(L2HingeLoss(), -1, 2)
6

julia&gt; deriv(L2HingeLoss(), -1f0, 2f0)
6.0f0</code></pre></div></section><p>Similar to <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a>, this function also supports broadcasting and all the syntax benefits that come with it. Thus, one can make use of preallocated memory for storing the element-wise derivatives.</p><pre><code class="language-julia-repl">julia&gt; deriv.(L2DistLoss(), [1,2,3], [2,5,-2])
3-element Array{Int64,1}:
   2
   6
 -10

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= deriv.(L2DistLoss(), [1.,2,3], [2,5,-2])
3-element Array{Float64,1}:
   2.0
   6.0
 -10.0</code></pre><p>Furthermore, with the loop fusion changes that were introduced in Julia 0.6, one can also easily weight the influence of each observation without allocating a temporary array.</p><pre><code class="language-julia-repl">julia&gt; buffer .= deriv.(L2DistLoss(), [1.,2,3], [2,5,-2]) .* [2,1,0.5]
3-element Array{Float64,1}:
  4.0
  6.0
 -5.0</code></pre><p>While broadcast is supported, we do expose a vectorized method natively. This is done mainly for API consistency reasons. Internally it even uses broadcast itself, but it does provide the additional benefit of a more reliable type-inference.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.deriv-Tuple{SupervisedLoss,AbstractArray,AbstractArray}" href="#LearnBase.deriv-Tuple{SupervisedLoss,AbstractArray,AbstractArray}"><code>LearnBase.deriv</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv(loss, targets::AbstractArray, outputs::AbstractArray)</code></pre><p>Compute the derivative of the loss function in respect to the output for each index-pair in <code>targets</code> and <code>outputs</code> individually and return the result as an array of the appropriate size.</p><p>In the case that the two parameters are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; deriv(L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
   2.0
   6.0
 -10.0
</code></pre></div></div></section><p>We also provide a mutating version for the same reasons. It even utilizes <span>$broadcast!$</span> underneath.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.deriv!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}" href="#LearnBase.deriv!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}"><code>LearnBase.deriv!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv!(buffer::AbstractArray, loss, targets::AbstractArray, outputs::AbstractArray) -&gt; buffer</code></pre><p>Compute the derivative of the loss function in respect to the output for each index-pair in <code>targets</code> and <code>outputs</code> individually, and store them in the preallocated <code>buffer</code>. Note that <code>buffer</code> has to be of the appropriate size.</p><p>In the case that the two parameters, <code>targets</code> and <code>outputs</code>, are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>buffer::AbstractArray</code>: Array to store the computed values in. Old values will be overwritten and lost.</p></li><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; deriv!(buffer, L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
   2.0
   6.0
 -10.0
</code></pre></div></div></section><p>It is also possible to compute the value and derivative at the same time. For some losses that means less computation overhead.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.value_deriv-Tuple{SupervisedLoss,Number,Number}" href="#LearnBase.value_deriv-Tuple{SupervisedLoss,Number,Number}"><code>LearnBase.value_deriv</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">value_deriv(loss, target::Number, output::Number) -&gt; Tuple</code></pre><p>Return the results of <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a> and <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a> as a tuple, in which the first element is the value and the second element the derivative.</p><p>In some cases this function can yield better performance, because the losses can make use of shared variables when computing the results. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>target::Number</code>: The ground truth <span>$y \in Y$</span> of the observation.</p></li><li><p><code>output::Number</code>: The predicted output <span>$\hat{y} \in \mathbb{R}$</span></p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">#                     loss         y    ŷ
julia&gt; value_deriv(L2DistLoss(), -1.0, 3.0)
(16.0, 8.0)</code></pre></div></section><h2><a class="nav-anchor" id="Computing-the-2nd-Derivatives-1" href="#Computing-the-2nd-Derivatives-1">Computing the 2nd Derivatives</a></h2><p>Additionally to the first derivative, we also provide the corresponding methods for the second derivative through the function <a href="../aggregate/#LearnBase.deriv2-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv2</code></a>. Note again, that we always compute the derivative in respect to the predicted <code>output</code>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.deriv2-Tuple{SupervisedLoss,Number,Number}" href="#LearnBase.deriv2-Tuple{SupervisedLoss,Number,Number}"><code>LearnBase.deriv2</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">deriv2(loss, target::Number, output::Number) -&gt; Number</code></pre><p>Compute the second derivative for the loss-function (denoted by the parameter <code>loss</code>) in respect to the <code>output</code>. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we want to compute the second derivative with.</p></li><li><p><code>target::Number</code>: The ground truth <span>$y \in Y$</span> of the observation.</p></li><li><p><code>output::Number</code>: The predicted output <span>$\hat{y} \in \mathbb{R}$</span> for the observation.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">#               loss             y    ŷ
julia&gt; deriv2(LogitDistLoss(), -0.5, 0.3)
0.42781939304058886

julia&gt; deriv2(LogitMarginLoss(), -1f0, 2f0)
0.104993574f0</code></pre></div></section><p>Just like <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a> and <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a>, this function also supports broadcasting and all the syntax benefits that come with it. Thus, one can make use of preallocated memory for storing the element-wise derivatives.</p><pre><code class="language-julia-repl">julia&gt; deriv2.(LogitDistLoss(), [-0.5, 1.2, 3], [0.3, 2.3, -2])
3-element Array{Float64,1}:
 0.42781939304058886
 0.3747397590950412
 0.013296113341580313

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= deriv2.(LogitDistLoss(), [-0.5, 1.2, 3], [0.3, 2.3, -2])
3-element Array{Float64,1}:
 0.42781939304058886
 0.3747397590950412
 0.013296113341580313</code></pre><p>Furthermore <a href="../aggregate/#LearnBase.deriv2-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv2</code></a> supports all the same method signatures as <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a> does.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.deriv2-Tuple{SupervisedLoss,AbstractArray,AbstractArray}" href="#LearnBase.deriv2-Tuple{SupervisedLoss,AbstractArray,AbstractArray}"><code>LearnBase.deriv2</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv2(loss, targets::AbstractArray, outputs::AbstractArray)</code></pre><p>Compute the second derivative of the loss function in respect to the output for each index-pair in <code>targets</code> and <code>outputs</code> individually and return the result as an array of the appropriate size.</p><p>In the case that the two parameters are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; deriv2(L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
 2.0
 2.0
 2.0
</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.deriv2!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}" href="#LossFunctions.deriv2!-Tuple{AbstractArray,SupervisedLoss,AbstractArray,AbstractArray}"><code>LossFunctions.deriv2!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv2!(buffer::AbstractArray, loss, targets::AbstractArray, outputs::AbstractArray) -&gt; buffer</code></pre><p>Compute the second derivative of the loss function in respect to the output for each index-pair in <code>targets</code> and <code>outputs</code> individually, and store them in the preallocated <code>buffer</code>. Note that <code>buffer</code> has to be of the appropriate size.</p><p>In the case that the two parameters, <code>targets</code> and <code>outputs</code>, are arrays with a different number of dimensions, broadcast will be performed. Note that the given parameters are expected to have the same size in the dimensions they share.</p><p>Note: This function should always be type-stable. If it isn&#39;t, you likely found a bug.</p><p><strong>Arguments</strong></p><ul><li><p><code>buffer::AbstractArray</code>: Array to store the computed values in. Old values will be overwritten and lost.</p></li><li><p><code>loss::SupervisedLoss</code>: The loss-function <span>$L$</span> we are working with.</p></li><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; deriv2!(buffer, L2DistLoss(), [1.0, 2.0, 3.0], [2, 5, -2])
3-element Array{Float64,1}:
 2.0
 2.0
 2.0
</code></pre></div></div></section><h2><a class="nav-anchor" id="Function-Closures-1" href="#Function-Closures-1">Function Closures</a></h2><p>In some circumstances it may be convenient to have the loss function or its derivative as a proper Julia function. Instead of exporting special function names for every implemented loss (like <code>l2distloss(...)</code>), we provide the ability to generate a true function on the fly for any given loss.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.value_fun-Tuple{SupervisedLoss}" href="#LossFunctions.value_fun-Tuple{SupervisedLoss}"><code>LossFunctions.value_fun</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">value_fun(loss::SupervisedLoss) -&gt; Function</code></pre><p>Returns a new function that computes the <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a> for the given <code>loss</code>. This new function will support all the signatures that <a href="../aggregate/#LearnBase.value-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>value</code></a> does.</p><pre><code class="language-julia-repl">julia&gt; f = value_fun(L2DistLoss());

julia&gt; f(-1.0, 3.0) # computes the value of L2DistLoss
16.0

julia&gt; f.([1.,2], [4,7])
2-element Array{Float64,1}:
  9.0
 25.0</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.deriv_fun-Tuple{SupervisedLoss}" href="#LossFunctions.deriv_fun-Tuple{SupervisedLoss}"><code>LossFunctions.deriv_fun</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv_fun(loss::SupervisedLoss) -&gt; Function</code></pre><p>Returns a new function that computes the <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a> for the given <code>loss</code>. This new function will support all the signatures that <a href="../aggregate/#LearnBase.deriv-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv</code></a> does.</p><pre><code class="language-julia-repl">julia&gt; g = deriv_fun(L2DistLoss());

julia&gt; g(-1.0, 3.0) # computes the deriv of L2DistLoss
8.0

julia&gt; g.([1.,2], [4,7])
2-element Array{Float64,1}:
  6.0
 10.0</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.deriv2_fun-Tuple{SupervisedLoss}" href="#LossFunctions.deriv2_fun-Tuple{SupervisedLoss}"><code>LossFunctions.deriv2_fun</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">deriv2_fun(loss::SupervisedLoss) -&gt; Function</code></pre><p>Returns a new function that computes the <a href="../aggregate/#LearnBase.deriv2-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv2</code></a> (i.e. second derivative) for the given <code>loss</code>. This new function will support all the signatures that <a href="../aggregate/#LearnBase.deriv2-Tuple{Loss,AbstractArray,AbstractArray,LossFunctions.AggregateMode}"><code>deriv2</code></a> does.</p><pre><code class="language-julia-repl">julia&gt; g2 = deriv2_fun(L2DistLoss());

julia&gt; g2(-1.0, 3.0) # computes the second derivative of L2DistLoss
2.0

julia&gt; g2.([1.,2], [4,7])
2-element Array{Float64,1}:
 2.0
 2.0</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.value_deriv_fun-Tuple{SupervisedLoss}" href="#LossFunctions.value_deriv_fun-Tuple{SupervisedLoss}"><code>LossFunctions.value_deriv_fun</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">value_deriv_fun(loss::SupervisedLoss) -&gt; Function</code></pre><p>Returns a new function that computes the <a href="#LearnBase.value_deriv-Tuple{SupervisedLoss,Number,Number}"><code>value_deriv</code></a> for the given <code>loss</code>. This new function will support all the signatures that <a href="#LearnBase.value_deriv-Tuple{SupervisedLoss,Number,Number}"><code>value_deriv</code></a> does.</p><pre><code class="language-julia-repl">julia&gt; fg = value_deriv_fun(L2DistLoss());

julia&gt; fg(-1.0, 3.0) # computes the second derivative of L2DistLoss
(16.0, 8.0)</code></pre></div></div></section><h2><a class="nav-anchor" id="Properties-of-a-Loss-1" href="#Properties-of-a-Loss-1">Properties of a Loss</a></h2><p>In some situations it can be quite useful to assert certain properties about a loss-function. One such scenario could be when implementing an algorithm that requires the loss to be strictly convex or Lipschitz continuous. Note that we will only skim over the defintions in most cases. A good treatment of all of the concepts involved can be found in either <a href="#footnote-BOYD2004">[BOYD2004]</a> or <a href="#footnote-STEINWART2008">[STEINWART2008]</a>.</p><div class="footnote" id="footnote-BOYD2004"><a href="#footnote-BOYD2004"><strong>[BOYD2004]</strong></a><p>Stephen Boyd and Lieven Vandenberghe. <a href="https://stanford.edu/~boyd/cvxbook/">&quot;Convex Optimization&quot;</a>. Cambridge University Press, 2004.</p></div><div class="footnote" id="footnote-STEINWART2008"><a href="#footnote-STEINWART2008"><strong>[STEINWART2008]</strong></a><p>Steinwart, Ingo, and Andreas Christmann. <a href="https://www.springer.com/us/book/9780387772417">&quot;Support vector machines&quot;</a>. Springer Science &amp; Business Media, 2008.</p></div><p>This package uses functions to represent individual properties of a loss. It follows a list of implemented property-functions defined in <a href="https://github.com/JuliaML/LearnBase.jl">LearnBase.jl</a>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isconvex" href="#LearnBase.isconvex"><code>LearnBase.isconvex</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isconvex(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a convex function. A function <span>$f : \mathbb{R}^n \rightarrow \mathbb{R}$</span> is convex if its domain is a convex set and if for all <span>$x, y$</span> in that domain, with <span>$\theta$</span> such that for <span>$0 \leq \theta \leq 1$</span>, we have</p><div>\[f(\theta x + (1 - \theta) y) \leq \theta f(x) + (1 - \theta) f(y)\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isconvex(LPDistLoss(0.5))
false

julia&gt; isconvex(ZeroOneLoss())
false

julia&gt; isconvex(L1DistLoss())
true

julia&gt; isconvex(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isstrictlyconvex" href="#LearnBase.isstrictlyconvex"><code>LearnBase.isstrictlyconvex</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isstrictlyconvex(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a strictly convex function. A function <span>$f : \mathbb{R}^n \rightarrow \mathbb{R}$</span> is strictly convex if its domain is a convex set and if for all <span>$x, y$</span> in that domain where <span>$x \neq y$</span>, with <span>$\theta$</span> such that for <span>$0 &lt; \theta &lt; 1$</span>, we have</p><div>\[f(\theta x + (1 - \theta) y) &lt; \theta f(x) + (1 - \theta) f(y)\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isstrictlyconvex(L1DistLoss())
false

julia&gt; isstrictlyconvex(LogitDistLoss())
true

julia&gt; isstrictlyconvex(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isstronglyconvex" href="#LearnBase.isstronglyconvex"><code>LearnBase.isstronglyconvex</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isstronglyconvex(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a strongly convex function. A function <span>$f : \mathbb{R}^n \rightarrow \mathbb{R}$</span> is <span>$m$</span>-strongly convex if its domain is a convex set and if <span>$\forall x,y \in$</span> <strong>dom</strong> <span>$f$</span> where <span>$x \neq y$</span>, and <span>$\theta$</span> such that for <span>$0 \le \theta \le 1$</span> , we have</p><div>\[f(\theta x + (1 - \theta)y) &lt; \theta f(x) + (1 - \theta) f(y) - 0.5 m \cdot \theta (1 - \theta) {\| x - y \|}_2^2\]</div><p>In a more familiar setting, if the loss function is differentiable we have</p><div>\[\left( \nabla f(x) - \nabla f(y) \right)^\top (x - y) \ge m {\| x - y\|}_2^2\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isstronglyconvex(L1DistLoss())
false

julia&gt; isstronglyconvex(LogitDistLoss())
false

julia&gt; isstronglyconvex(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isdifferentiable" href="#LearnBase.isdifferentiable"><code>LearnBase.isdifferentiable</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isdifferentiable(loss::SupervisedLoss, [x::Number]) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is differentiable (optionally limited to the given point <code>x</code> if specified).</p><p>A function <span>$f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$</span> is differentiable at a point <span>$x \in$</span> <strong>int dom</strong> <span>$f$</span>, if there exists a matrix <span>$Df(x) \in \mathbb{R}^{m \times n}$</span> such that it satisfies:</p><div>\[\lim_{z \neq x, z \to x} \frac{{\|f(z) - f(x) - Df(x)(z-x)\|}_2}{{\|z - x\|}_2} = 0\]</div><p>A function is differentiable if its domain is open and it is differentiable at every point <span>$x$</span>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isdifferentiable(L1DistLoss())
false

julia&gt; isdifferentiable(L1DistLoss(), 1)
true

julia&gt; isdifferentiable(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.istwicedifferentiable" href="#LearnBase.istwicedifferentiable"><code>LearnBase.istwicedifferentiable</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">istwicedifferentiable(loss::SupervisedLoss, [x::Number]) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is differentiable (optionally limited to the given point <code>x</code> if specified).</p><p>A function <span>$f : \mathbb{R}^{n} \rightarrow \mathbb{R}$</span> is said to be twice differentiable at a point <span>$x \in$</span> <strong>int dom</strong> <span>$f$</span>, if the function derivative for <span>$\nabla f$</span> exists at <span>$x$</span>.</p><div>\[\nabla^2 f(x) = D \nabla f(x)\]</div><p>A function is twice differentiable if its domain is open and it is twice differentiable at every point <span>$x$</span>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isdifferentiable(L1DistLoss())
false

julia&gt; isdifferentiable(L1DistLoss(), 1)
true

julia&gt; isdifferentiable(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.islocallylipschitzcont" href="#LearnBase.islocallylipschitzcont"><code>LearnBase.islocallylipschitzcont</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">islocallylipschitzcont(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is locally-Lipschitz continous.</p><p>A supervised loss <span>$L : Y \times \mathbb{R} \rightarrow [0, \infty)$</span> is called locally Lipschitz continuous if <span>$\forall a \ge 0$</span> there exists a constant :math:<code>c_a \ge 0</code>, such that</p><div>\[\sup_{y \in Y} \left| L(y,t) − L(y,t′) \right| \le c_a |t − t′|,  \qquad  t,t′ \in [−a,a]\]</div><p>Every convex function is locally lipschitz continuous</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; islocallylipschitzcont(ExpLoss())
true

julia&gt; islocallylipschitzcont(SigmoidLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.islipschitzcont" href="#LearnBase.islipschitzcont"><code>LearnBase.islipschitzcont</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">islipschitzcont(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is Lipschitz continuous.</p><p>A supervised loss function <span>$L : Y \times \mathbb{R} \rightarrow [0, \infty)$</span> is Lipschitz continous, if there exists a finite constant <span>$M &lt; \infty$</span> such that</p><div>\[|L(y, t) - L(y, t′)| \le M |t - t′|,  \qquad  \forall (y, t) \in Y \times \mathbb{R}\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; islipschitzcont(SigmoidLoss())
true

julia&gt; islipschitzcont(ExpLoss())
false</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isnemitski" href="#LearnBase.isnemitski"><code>LearnBase.isnemitski</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isnemitski(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a Nemitski loss function.</p><p>We call a supervised loss function <span>$L : Y \times \mathbb{R} \rightarrow [0,\infty)$</span> a Nemitski loss if there exist a measurable function <span>$b : Y \rightarrow [0, \infty)$</span> and an increasing function <span>$h : [0, \infty) \rightarrow [0, \infty)$</span> such that</p><div>\[L(y,\hat{y}) \le b(y) + h(|\hat{y}|),  \qquad  (y, \hat{y}) \in Y \times \mathbb{R}.\]</div><p>If a loss if locally lipsschitz continuous then it is a Nemitski loss</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isclipable" href="#LearnBase.isclipable"><code>LearnBase.isclipable</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isclipable(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is clipable. A supervised loss <span>$L : Y \times \mathbb{R} \rightarrow [0, \infty)$</span> can be clipped at <span>$M &gt; 0$</span> if, for all <span>$(y,t) \in Y \times \mathbb{R}$</span>,</p><div>\[L(y, \hat{t}) \le L(y, t)\]</div><p>where <span>$\hat{t}$</span> denotes the clipped value of <span>$t$</span> at <span>$\pm M$</span>. That is</p><div>\[\hat{t} = \begin{cases} -M &amp; \quad \text{if } t &lt; -M \\ t &amp; \quad \text{if } t \in [-M, M] \\ M &amp; \quad \text{if } t &gt; M \end{cases}\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isclipable(ExpLoss())
false

julia&gt; isclipable(L2DistLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.ismarginbased" href="#LearnBase.ismarginbased"><code>LearnBase.ismarginbased</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">ismarginbased(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is a margin-based loss.</p><p>A supervised loss function <span>$L : Y \times \mathbb{R} \rightarrow [0, \infty)$</span> is said to be <strong>margin-based</strong>, if there exists a representing function <span>$\psi : \mathbb{R} \rightarrow [0, \infty)$</span> satisfying</p><div>\[L(y, \hat{y}) = \psi (y \cdot \hat{y}),  \qquad  (y, \hat{y}) \in Y \times \mathbb{R}\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; ismarginbased(HuberLoss(2))
false

julia&gt; ismarginbased(L2MarginLoss())
true</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isclasscalibrated" href="#LearnBase.isclasscalibrated"><code>LearnBase.isclasscalibrated</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">isclasscalibrated(loss::SupervisedLoss) -&gt; Bool</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.isdistancebased" href="#LearnBase.isdistancebased"><code>LearnBase.isdistancebased</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">isdistancebased(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> ifthe given <code>loss</code> is a distance-based loss.</p><p>A supervised loss function <span>$L : Y \times \mathbb{R} \rightarrow [0, \infty)$</span> is said to be <strong>distance-based</strong>, if there exists a representing function <span>$\psi : \mathbb{R} \rightarrow [0, \infty)$</span> satisfying <span>$\psi (0) = 0$</span> and</p><div>\[L(y, \hat{y}) = \psi (\hat{y} - y),  \qquad  (y, \hat{y}) \in Y \times \mathbb{R}\]</div><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; isdistancebased(HuberLoss(2))
true

julia&gt; isdistancebased(L2MarginLoss())
false</code></pre></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LinearAlgebra.issymmetric" href="#LinearAlgebra.issymmetric"><code>LinearAlgebra.issymmetric</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">issymmetric(loss::SupervisedLoss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given loss is a symmetric loss.</p><p>A function <span>$f : \mathbb{R} \rightarrow [0,\infty)$</span> is said to be symmetric about origin if we have</p><div>\[f(x) = f(-x), \qquad  \forall x \in \mathbb{R}\]</div><p>A distance-based loss is said to be symmetric if its representing function is symmetric.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; issymmetric(QuantileLoss(0.2))
false

julia&gt; issymmetric(LPDistLoss(2))
true</code></pre></div></section><footer><hr/><a class="previous" href="../../introduction/motivation/"><span class="direction">Previous</span><span class="title">Background and Motivation</span></a><a class="next" href="../aggregate/"><span class="direction">Next</span><span class="title">Efficient Sum and Mean</span></a></footer></article></body></html>
