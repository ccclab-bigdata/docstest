<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distance-based Losses · LossFunctions.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><h1>LossFunctions.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><span class="toctext">Introduction</span><ul><li><a class="toctext" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="../../user/interface/">Working with Losses</a></li><li><a class="toctext" href="../../user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="toctext">Available Losses</span><ul><li class="current"><a class="toctext" href>Distance-based Losses</a><ul class="internal"><li><a class="toctext" href="#LPDistLoss-1">LPDistLoss</a></li><li><a class="toctext" href="#L1DistLoss-1">L1DistLoss</a></li><li><a class="toctext" href="#L2DistLoss-1">L2DistLoss</a></li><li><a class="toctext" href="#LogitDistLoss-1">LogitDistLoss</a></li><li><a class="toctext" href="#HuberLoss-1">HuberLoss</a></li><li><a class="toctext" href="#L1EpsilonInsLoss-1">L1EpsilonInsLoss</a></li><li><a class="toctext" href="#L2EpsilonInsLoss-1">L2EpsilonInsLoss</a></li><li><a class="toctext" href="#PeriodicLoss-1">PeriodicLoss</a></li><li><a class="toctext" href="#QuantileLoss-1">QuantileLoss</a></li></ul></li><li><a class="toctext" href="../margin/">Margin-based Losses</a></li></ul></li><li><span class="toctext">Advances Topics</span><ul><li><a class="toctext" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="toctext" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="toctext" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Available Losses</li><li><a href>Distance-based Losses</a></li></ul></nav><hr/><div id="topbar"><span>Distance-based Losses</span><a class="fa fa-bars" href="#"></a></div></header><div class="loss-docs"><h1><a class="nav-anchor" id="Distance-based-Losses-1" href="#Distance-based-Losses-1">Distance-based Losses</a></h1><p>Loss functions that belong to the category &quot;distance-based&quot; are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.</p><p>This section lists all the subtypes of <a href="../../advanced/developer/#LearnBase.DistanceLoss"><code>DistanceLoss</code></a> that are implemented in this package.</p><h2><a class="nav-anchor" id="LPDistLoss-1" href="#LPDistLoss-1">LPDistLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.LPDistLoss" href="#LossFunctions.LPDistLoss"><code>LossFunctions.LPDistLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">LPDistLoss{P} &lt;: DistanceLoss</code></pre><p>The P-th power absolute distance loss. It is Lipschitz continuous iff <code>P == 1</code>, convex if and only if <code>P &gt;= 1</code>, and strictly convex iff <code>P &gt; 1</code>.</p><div>\[L(r) = |r|^P\]</div></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LPDistLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LPDistLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \mid r \mid ^p$</span></td><td><span>$L&#39;(r) = p \cdot r \cdot \mid r \mid ^{p-2}$</span></td></tr></table><h2><a class="nav-anchor" id="L1DistLoss-1" href="#L1DistLoss-1">L1DistLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L1DistLoss" href="#LossFunctions.L1DistLoss"><code>LossFunctions.L1DistLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L1DistLoss &lt;: DistanceLoss</code></pre><p>The absolute distance loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=1</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(r) = |r|\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │\.                     ./│    1 │            ┌------------│
      │ &#39;\.                 ./&#39; │      │            |            │
      │   \.               ./   │      │            |            │
      │    &#39;\.           ./&#39;    │      │_           |           _│
    L │      \.         ./      │   L&#39; │            |            │
      │       &#39;\.     ./&#39;       │      │            |            │
      │         \.   ./         │      │            |            │
    0 │          &#39;\./&#39;          │   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1DistLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1DistLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \mid r \mid$</span></td><td><span>$L&#39;(r) = \textrm{sign}(r)$</span></td></tr></table><h2><a class="nav-anchor" id="L2DistLoss-1" href="#L2DistLoss-1">L2DistLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L2DistLoss" href="#LossFunctions.L2DistLoss"><code>LossFunctions.L2DistLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L2DistLoss &lt;: DistanceLoss</code></pre><p>The least squares loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=2</code>. It is strictly convex.</p><div>\[L(r) = |r|^2\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    9 │\                       /│    3 │                   .r/   │
      │&quot;.                     .&quot;│      │                 .r&#39;     │
      │ &quot;.                   .&quot; │      │              _./&#39;       │
      │  &quot;.                 .&quot;  │      │_           .r/         _│
    L │   &quot;.               .&quot;   │   L&#39; │         _:/&#39;            │
      │    &#39;\.           ./&#39;    │      │       .r&#39;               │
      │      \.         ./      │      │     .r&#39;                 │
    0 │        &quot;-.___.-&quot;        │   -3 │  _/r&#39;                   │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2DistLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2DistLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \mid r \mid ^2$</span></td><td><span>$L&#39;(r) = 2 r$</span></td></tr></table><h2><a class="nav-anchor" id="LogitDistLoss-1" href="#LogitDistLoss-1">LogitDistLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.LogitDistLoss" href="#LossFunctions.LogitDistLoss"><code>LossFunctions.LogitDistLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">LogitDistLoss &lt;: DistanceLoss</code></pre><p>The distance-based logistic loss for regression. It is strictly convex and Lipschitz continuous.</p><div>\[L(r) = - \ln \frac{4 e^r}{(1 + e^r)^2}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                   _--&#39;&#39;&#39;│
      │\                       /│      │                ./&#39;      │
      │ \.                   ./ │      │              ./         │
      │  &#39;.                 .&#39;  │      │_           ./          _│
    L │   &#39;.               .&#39;   │   L&#39; │           ./            │
      │     \.           ./     │      │         ./              │
      │      &#39;.         .&#39;      │      │       ./                │
    0 │        &#39;-.___.-&#39;        │   -1 │___.-&#39;&#39;                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -4                        4
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitDistLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitDistLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = - \ln \frac{4 e^r}{(1 + e^r)^2}$</span></td><td><span>$L&#39;(r) = \tanh \left( \frac{r}{2} \right)$</span></td></tr></table><h2><a class="nav-anchor" id="HuberLoss-1" href="#HuberLoss-1">HuberLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.HuberLoss" href="#LossFunctions.HuberLoss"><code>LossFunctions.HuberLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">HuberLoss &lt;: DistanceLoss</code></pre><p>Loss function commonly used for robustness to outliers. For large values of <code>d</code> it becomes close to the <a href="#LossFunctions.L1DistLoss"><code>L1DistLoss</code></a>, while for small values of <code>d</code> it resembles the <a href="#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(r) = \begin{cases} \frac{r^2}{2} &amp; \quad \text{if } | r | \le \alpha \\ \alpha | r | - \frac{\alpha^3}{2} &amp; \quad \text{otherwise}\\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction (d=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                .+-------│
      │                         │      │              ./&#39;        │
      │\.                     ./│      │             ./          │
      │ &#39;.                   .&#39; │      │_           ./          _│
    L │   \.               ./   │   L&#39; │           /&#39;            │
      │     \.           ./     │      │          /&#39;             │
      │      &#39;.         .&#39;      │      │        ./&#39;              │
    0 │        &#39;-.___.-&#39;        │   -1 │-------+&#39;                │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/HuberLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/HuberLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \begin{cases} \frac{r^2}{2} &amp; \quad \text{if } \mid r \mid \le \alpha \\ \alpha \mid r \mid - \frac{\alpha^2}{2} &amp; \quad \text{otherwise}\\ \end{cases}$</span></td><td><span>$L&#39;(r) = \begin{cases} r &amp; \quad \text{if } \mid r \mid \le \alpha \\ \alpha \cdot \textrm{sign}(r) &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="L1EpsilonInsLoss-1" href="#L1EpsilonInsLoss-1">L1EpsilonInsLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L1EpsilonInsLoss" href="#LossFunctions.L1EpsilonInsLoss"><code>LossFunctions.L1EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L1EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances linarily. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(r) = \max \{ 0, | r | - \epsilon \}\]</div><hr/><pre><code class="language-none">              Lossfunction (ϵ=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\                       /│    1 │                  ┌------│
      │ \                     / │      │                  |      │
      │  \                   /  │      │                  |      │
      │   \                 /   │      │_      ___________!     _│
    L │    \               /    │   L&#39; │      |                  │
      │     \             /     │      │      |                  │
      │      \           /      │      │      |                  │
    0 │       \_________/       │   -1 │------┘                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1EpsilonInsLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1EpsilonInsLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \max \{ 0, \mid r \mid - \epsilon \}$</span></td><td><span>$L&#39;(r) = \begin{cases} \frac{r}{ \mid r \mid } &amp; \quad \text{if } \epsilon \le \mid r \mid \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="L2EpsilonInsLoss-1" href="#L2EpsilonInsLoss-1">L2EpsilonInsLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L2EpsilonInsLoss" href="#LossFunctions.L2EpsilonInsLoss"><code>LossFunctions.L2EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L2EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The quadratic <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances quadratically. It is convex, but not strictly convex.</p><div>\[L(r) = \max \{ 0, | r | - \epsilon \}^2\]</div><hr/><pre><code class="language-none">              Lossfunction (ϵ=0.5)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    8 │                         │    1 │                  /      │
      │:                       :│      │                 /       │
      │&#39;.                     .&#39;│      │                /        │
      │ \.                   ./ │      │_         _____/        _│
    L │  \.                 ./  │   L&#39; │         /               │
      │   \.               ./   │      │        /                │
      │    &#39;\.           ./&#39;    │      │       /                 │
    0 │      &#39;-._______.-&#39;      │   -1 │      /                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2EpsilonInsLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2EpsilonInsLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \max \{ 0, \mid r \mid - \epsilon \}^2$</span></td><td><span>$L&#39;(r) = \begin{cases} 2 \cdot \textrm{sign}(r) \cdot \left( \mid r \mid - \epsilon \right) &amp; \quad \text{if } \epsilon \le \mid r \mid \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="PeriodicLoss-1" href="#PeriodicLoss-1">PeriodicLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.PeriodicLoss" href="#LossFunctions.PeriodicLoss"><code>LossFunctions.PeriodicLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">PeriodicLoss &lt;: DistanceLoss</code></pre><p>Measures distance on a circle of specified circumference <code>c</code>.</p><div>\[L(r) = 1 - \cos \left( \frac{2 r \pi}{c} \right)\]</div></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PeriodicLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PeriodicLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = 1 - \cos \left ( \frac{2 r \pi}{c} \right )$</span></td><td><span>$L&#39;(r) = \frac{2 \pi}{c} \cdot \sin \left( \frac{2r \pi}{c} \right)$</span></td></tr></table><h2><a class="nav-anchor" id="QuantileLoss-1" href="#QuantileLoss-1">QuantileLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.QuantileLoss" href="#LossFunctions.QuantileLoss"><code>LossFunctions.QuantileLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">QuantileLoss &lt;: DistanceLoss</code></pre><p>The distance-based quantile loss, also known as pinball loss, can be used to estimate conditional τ-quantiles. It is Lipschitz continuous and convex, but not strictly convex. Furthermore it is symmetric if and only if <code>τ = 1/2</code>.</p><div>\[L(r) = \begin{cases} -\left( 1 - \tau  \right) r &amp; \quad \text{if } r &lt; 0 \\ \tau r &amp; \quad \text{if } r \ge 0 \\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction (τ=0.7)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │&#39;\                       │  0.3 │            ┌------------│
      │  \.                     │      │            |            │
      │   &#39;\                    │      │_           |           _│
      │     \.                  │      │            |            │
    L │      &#39;\              ._-│   L&#39; │            |            │
      │        \.         ..-&#39;  │      │            |            │
      │         &#39;.     _r/&#39;     │      │            |            │
    0 │           &#39;_./&#39;         │ -0.7 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/QuantileLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/QuantileLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(r) = \begin{cases} \left( 1 - \tau \right) r &amp; \quad \text{if } r \ge 0 \\ - \tau r &amp; \quad \text{otherwise} \\ \end{cases}$</span></td><td><span>$L(r) = \begin{cases} 1 - \tau &amp; \quad \text{if } r \ge 0 \\ - \tau &amp; \quad \text{otherwise} \\ \end{cases}$</span></td></tr></table><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>You may note that our definition of the QuantileLoss looks different to what one usually sees in other literature. The reason is that we have to correct for the fact that in our case <span>$r = \hat{y} - y$</span> instead of <span>$r_{\textrm{usual}} = y - \hat{y}$</span>, which means that our definition relates to that in the manner of <span>$r = -1 * r_{\textrm{usual}}$</span>.</p></div></div></div><footer><hr/><a class="previous" href="../../user/aggregate/"><span class="direction">Previous</span><span class="title">Efficient Sum and Mean</span></a><a class="next" href="../margin/"><span class="direction">Next</span><span class="title">Margin-based Losses</span></a></footer></article></body></html>
