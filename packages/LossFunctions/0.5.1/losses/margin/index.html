<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Margin-based Losses · LossFunctions.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><h1>LossFunctions.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><span class="toctext">Introduction</span><ul><li><a class="toctext" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="../../user/interface/">Working with Losses</a></li><li><a class="toctext" href="../../user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="toctext">Available Losses</span><ul><li><a class="toctext" href="../distance/">Distance-based Losses</a></li><li class="current"><a class="toctext" href>Margin-based Losses</a><ul class="internal"><li><a class="toctext" href="#ZeroOneLoss-1">ZeroOneLoss</a></li><li><a class="toctext" href="#PerceptronLoss-1">PerceptronLoss</a></li><li><a class="toctext" href="#L1HingeLoss-1">L1HingeLoss</a></li><li><a class="toctext" href="#SmoothedL1HingeLoss-1">SmoothedL1HingeLoss</a></li><li><a class="toctext" href="#ModifiedHuberLoss-1">ModifiedHuberLoss</a></li><li><a class="toctext" href="#DWDMarginLoss-1">DWDMarginLoss</a></li><li><a class="toctext" href="#L2MarginLoss-1">L2MarginLoss</a></li><li><a class="toctext" href="#L2HingeLoss-1">L2HingeLoss</a></li><li><a class="toctext" href="#LogitMarginLoss-1">LogitMarginLoss</a></li><li><a class="toctext" href="#ExpLoss-1">ExpLoss</a></li><li><a class="toctext" href="#SigmoidLoss-1">SigmoidLoss</a></li></ul></li></ul></li><li><span class="toctext">Advances Topics</span><ul><li><a class="toctext" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="toctext" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="toctext" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Available Losses</li><li><a href>Margin-based Losses</a></li></ul></nav><hr/><div id="topbar"><span>Margin-based Losses</span><a class="fa fa-bars" href="#"></a></div></header><div class="loss-docs"><h1><a class="nav-anchor" id="Margin-based-Losses-1" href="#Margin-based-Losses-1">Margin-based Losses</a></h1><p>Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction. Instead they penalize predictions based on how well they agree with the sign of the target.</p><p>This section lists all the subtypes of <a href="../../advanced/developer/#LearnBase.MarginLoss"><code>MarginLoss</code></a> that are implemented in this package.</p><h2><a class="nav-anchor" id="ZeroOneLoss-1" href="#ZeroOneLoss-1">ZeroOneLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.ZeroOneLoss" href="#LossFunctions.ZeroOneLoss"><code>LossFunctions.ZeroOneLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">ZeroOneLoss &lt;: MarginLoss</code></pre><p>The classical classification loss. It penalizes every misclassified observation with a loss of <code>1</code> while every correctly classified observation has a loss of <code>0</code>. It is not convex nor continuous and thus seldom used directly. Instead one usually works with some classification-calibrated surrogate loss, such as <a href="#L1HingeLoss-1">L1HingeLoss</a>.</p><div>\[L(a) = \begin{cases} 1 &amp; \quad \text{if } a &lt; 0 \\ 0 &amp; \quad \text{if } a &gt;= 0\\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    1 │------------┐            │    1 │                         │
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │_________________________│
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │                         │
    0 │            └------------│   -1 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                y * h(x)                         y * h(x)</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ZeroOneLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ZeroOneLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \begin{cases} 1 &amp; \quad \text{if } a &lt; 0 \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td><td><span>$L&#39;(a) = 0$</span></td></tr></table><h2><a class="nav-anchor" id="PerceptronLoss-1" href="#PerceptronLoss-1">PerceptronLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.PerceptronLoss" href="#LossFunctions.PerceptronLoss"><code>LossFunctions.PerceptronLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">PerceptronLoss &lt;: MarginLoss</code></pre><p>The perceptron loss linearly penalizes every prediction where the resulting <code>agreement &lt;= 0</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \max \{ 0, -a \}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │            ┌------------│
      │ &#39;..                     │      │            |            │
      │   \.                    │      │            |            │
      │     &#39;.                  │      │            |            │
    L │      &#39;.                 │   L&#39; │            |            │
      │        \.               │      │            |            │
      │         &#39;.              │      │            |            │
    0 │           \.____________│   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PerceptronLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PerceptronLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \max \{ 0, - a \}$</span></td><td><span>$L&#39;(a) = \begin{cases} -1 &amp; \quad \text{if } a &lt; 0 \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="L1HingeLoss-1" href="#L1HingeLoss-1">L1HingeLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L1HingeLoss" href="#LossFunctions.L1HingeLoss"><code>LossFunctions.L1HingeLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L1HingeLoss &lt;: MarginLoss</code></pre><p>The hinge loss linearly penalizes every predicition where the resulting <code>agreement &lt; 1</code> . It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \max \{ 0, 1 - a \}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │&#39;\.                      │    0 │                  ┌------│
      │  &#39;&#39;_                    │      │                  |      │
      │     \.                  │      │                  |      │
      │       &#39;.                │      │                  |      │
    L │         &#39;&#39;_             │   L&#39; │                  |      │
      │            \.           │      │                  |      │
      │              &#39;.         │      │                  |      │
    0 │                &#39;&#39;_______│   -1 │------------------┘      │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1HingeLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1HingeLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \max \{ 0, 1 - a \}$</span></td><td><span>$L&#39;(a) = \begin{cases} -1 &amp; \quad \text{if } a &lt; 1 \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="SmoothedL1HingeLoss-1" href="#SmoothedL1HingeLoss-1">SmoothedL1HingeLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.SmoothedL1HingeLoss" href="#LossFunctions.SmoothedL1HingeLoss"><code>LossFunctions.SmoothedL1HingeLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">SmoothedL1HingeLoss &lt;: MarginLoss</code></pre><p>As the name suggests a smoothed version of the L1 hinge loss. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \begin{cases} \frac{0.5}{\gamma} \cdot \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge 1 - \gamma \\ 1 - \frac{\gamma}{2} - a &amp; \quad \text{otherwise}\\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction (γ=2)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │                 ,r------│
      │ &#39;.                      │      │               ./&#39;       │
      │   \.                    │      │              ,/         │
      │     &#39;.                  │      │            ./&#39;          │
    L │      &#39;.                 │   L&#39; │           ,&#39;            │
      │        \.               │      │         ,/              │
      │          &#39;,             │      │       ./&#39;               │
    0 │            &#39;*-._________│   -1 │______./                 │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/SmoothedL1HingeLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/SmoothedL1HingeLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \begin{cases} \frac{1}{2 \gamma} \cdot \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge 1 - \gamma \\ 1 - \frac{\gamma}{2} - a &amp; \quad \text{otherwise}\\ \end{cases}$</span></td><td><span>$L&#39;(a) = \begin{cases} - \frac{1}{\gamma} \cdot \max \{ 0, 1 - a \} &amp; \quad \text{if } a \ge 1 - \gamma \\ - 1 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="ModifiedHuberLoss-1" href="#ModifiedHuberLoss-1">ModifiedHuberLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.ModifiedHuberLoss" href="#LossFunctions.ModifiedHuberLoss"><code>LossFunctions.ModifiedHuberLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">ModifiedHuberLoss &lt;: MarginLoss</code></pre><p>A special (4 times scaled) case of the <a href="#LossFunctions.SmoothedL1HingeLoss"><code>SmoothedL1HingeLoss</code></a> with <code>γ=2</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \begin{cases} \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge -1 \\ - 4 a &amp; \quad \text{otherwise}\\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │    &#39;.                   │    0 │                .+-------│
      │     &#39;.                  │      │              ./&#39;        │
      │      &#39;\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         &#39;.              │   L&#39; │         ./              │
      │          &#39;.             │      │       ./&#39;               │
      │            \.           │      │______/&#39;                 │
    0 │              &#39;-.________│   -5 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ModifiedHuberLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ModifiedHuberLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \begin{cases} \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge -1 \\ - 4 a &amp; \quad \text{otherwise}\\ \end{cases}$</span></td><td><span>$L&#39;(a) = \begin{cases} - 2 \cdot \max \{ 0, 1 - a \} &amp; \quad \text{if } a \ge -1 \\ - 4 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="DWDMarginLoss-1" href="#DWDMarginLoss-1">DWDMarginLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.DWDMarginLoss" href="#LossFunctions.DWDMarginLoss"><code>LossFunctions.DWDMarginLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">DWDMarginLoss &lt;: MarginLoss</code></pre><p>The distance weighted discrimination margin loss. It is a differentiable generalization of the <a href="#L1HingeLoss-1">L1HingeLoss</a> that is different than the <a href="#SmoothedL1HingeLoss-1">SmoothedL1HingeLoss</a>. It is Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \begin{cases} 1 - a &amp; \quad \text{if } a \ge \frac{q}{q+1} \\ \frac{1}{a^q} \frac{q^q}{(q+1)^{q+1}} &amp; \quad \text{otherwise}\\ \end{cases}\]</div><hr/><pre><code class="language-none">              Lossfunction (q=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │      &quot;.                 │    0 │                     ._r-│
      │        \.               │      │                   ./    │
      │         &#39;,              │      │                 ./      │
      │           \.            │      │                 /       │
    L │            &quot;\.          │   L&#39; │                .        │
      │              \.         │      │                /        │
      │               &quot;:__      │      │               ;         │
    0 │                   &#39;&quot;&quot;---│   -1 │---------------┘         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/DWDMarginLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/DWDMarginLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \begin{cases} 1 - a &amp; \quad \text{if } a \le \frac{q}{q+1} \\ \frac{1}{a^q} \frac{q^q}{(q+1)^{q+1}} &amp; \quad \text{otherwise}\\ \end{cases}$</span></td><td><span>$L&#39;(a) = \begin{cases} - 1 &amp; \quad \text{if } a \le \frac{q}{q+1} \\ - \frac{1}{a^{q+1}} \left( \frac{q}{q+1} \right)^{q+1} &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="L2MarginLoss-1" href="#L2MarginLoss-1">L2MarginLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L2MarginLoss" href="#LossFunctions.L2MarginLoss"><code>LossFunctions.L2MarginLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L2MarginLoss &lt;: MarginLoss</code></pre><p>The margin-based least-squares loss for classification, which penalizes every prediction where <code>agreement != 1</code> quadratically. It is locally Lipschitz continuous and strongly convex.</p><div>\[L(a) = {\left( 1 - a \right)}^2\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    2 │                       ,r│
      │     &#39;.                  │      │                     ,/  │
      │      &#39;\                 │      │                   ,/    │
      │        \                │      ├                 ,/      ┤
    L │         &#39;.              │   L&#39; │               ./        │
      │          &#39;.             │      │             ./          │
      │            \.          .│      │           ./            │
    0 │              &#39;-.____.-&#39; │   -3 │         ./              │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2MarginLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2MarginLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = {\left( 1 - a \right)}^2$</span></td><td><span>$L&#39;(a) = 2 \left( a - 1 \right)$</span></td></tr></table><h2><a class="nav-anchor" id="L2HingeLoss-1" href="#L2HingeLoss-1">L2HingeLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.L2HingeLoss" href="#LossFunctions.L2HingeLoss"><code>LossFunctions.L2HingeLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">L2HingeLoss &lt;: MarginLoss</code></pre><p>The truncated least squares loss quadratically penalizes every predicition where the resulting <code>agreement &lt; 1</code>. It is locally Lipschitz continuous and convex, but not strictly convex.</p><div>\[L(a) = \max \{ 0, 1 - a \}^2\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    0 │                 ,r------│
      │     &#39;.                  │      │               ,/        │
      │      &#39;\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         &#39;.              │   L&#39; │         ./              │
      │          &#39;.             │      │       ./                │
      │            \.           │      │     ./                  │
    0 │              &#39;-.________│   -5 │   ./                    │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2HingeLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2HingeLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \max \{ 0, 1 - a \} ^2$</span></td><td><span>$L&#39;(a) = \begin{cases} 2 \left( a - 1 \right) &amp; \quad \text{if } a &lt; 1 \\ 0 &amp; \quad \text{otherwise}\\ \end{cases}$</span></td></tr></table><h2><a class="nav-anchor" id="LogitMarginLoss-1" href="#LogitMarginLoss-1">LogitMarginLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.LogitMarginLoss" href="#LossFunctions.LogitMarginLoss"><code>LossFunctions.LogitMarginLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">LogitMarginLoss &lt;: MarginLoss</code></pre><p>The margin version of the logistic loss. It is infinitely many times differentiable, strictly convex, and Lipschitz continuous.</p><div>\[L(a) = \ln (1 + e^{-a})\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │ \.                      │    0 │                  ._--/&quot;&quot;│
      │   \.                    │      │               ../&#39;      │
      │     \.                  │      │              ./         │
      │       \..               │      │            ./&#39;          │
    L │         &#39;-_             │   L&#39; │          .,&#39;            │
      │            &#39;-_          │      │         ./              │
      │               &#39;\-._     │      │      .,/&#39;               │
    0 │                    &#39;&quot;&quot;*-│   -1 │__.--&#39;&#39;                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -4                        4
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitMarginLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitMarginLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = \ln (1 + e^{-a})$</span></td><td><span>$L&#39;(a) = - \frac{1}{1 + e^a}$</span></td></tr></table><h2><a class="nav-anchor" id="ExpLoss-1" href="#ExpLoss-1">ExpLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.ExpLoss" href="#LossFunctions.ExpLoss"><code>LossFunctions.ExpLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">ExpLoss &lt;: MarginLoss</code></pre><p>The margin-based exponential loss for classification, which penalizes every prediction exponentially. It is infinitely many times differentiable, locally Lipschitz continuous and strictly convex, but not clipable.</p><div>\[L(a) = e^{-a}\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │  \.                     │    0 │               _,,---:&#39;&quot;&quot;│
      │   l                     │      │           _r/&quot;&#39;         │
      │    l.                   │      │        .r/&#39;             │
      │     &quot;:                  │      │      .r&#39;                │
    L │       \.                │   L&#39; │     ./                  │
      │        &quot;\..             │      │    .&#39;                   │
      │           &#39;&quot;:,_         │      │   ,&#39;                    │
    0 │                &quot;&quot;---:.__│   -5 │  ./                     │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ExpLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/ExpLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = e^{-a}$</span></td><td><span>$L&#39;(a) = - e^{-a}$</span></td></tr></table><h2><a class="nav-anchor" id="SigmoidLoss-1" href="#SigmoidLoss-1">SigmoidLoss</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.SigmoidLoss" href="#LossFunctions.SigmoidLoss"><code>LossFunctions.SigmoidLoss</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">SigmoidLoss &lt;: MarginLoss</code></pre><p>Continuous loss which penalizes every prediction with a loss within in the range (0,2). It is infinitely many times differentiable, Lipschitz continuous but nonconvex.</p><div>\[L(a) = 1 - \tanh(a)\]</div><hr/><pre><code class="language-none">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │&quot;&quot;&#39;--,.                  │    0 │..                     ..│
      │      &#39;\.                │      │ &quot;\.                 ./&quot; │
      │         &#39;.              │      │    &#39;,             ,&#39;    │
      │           \.            │      │      \           /      │
    L │            &quot;\.          │   L&#39; │       \         /       │
      │              \.         │      │        \.     ./        │
      │                \,       │      │         \.   ./         │
    0 │                  &#39;&quot;-:.__│   -1 │          &#39;,_,&#39;          │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section><table><tr><th>Lossfunction</th><th>Derivative</th></tr><tr><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/SigmoidLoss1.svg" alt="loss"/></td><td><img src="https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/SigmoidLoss2.svg" alt="deriv"/></td></tr><tr><td><span>$L(a) = 1 - \tanh(a)$</span></td><td><span>$L&#39;(a) = - \textrm{sech}^2 (a)$</span></td></tr></table></div><footer><hr/><a class="previous" href="../distance/"><span class="direction">Previous</span><span class="title">Distance-based Losses</span></a><a class="next" href="../../advanced/extend/"><span class="direction">Next</span><span class="title">Altering existing Losses</span></a></footer></article></body></html>
