<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Altering existing Losses · LossFunctions.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><h1>LossFunctions.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><span class="toctext">Introduction</span><ul><li><a class="toctext" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="../../user/interface/">Working with Losses</a></li><li><a class="toctext" href="../../user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="toctext">Available Losses</span><ul><li><a class="toctext" href="../../losses/distance/">Distance-based Losses</a></li><li><a class="toctext" href="../../losses/margin/">Margin-based Losses</a></li></ul></li><li><span class="toctext">Advances Topics</span><ul><li class="current"><a class="toctext" href>Altering existing Losses</a><ul class="internal"><li><a class="toctext" href="#Scaling-a-Supervised-Loss-1">Scaling a Supervised Loss</a></li><li><a class="toctext" href="#Reweighting-a-Margin-Loss-1">Reweighting a Margin Loss</a></li></ul></li><li><a class="toctext" href="../developer/">Developer Documentation</a></li></ul></li><li><a class="toctext" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Advances Topics</li><li><a href>Altering existing Losses</a></li></ul></nav><hr/><div id="topbar"><span>Altering existing Losses</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Altering-existing-Losses-1" href="#Altering-existing-Losses-1">Altering existing Losses</a></h1><p>There are situations in which one wants to work with slightly altered versions of specific loss functions. This package provides two generic ways to create such meta losses for specific families of loss functions.</p><ol><li><p>Scaling a supervised loss by a constant real number. This is done at compile time and can in some situations even lead to simpler code (e.g. in the case of the derivative for a <a href="../../losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>)</p></li><li><p>Weighting the classes of a margin-based loss differently in order to better deal with unbalanced binary classification problems.</p></li></ol><h2><a class="nav-anchor" id="Scaling-a-Supervised-Loss-1" href="#Scaling-a-Supervised-Loss-1">Scaling a Supervised Loss</a></h2><p>It is quite common in machine learning courses to define the least squares loss as <span>$\frac{1}{2} (\hat{y} - y)^2$</span>, while this package implements that type of loss as an <span>$L_2$</span> distance loss using <span>$(\hat{y} - y)^2$</span>, i.e. without the constant scale factor.</p><p>For situations in which one wants a scaled version of an existing loss type, we provide the concept of a <strong>scaled loss</strong>. The difference is literally only a constant real number that gets multiplied to the existing implementation of the loss function (and derivatives).</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LearnBase.scaled" href="#LearnBase.scaled"><code>LearnBase.scaled</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">scaled(loss::SupervisedLoss, K)</code></pre><p>Returns a version of <code>loss</code> that is uniformly scaled by <code>K</code>. This function dispatches on the type of <code>loss</code> in order to choose the appropriate type of scaled loss that will be used as the decorator. For example, if <code>typeof(loss) &lt;: DistanceLoss</code> then the given <code>loss</code> will be boxed into a <code>ScaledDistanceLoss</code>.</p><p>Note: If <code>typeof(K) &lt;: Number</code>, then this method will poison the type-inference of the calling scope. This is because <code>K</code> will be promoted to a type parameter. For a typestable version use the following signature: <code>scaled(loss, Val(K))</code></p></div></div></section><pre><code class="language-julia-repl">julia&gt; lsloss = 1/2 * L2DistLoss()
ScaledDistanceLoss{LPDistLoss{2},0.5}(LPDistLoss{2}())

julia&gt; value(L2DistLoss(), 0.0, 4.0)
16.0

julia&gt; value(lsloss, 0.0, 4.0)
8.0</code></pre><p>While the resulting loss is of the same basic family as the original loss (i.e. margin-based or distance-based), it is not a sub-type of it.</p><pre><code class="language-julia-repl">julia&gt; typeof(lsloss) &lt;: DistanceLoss
true

julia&gt; typeof(lsloss) &lt;: L2DistLoss
false</code></pre><p>As you have probably noticed, the constant scale factor gets promoted to a type-parameter. This can be quite an overhead when done on the fly every time the loss value is computed. To avoid this one can make use of <code>Val</code> to specify the scale factor in a type-stable manner.</p><pre><code class="language-julia-repl">julia&gt; lsloss = scaled(L2DistLoss(), Val(0.5))
ScaledDistanceLoss{LPDistLoss{2},0.5}(LPDistLoss{2}())</code></pre><p>Storing the scale factor as a type-parameter instead of a member variable has some nice advantages. For one it makes it possible to define new types of losses using simple type-aliases.</p><pre><code class="language-julia-repl">julia&gt; const LeastSquaresLoss = LossFunctions.ScaledDistanceLoss{L2DistLoss,0.5}
ScaledDistanceLoss{LPDistLoss{2},0.5}

julia&gt; value(LeastSquaresLoss(), 0.0, 4.0)
8.0</code></pre><p>Furthermore, it allows the compiler to do some quite convenient optimizations if possible. For example the compiler is able to figure out that the derivative simplifies for our newly defined <code>LeastSquaresLoss</code>, because <code>1/2 * 2</code> cancels each other. This is accomplished using the power of <code>@fastmath</code>.</p><pre><code class="language-julia-repl">julia&gt; @code_llvm deriv(L2DistLoss(), 0.0, 4.0)
define double @julia_deriv_71652(double, double) #0 {
top:
  %2 = fsub double %1, %0
  %3 = fmul double %2, 2.000000e+00
  ret double %3
}

julia&gt; @code_llvm deriv(LeastSquaresLoss(), 0.0, 4.0)
define double @julia_deriv_71659(double, double) #0 {
top:
  %2 = fsub double %1, %0
  ret double %2
}</code></pre><h2><a class="nav-anchor" id="Reweighting-a-Margin-Loss-1" href="#Reweighting-a-Margin-Loss-1">Reweighting a Margin Loss</a></h2><p>It is not uncommon in classification scenarios to find yourself working with in-balanced data sets, where one class has much more observations than the other one. There are different strategies to deal with this kind of problem. The approach that this package provides is to weight the loss for the classes differently. This basically means that we penalize mistakes in one class more than mistakes in the other class. More specifically we scale the loss of the positive class by the weight-factor <span>$w$</span> and the loss of the negative class with <span>$1-w$</span>.</p><pre><code class="language-julia-repl">if target &gt; 0
    w * loss(target, output)
else
    (1-w) * loss(target, output)
end</code></pre><p>Instead of providing special functions to compute a class-weighted loss, we instead expose a generic way to create new weighted versions of already existing unweighted losses. This way, every existing subtype of <a href="../developer/#LearnBase.MarginLoss"><code>MarginLoss</code></a> can be re-weighted arbitrarily. Furthermore, it allows every algorithm that expects a binary loss to work with weighted binary losses as well.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LossFunctions.weightedloss" href="#LossFunctions.weightedloss"><code>LossFunctions.weightedloss</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">weightedloss(loss, weight)</code></pre><p>Returns a weighted version of <code>loss</code> for which the value of the positive class is changed to be <code>weight</code> times its original, and the negative class <code>1 - weight</code> times its original respectively.</p><p>Note: If <code>typeof(weight) &lt;: Number</code>, then this method will poison the type-inference of the calling scope. This is because <code>weight</code> will be promoted to a type parameter. For a typestable version use the following signature: <code>weightedloss(loss, Val(weight))</code></p></div></div></section><pre><code class="language-julia-repl">julia&gt; myloss = weightedloss(HingeLoss(), 0.8)
WeightedBinaryLoss{L1HingeLoss,0.8}(L1HingeLoss())

julia&gt; value(myloss, 1.0, -4.0) # positive class
4.0

julia&gt; value(HingeLoss(), 1.0, -4.0)
5.0

julia&gt; value(myloss, -1.0, 4.0) # negative class
0.9999999999999998

julia&gt; value(HingeLoss(), -1.0, 4.0)
5.0</code></pre><p>Note that the scaled version of a margin-based loss does not anymore belong to the family of margin-based losses itself. In other words the resulting loss is neither a subtype of <a href="../developer/#LearnBase.MarginLoss"><code>MarginLoss</code></a>, nor of the original type of loss.</p><pre><code class="language-julia-repl">julia&gt; typeof(myloss) &lt;: MarginLoss
false

julia&gt; typeof(myloss) &lt;: HingeLoss
false</code></pre><p>Similar to scaled losses, the constant weight factor gets promoted to a type-parameter. This can be quite an overhead when done on the fly every time the loss value is computed. To avoid this one can make use of <code>Val</code> to specify the scale factor in a type-stable manner.</p><pre><code class="language-julia-repl">julia&gt; myloss = weightedloss(HingeLoss(), Val(0.8))
WeightedBinaryLoss{L1HingeLoss,0.8}(L1HingeLoss())</code></pre><p>Storing the scale factor as a type-parameter instead of a member variable has a nice advantage. It makes it possible to define new types of losses using simple type-aliases.</p><pre><code class="language-julia-repl">julia&gt; const MyWeightedHingeLoss = LossFunctions.WeightedBinaryLoss{HingeLoss,0.8}
WeightedBinaryLoss{L1HingeLoss,0.8}

julia&gt; value(MyWeightedHingeLoss(), 1.0, -4.0)
4.0</code></pre><footer><hr/><a class="previous" href="../../losses/margin/"><span class="direction">Previous</span><span class="title">Margin-based Losses</span></a><a class="next" href="../developer/"><span class="direction">Next</span><span class="title">Developer Documentation</span></a></footer></article></body></html>
