<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home Â· LossFunctions.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><link href="assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.png" alt="LossFunctions.jl logo"/></a><h1>LossFunctions.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li><a class="toctext" href="#Introduction-and-Motivation-1">Introduction and Motivation</a></li><li><a class="toctext" href="#User&#39;s-Guide-1">User&#39;s Guide</a></li><li><a class="toctext" href="#Available-Losses-1">Available Losses</a></li><li><a class="toctext" href="#Advanced-Topics-1">Advanced Topics</a></li><li><a class="toctext" href="#Index-1">Index</a></li></ul></li><li><span class="toctext">Introduction</span><ul><li><a class="toctext" href="introduction/gettingstarted/">Getting Started</a></li><li><a class="toctext" href="introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="user/interface/">Working with Losses</a></li><li><a class="toctext" href="user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="toctext">Available Losses</span><ul><li><a class="toctext" href="losses/distance/">Distance-based Losses</a></li><li><a class="toctext" href="losses/margin/">Margin-based Losses</a></li></ul></li><li><span class="toctext">Advances Topics</span><ul><li><a class="toctext" href="advanced/extend/">Altering existing Losses</a></li><li><a class="toctext" href="advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="toctext" href="acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="LossFunctions.jl&#39;s-documentation-1" href="#LossFunctions.jl&#39;s-documentation-1">LossFunctions.jl&#39;s documentation</a></h1><p>This package represents a community effort to centralize the definition and implementation of <strong>loss functions</strong> in Julia. As such, it is a part of the <a href="https://github.com/JuliaML">JuliaML</a> ecosystem.</p><p>The sole purpose of this package is to provide an efficient and extensible implementation of various loss functions used throughout Machine Learning (ML). It is thus intended to serve as a special purpose back-end for other ML libraries that require losses to accomplish their tasks. To that end we provide a considerable amount of carefully implemented loss functions, as well as an API to query their properties (e.g. convexity). Furthermore, we expose methods to compute their values, derivatives, and second derivatives for single observations as well as arbitrarily sized arrays of observations. In the case of arrays a user additionally has the ability to define if and how element-wise results are averaged or summed over.</p><p>From an end-user&#39;s perspective one normally does not need to import this package directly. That said, it should provide a decent starting point for any student that is interested in investigating the properties or behaviour of loss functions.</p><h2><a class="nav-anchor" id="Introduction-and-Motivation-1" href="#Introduction-and-Motivation-1">Introduction and Motivation</a></h2><p>If this is the first time you consider using LossFunctions for your machine learning related experiments or packages, make sure to check out the &quot;Getting Started&quot; section.</p><ul><li><a href="introduction/gettingstarted/#Getting-Started-1">Getting Started</a></li><ul><li><a href="introduction/gettingstarted/#Installation-1">Installation</a></li><li><a href="introduction/gettingstarted/#Overview-1">Overview</a></li><li><a href="introduction/gettingstarted/#Getting-Help-1">Getting Help</a></li></ul></ul><p>If you are new to Machine Learning in Julia, or are simply interested in how and why this package works the way it works, feel free to take a look at the following sections. There we discuss the concepts involved and outline the most important terms and definitions.</p><ul><li><a href="introduction/motivation/#Background-and-Motivation-1">Background and Motivation</a></li><ul><li><a href="introduction/motivation/#Terminology-1">Terminology</a></li><li><a href="introduction/motivation/#Definitions-1">Definitions</a></li><li><a href="introduction/motivation/#Alternative-Viewpoints-1">Alternative Viewpoints</a></li><li><a href="introduction/motivation/#References-1">References</a></li></ul></ul><h2><a class="nav-anchor" id="User&#39;s-Guide-1" href="#User&#39;s-Guide-1">User&#39;s Guide</a></h2><p>This section gives a more detailed treatment of the exposed functions and their available methods. We will start by describing how to instantiate a loss, as well as the basic interface that all loss functions share.</p><ul><li><a href="user/interface/#Working-with-Losses-1">Working with Losses</a></li><ul><li><a href="user/interface/#Instantiating-a-Loss-1">Instantiating a Loss</a></li><li><a href="user/interface/#Computing-the-Values-1">Computing the Values</a></li><li><a href="user/interface/#Computing-the-1st-Derivatives-1">Computing the 1st Derivatives</a></li><li><a href="user/interface/#Computing-the-2nd-Derivatives-1">Computing the 2nd Derivatives</a></li><li><a href="user/interface/#Function-Closures-1">Function Closures</a></li><li><a href="user/interface/#Properties-of-a-Loss-1">Properties of a Loss</a></li></ul></ul><p>Next we will consider how to average or sum the results of the loss functions more efficiently. The methods described here are implemented in such a way as to avoid allocating a temporary array.</p><ul><li><a href="user/aggregate/#Efficient-Sum-and-Mean-1">Efficient Sum and Mean</a></li><ul><li><a href="user/aggregate/#Average-Modes-1">Average Modes</a></li><li><a href="user/aggregate/#Unweighted-Sum-and-Mean-1">Unweighted Sum and Mean</a></li><li><a href="user/aggregate/#Sum-and-Mean-per-Observation-1">Sum and Mean per Observation</a></li><li><a href="user/aggregate/#Weighted-Sum-and-Mean-1">Weighted Sum and Mean</a></li></ul></ul><h2><a class="nav-anchor" id="Available-Losses-1" href="#Available-Losses-1">Available Losses</a></h2><p>Aside from the interface, this package also provides a number of popular (and not so popular) loss functions out-of-the-box. Great effort has been put into ensuring a correct, efficient, and type-stable implementation for those. Most of them either belong to the family of distance-based or margin-based losses. These two categories are also indicative for if a loss is intended for regression or classification problems</p><h3><a class="nav-anchor" id="Loss-Functions-for-Regression-1" href="#Loss-Functions-for-Regression-1">Loss Functions for Regression</a></h3><p>Loss functions that belong to the category &quot;distance-based&quot; are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.</p><table><tbody><tr><td style="text-align: left;"><ul><li><a href="losses/distance/#Distance-based-Losses-1">Distance-based Losses</a></li><ul><li><a href="losses/distance/#LPDistLoss-1">LPDistLoss</a></li><li><a href="losses/distance/#L1DistLoss-1">L1DistLoss</a></li><li><a href="losses/distance/#L2DistLoss-1">L2DistLoss</a></li><li><a href="losses/distance/#LogitDistLoss-1">LogitDistLoss</a></li><li><a href="losses/distance/#HuberLoss-1">HuberLoss</a></li><li><a href="losses/distance/#L1EpsilonInsLoss-1">L1EpsilonInsLoss</a></li><li><a href="losses/distance/#L2EpsilonInsLoss-1">L2EpsilonInsLoss</a></li><li><a href="losses/distance/#PeriodicLoss-1">PeriodicLoss</a></li><li><a href="losses/distance/#QuantileLoss-1">QuantileLoss</a></li></ul></ul></td><td><p><img src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/distance.svg" alt="distance-based losses"/></p></td></tr></tbody></table><h3><a class="nav-anchor" id="Loss-Functions-for-Classification-1" href="#Loss-Functions-for-Classification-1">Loss Functions for Classification</a></h3><p>Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction. Instead they penalize predictions based on how well they agree with the sign of the target.</p><table><tbody><tr><td style="text-align: left;"><ul><li><a href="losses/margin/#Margin-based-Losses-1">Margin-based Losses</a></li><ul><li><a href="losses/margin/#ZeroOneLoss-1">ZeroOneLoss</a></li><li><a href="losses/margin/#PerceptronLoss-1">PerceptronLoss</a></li><li><a href="losses/margin/#L1HingeLoss-1">L1HingeLoss</a></li><li><a href="losses/margin/#SmoothedL1HingeLoss-1">SmoothedL1HingeLoss</a></li><li><a href="losses/margin/#ModifiedHuberLoss-1">ModifiedHuberLoss</a></li><li><a href="losses/margin/#DWDMarginLoss-1">DWDMarginLoss</a></li><li><a href="losses/margin/#L2MarginLoss-1">L2MarginLoss</a></li><li><a href="losses/margin/#L2HingeLoss-1">L2HingeLoss</a></li><li><a href="losses/margin/#LogitMarginLoss-1">LogitMarginLoss</a></li><li><a href="losses/margin/#ExpLoss-1">ExpLoss</a></li><li><a href="losses/margin/#SigmoidLoss-1">SigmoidLoss</a></li></ul></ul></td><td><p><img src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/margin.svg" alt="margin-based losses"/></p></td></tr></tbody></table><h2><a class="nav-anchor" id="Advanced-Topics-1" href="#Advanced-Topics-1">Advanced Topics</a></h2><p>In some situations it can be useful to slightly alter an existing loss function. We provide two general ways to accomplish that. The first way is to scale a loss by a constant factor. This can for example be useful to transform the <a href="losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a> into the least squares loss one knows from statistics. The second way is to reweight the two classes of a binary classification loss. This is useful for handling inbalanced class distributions.</p><ul><li><a href="advanced/extend/#Altering-existing-Losses-1">Altering existing Losses</a></li><ul><li><a href="advanced/extend/#Scaling-a-Supervised-Loss-1">Scaling a Supervised Loss</a></li><li><a href="advanced/extend/#Reweighting-a-Margin-Loss-1">Reweighting a Margin Loss</a></li></ul></ul><p>If you are interested in contributing to LossFunctions.jl, or simply want to understand how and why the package does then take a look at our developer documentation (although it is a bit sparse at the moment).</p><ul><li><a href="advanced/developer/#Developer-Documentation-1">Developer Documentation</a></li><ul><li><a href="advanced/developer/#Abstract-Types-1">Abstract Types</a></li><li><a href="advanced/developer/#Shared-Interface-1">Shared Interface</a></li></ul></ul><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><ul><li><a href="indices/#Functions-1">Functions</a></li><li><a href="indices/#Types-1">Types</a></li></ul></ul><footer><hr/><a class="next" href="introduction/gettingstarted/"><span class="direction">Next</span><span class="title">Getting Started</span></a></footer></article></body></html>
