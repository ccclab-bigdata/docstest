<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· MPI.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MPI.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Installing-1">Installing</a></li><li><a class="toctext" href="#Usage-:-MPI-Only-mode-1">Usage : MPI-Only mode</a></li><li><a class="toctext" href="#Usage-:-MPI-and-Julia-parallel-constructs-together-1">Usage : MPI and Julia parallel constructs together</a></li><li><a class="toctext" href="#Julia-MPI-only-interface-1">Julia MPI-only interface</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="MPI-interface-for-the-Julia-language-1" href="#MPI-interface-for-the-Julia-language-1">MPI interface for the Julia language</a></h1><p><a href="https://travis-ci.org/JuliaParallel/MPI.jl"><img src="https://travis-ci.org/JuliaParallel/MPI.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://ci.appveyor.com/project/eschnett/mpi-jl/branch/master"><img src="https://ci.appveyor.com/api/projects/status/e8mr8rx8sjryyba6/branch/master?svg=true" alt="Build status"/></a> <a href="https://codecov.io/github/JuliaParallel/MPI.jl?branch=master"><img src="https://codecov.io/github/JuliaParallel/MPI.jl/coverage.svg?branch=master" alt="codecov.io"/></a> <a href="https://coveralls.io/github/JuliaParallel/MPI.jl?branch=master"><img src="https://coveralls.io/repos/JuliaParallel/MPI.jl/badge.svg?branch=master&amp;service=github" alt="Coverage Status"/></a></p><p>This is a basic [Julia] wrapper for the portable message passing system Message Passing Interface ([MPI]). Inspiration is taken from mpi4py, although we generally follow the C and not the C++ MPI API. (The C++ MPI API is deprecated.)</p><h2><a class="nav-anchor" id="Installing-1" href="#Installing-1">Installing</a></h2><h3><a class="nav-anchor" id="Unix-systems-(OSX-and-Linux)-1" href="#Unix-systems-(OSX-and-Linux)-1">Unix systems (OSX and Linux)</a></h3><p>[CMake] is used to piece together the MPI wrapper. Currently a shared library MPI installation for C and Fortran is required (tested with [Open MPI] and [MPICH]). To install MPI.jl using the Julia packaging system, run</p><pre><code class="language-julia">Pkg.update()
Pkg.add(&quot;MPI&quot;)</code></pre><p>Alternatively,</p><pre><code class="language-julia">Pkg.clone(&quot;https://github.com/JuliaParallel/MPI.jl.git&quot;)
Pkg.build()</code></pre><p>which will build and install the wrapper into <code>$HOME/.julia/vX.Y/MPI</code>.</p><h4><a class="nav-anchor" id="Platform-specific-notes:-1" href="#Platform-specific-notes:-1">Platform specific notes:</a></h4><ul><li>If you are trying to build on OSX with Homebrew, the necessary Fortran headers are not included in the OpenMPI bottle.  To workaround this you can build OpenMPI from source: <code>brew install --build-from-source openmpi</code></li></ul><h4><a class="nav-anchor" id="Overriding-the-auto-detected-MPI-version-1" href="#Overriding-the-auto-detected-MPI-version-1">Overriding the auto-detected MPI version</a></h4><p>It may be that CMake selects the wrong MPI version, or that CMake fails to correctly detect and configure your MPI implementation. You can override CMake&#39;s mechanism by setting certain environment variables:</p><pre><code class="language-sh">JULIA_MPI_C_LIBRARIES
JULIA_MPI_Fortran_INCLUDE_PATH
JULIA_MPI_Fortran_LIBRARIES</code></pre><p>This will set <code>MPI_C_LIBRARIES</code>, <code>MPI_Fortran_INCLUDE_PATH</code>, and <code>MPI_Fortran_LIBRARIES</code> when calling CMake as described in its [FindMPI] module. You can set these variables either in your shell startup file, or e.g. via your <code>~/.juliarc</code> file. Here is an example:</p><pre><code class="language-Julia">ENV[&quot;JULIA_MPI_C_LIBRARIES&quot;] = &quot;-L/opt/local/lib/openmpi-gcc5 -lmpi&quot;
ENV[&quot;JULIA_MPI_Fortran_INCLUDE_PATH&quot;] = &quot;-I/opt/local/include&quot;
ENV[&quot;JULIA_MPI_Fortran_LIBRARIES&quot;] = &quot;-L/opt/local/lib/openmpi-gcc5 -lmpi_usempif08 -lmpi_mpifh -lmpi&quot;</code></pre><p>You can set other configuration variables as well (by adding a <code>JULIA_</code> prefix); the full list of variables currently supported is</p><pre><code class="language-sh">MPI_C_COMPILER
MPI_C_COMPILE_FLAGS
MPI_C_INCLUDE_PATH
MPI_C_LINK_FLAGS
MPI_C_LIBRARIES
MPI_Fortran_COMPILER
MPI_Fortran_COMPILE_FLAGS
MPI_Fortran_INCLUDE_PATH
MPI_Fortran_LINK_FLAGS
MPI_Fortran_LIBRARIES
MPI_INCLUDE_PATH
MPI_LIBRARIES</code></pre><h3><a class="nav-anchor" id="Windows-1" href="#Windows-1">Windows</a></h3><p>You need to install the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=49926">Microsoft MPI</a> runtime on your system (the SDK is not required). Then simply add the MPI.jl package with</p><pre><code class="language-julia">Pkg.update()
Pkg.add(&quot;MPI&quot;)</code></pre><p>If you would like to wrap an MPI function on Windows, keep in mind you may need to add its signature to <code>src/win_mpiconstants.jl</code>.</p><h2><a class="nav-anchor" id="Usage-:-MPI-Only-mode-1" href="#Usage-:-MPI-Only-mode-1">Usage : MPI-Only mode</a></h2><p>To run a Julia script with MPI, first make sure that <code>using MPI</code> or <code>import MPI</code> is included at the top of your script. You should then be able to run the MPI job as expected, e.g., with</p><p><code>mpirun -np 3 julia 01-hello.jl</code></p><h3><a class="nav-anchor" id="Cleanup-1" href="#Cleanup-1">Cleanup</a></h3><p>In Julia code building on this package, it may happen that you want to run MPI cleanup functions in a finalizer. This makes it impossible to manually call <code>MPI.Finalize()</code>, since the Julia finalizers may run after this call. To solve this, a C <code>atexit</code> hook to run <code>MPI.Finalize()</code> can be set using <code>MPI.finalize_atexit()</code>. It is possible to check if this function was called by checking the global <code>Ref</code> <code>MPI.FINALIZE_ATEXIT</code>.</p><h2><a class="nav-anchor" id="Usage-:-MPI-and-Julia-parallel-constructs-together-1" href="#Usage-:-MPI-and-Julia-parallel-constructs-together-1">Usage : MPI and Julia parallel constructs together</a></h2><p>In order for MPI calls to be made from a Julia cluster, it requires the use of MPIManager, a cluster manager that will start the julia workers using <code>mpirun</code></p><p>Currently MPIManager only works with Julia 0.4 . It has three modes of operation</p><ul><li><p>Only worker processes execute MPI code. The Julia master process executes outside of and is not part of the MPI cluster. Free bi-directional TCP/IP connectivity is required between all processes</p></li><li><p>All processes (including Julia master) are part of both the MPI as well as Julia cluster. Free bi-directional TCP/IP connectivity is required between all processes.</p></li><li><p>All processes are part of both the MPI as well as Julia cluster. MPI is used as the transport for julia messages. This is useful on environments which do not allow TCP/IP connectivity between worker processes</p></li></ul><h3><a class="nav-anchor" id="MPIManager-(only-workers-execute-MPI-code)-1" href="#MPIManager-(only-workers-execute-MPI-code)-1">MPIManager (only workers execute MPI code)</a></h3><p>An example is provided in <code>examples/05-juliacman.jl</code>. The julia master process is NOT part of the MPI cluster. The main script should be launched directly, MPIManager internally calls <code>mpirun</code> to launch julia/mpi workers. All the workers started via MPIManager will be part of the MPI cluster.</p><p><code>MPIManager(;np=Sys.CPU_THREADS, mpi_cmd=false, launch_timeout=60.0)</code></p><p>If not specified, <code>mpi_cmd</code> defaults to <code>mpirun -np $np</code> <code>stdout</code> from the launched workers is redirected back to the julia session calling <code>addprocs</code> via a TCP connection. Thus the workers must be able to freely connect via TCP to the host session. The following lines will be typically required on the julia master process to support both julia and mpi</p><pre><code class="language-none"># to import MPIManager
using MPI

# specify, number of mpi workers, launch cmd, etc.
manager=MPIManager(np=4)

# start mpi workers and add them as julia workers too.
addprocs(manager)
</code></pre><p>To execute code with MPI calls on all workers, use <code>@mpi_do</code>.</p><p><code>@mpi_do manager expr</code> executes <code>expr</code> on all processes that are part of <code>manager</code></p><p>For example: <code>@mpi_do manager (comm=MPI.COMM_WORLD; println(&quot;Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))&quot;)</code> executes on all mpi workers belonging to <code>manager</code> only</p><p><code>examples/05-juliacman.jl</code> is a simple example of calling MPI functions on all workers interspersed with Julia parallel methods. <code>cd</code> to the <code>examples</code> directory and run <code>julia 05-juliacman.jl</code></p><p>A single instation of <code>MPIManager</code> can be used only once to launch MPI workers (via <code>addprocs</code>). To create multiple sets of MPI clusters, use separate, distinct <code>MPIManager</code> objects.</p><p>procs(manager::MPIManager) returns a list of julia pids belonging to <code>manager</code> mpiprocs(manager::MPIManager) returns a list of MPI ranks belonging to <code>manager</code></p><p>Fields <code>j2mpi</code> and <code>mpi2j</code> of <code>MPIManager</code> are associative collections mapping julia pids to MPI ranks and vice-versa.</p><h3><a class="nav-anchor" id="MPIManager-1" href="#MPIManager-1">MPIManager</a></h3><h3><a class="nav-anchor" id="(TCP/IP-transport-all-processes-execute-MPI-code)-1" href="#(TCP/IP-transport-all-processes-execute-MPI-code)-1">(TCP/IP transport - all processes execute MPI code)</a></h3><ul><li>Useful on environments which do not allow TCP connections outside of the cluster</li><li>An example is in <code>examples/06-cman-transport.jl</code></li></ul><p><code>mpirun -np 5 julia 06-cman-transport.jl TCP</code></p><p>This launches a total of 5 processes, mpi rank 0 is the julia pid 1. mpi rank 1 is julia pid 2 and so on.</p><p>The program must call <code>MPI.start(TCP_TRANSPORT_ALL)</code> with argument <code>TCP_TRANSPORT_ALL</code>. On mpi rank 0, it returns a <code>manager</code> which can be used with <code>@mpi_do</code> On other processes (i.e., the workers) the function does not return</p><h3><a class="nav-anchor" id="MPIManager-2" href="#MPIManager-2">MPIManager</a></h3><h3><a class="nav-anchor" id="(MPI-transport-all-processes-execute-MPI-code)-1" href="#(MPI-transport-all-processes-execute-MPI-code)-1">(MPI transport - all processes execute MPI code)</a></h3><p><code>MPI.start</code> must be called with option <code>MPI_TRANSPORT_ALL</code> to use MPI as transport. <code>mpirun -np 5 julia 06-cman-transport.jl MPI</code> will run the example using MPI as transport.</p><h2><a class="nav-anchor" id="Julia-MPI-only-interface-1" href="#Julia-MPI-only-interface-1">Julia MPI-only interface</a></h2><h3><a class="nav-anchor" id="Communicators:-1" href="#Communicators:-1">Communicators:</a></h3><p>Julia interfaces to the Fortran versions of the MPI functions. Since the C and Fortran communicators are different, if a C communicator is required (e.g., to interface with a C library), this can be achieved with the Fortran to C communicator conversion:</p><pre><code class="language-none">juliacomm = MPI.COMM_WORLD
ccomm = MPI.CComm(juliacomm)</code></pre><h3><a class="nav-anchor" id="Currently-wrapped-MPI-functions-1" href="#Currently-wrapped-MPI-functions-1">Currently wrapped MPI functions</a></h3><p>Convention: <code>MPI_Fun =&gt; MPI.Fun</code></p><p>Constants like <code>MPI_SUM</code> are wrapped as <code>MPI.SUM</code>.   Note also that arbitrary Julia functions <code>f(x,y)</code> can be passed as reduction operations to the MPI <code>Allreduce</code> and <code>Reduce</code> functions.</p><h4><a class="nav-anchor" id="Administrative-functions-1" href="#Administrative-functions-1">Administrative functions</a></h4><table><tr><th>Julia Function (assuming <code>import MPI</code>)</th><th>Fortran Function</th></tr><tr><td><code>MPI.Abort</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Abort.3.php"><code>MPI_Abort</code></a></td></tr><tr><td><code>MPI.Comm_dup</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_dup.3.php"><code>MPI_Comm_dup</code></a></td></tr><tr><td><code>MPI.Comm_free</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_free.3.php"><code>MPI_Comm_free</code></a></td></tr><tr><td><code>MPI.Comm_get_parent</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Comm_get_parent.3.php"><code>MPI_Comm_get_parent</code></a></td></tr><tr><td><code>MPI.Comm_rank</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_rank.3.php"><code>MPI_Comm_rank</code></a></td></tr><tr><td><code>MPI.Comm_size</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_size.3.php"><code>MPI_Comm_size</code></a></td></tr><tr><td><code>MPI.Comm_spawn</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Comm_spawn.3.php"><code>MPI_Comm_spawn</code></a></td></tr><tr><td><code>MPI.Finalize</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Finalize.3.php"><code>MPI_Finalize</code></a></td></tr><tr><td><code>MPI.Finalized</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Finalized.3.php"><code>MPI_Finalized</code></a></td></tr><tr><td><code>MPI.Get_address</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_address.3.php"><code>MPI_Get_address</code></a></td></tr><tr><td><code>MPI.Init</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Init.3.php"><code>MPI_Init</code></a></td></tr><tr><td><code>MPI.Initialized</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Initialized.3.php"><code>MPI_Initialized</code></a></td></tr><tr><td><code>MPI.Intercomm_merge</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Intercomm_merge.3.php"><code>MPI_Intercomm_merge</code></a></td></tr><tr><td><code>MPI.mpitype</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_create_struct.3.php"><code>MPI_Type_create_struct</code></a> and <a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_commit.3.php"><code>MPI_Type_commit</code></a> (see: <a href="#mpitypenote">mpitype note</a>)</td></tr></table><p>&lt;a name=&quot;mpitypenote&quot;&gt;mpitype note&lt;/a&gt;: This is not strictly a wrapper for <code>MPI_Type_create_struct</code> and <code>MPI_Type_commit</code>, it also is an accessor for previously created types.</p><h4><a class="nav-anchor" id="Point-to-point-communication-1" href="#Point-to-point-communication-1">Point-to-point communication</a></h4><table><tr><th>Julia Function (assuming <code>import MPI</code>)</th><th>Fortran Function</th></tr><tr><td><code>MPI.Cancel!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Cancel.3.php"><code>MPI_Cancel</code></a></td></tr><tr><td><code>MPI.Get_count</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Get_count.3.php"><code>MPI_Get_count</code></a></td></tr><tr><td><code>MPI.Iprobe</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Iprobe.3.php"><code>MPI_Iprobe</code></a></td></tr><tr><td><code>MPI.Irecv!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Irecv.3.php"><code>MPI_Irecv</code></a></td></tr><tr><td><code>MPI.Isend</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Isend.3.php"><code>MPI_Isend</code></a></td></tr><tr><td><code>MPI.Probe</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Probe.3.php"><code>MPI_Probe</code></a></td></tr><tr><td><code>MPI.Recv!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Recv.3.php"><code>MPI_Recv</code></a></td></tr><tr><td><code>MPI.Send</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Send.3.php"><code>MPI_Send</code></a></td></tr><tr><td><code>MPI.Test!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Test.3.php"><code>MPI_Test</code></a></td></tr><tr><td><code>MPI.Testall!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testall.3.php"><code>MPI_Testall</code></a></td></tr><tr><td><code>MPI.Testany!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testany.3.php"><code>MPI_Testany</code></a></td></tr><tr><td><code>MPI.Testsome!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testsome.3.php"><code>MPI_Testsome</code></a></td></tr><tr><td><code>MPI.Wait!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Wait.3.php"><code>MPI_Wait</code></a></td></tr><tr><td><code>MPI.Waitall!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitall.3.php"><code>MPI_Waitall</code></a></td></tr><tr><td><code>MPI.Waitany!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitany.3.php"><code>MPI_Waitany</code></a></td></tr><tr><td><code>MPI.Waitsome!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitsome.3.php"><code>MPI_Waitsome</code></a></td></tr></table><h4><a class="nav-anchor" id="Collective-communication-1" href="#Collective-communication-1">Collective communication</a></h4><table><tr><th>Julia Function (assuming <code>import MPI</code>)</th><th>Fortran Function</th></tr><tr><td><code>MPI.Allgather</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgather.3.php"><code>MPI_Allgather</code></a></td></tr><tr><td><code>MPI.Allgatherv</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgatherv.3.php"><code>MPI_Allgatherv</code></a></td></tr><tr><td><code>MPI.Alltoall</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoall.3.php"><code>MPI_Alltoall</code></a></td></tr><tr><td><code>MPI.Alltoallv</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoallv.3.php"><code>MPI_Alltoallv</code></a></td></tr><tr><td><code>MPI.Barrier</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Barrier.3.php"><code>MPI_Barrier</code></a></td></tr><tr><td><code>MPI.Bcast!</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Bcast.3.php"><code>MPI_Bcast</code></a></td></tr><tr><td><code>MPI.Exscan</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Exscan.3.php"><code>MPI_Exscan</code></a></td></tr><tr><td><code>MPI.Gather</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gather.3.php"><code>MPI_Gather</code></a></td></tr><tr><td><code>MPI.Gatherv</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gatherv.3.php"><code>MPI_Gatherv</code></a></td></tr><tr><td><code>MPI.Reduce</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Reduce.3.php"><code>MPI_Reduce</code></a></td></tr><tr><td><code>MPI.Scan</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scan.3.php"><code>MPI_Scan</code></a></td></tr><tr><td><code>MPI.Scatter</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatter.3.php"><code>MPI_Scatter</code></a></td></tr><tr><td><code>MPI.Scatterv</code></td><td><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatterv.3.php"><code>MPI_Scatterv</code></a></td></tr></table><h4><a class="nav-anchor" id="One-sided-communication-1" href="#One-sided-communication-1">One-sided communication</a></h4><table><tr><th>Julia Function (assuming <code>import MPI</code>)</th><th>Fortran Function</th></tr><tr><td><code>MPI.Win_create</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create.3.php"><code>MPI_Win_create</code></a></td></tr><tr><td><code>MPI.Win_create_dynamic</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create_dynamic.3.php"><code>MPI_Win_create_dynamic</code></a></td></tr><tr><td><code>MPI.Win_attach</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_attach.3.php"><code>MPI_Win_attach</code></a></td></tr><tr><td><code>MPI.Win_detach</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_detach.3.php"><code>MPI_Win_detach</code></a></td></tr><tr><td><code>MPI.Win_fence</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_fence.3.php"><code>MPI_Win_fence</code></a></td></tr><tr><td><code>MPI.Win_flush</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_flush.3.php"><code>MPI_Win_flush</code></a></td></tr><tr><td><code>MPI.Win_free</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_free.3.php"><code>MPI_Win_free</code></a></td></tr><tr><td><code>MPI.Win_sync</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_sync.3.php"><code>MPI_Win_sync</code></a></td></tr><tr><td><code>MPI.Win_lock</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_lock.3.php"><code>MPI_Win_lock</code></a></td></tr><tr><td><code>MPI.Win_unlock</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_unlock.3.php"><code>MPI_Win_unlock</code></a></td></tr><tr><td><code>MPI.Get</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get.3.php"><code>MPI_Get</code></a></td></tr><tr><td><code>MPI.Put</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Put.3.php"><code>MPI_Put</code></a></td></tr><tr><td><code>MPI.Fetch_and_op</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Fetch_and_op.3.php"><code>MPI_Fetch_and_op</code></a></td></tr><tr><td><code>MPI.Accumulate</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Accumulate.3.php"><code>MPI_Accumulate</code></a></td></tr><tr><td><code>MPI.Get_accumulate</code></td><td><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_accumulate.3.php"><code>MPI_Get_accumulate</code></a></td></tr></table><p>[Julia]: http://julialang.org/ [MPI]: http://www.mpi-forum.org/ [mpi4py]: http://mpi4py.scipy.org [CMake]: http://www.cmake.org/ [FindMPI]: http://www.cmake.org/cmake/help/v3.3/module/FindMPI.html [Open MPI]: http://www.open-mpi.org/ [MPICH]: http://www.mpich.org/</p><footer><hr/></footer></article></body></html>
