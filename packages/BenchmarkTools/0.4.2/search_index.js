var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Readme",
    "title": "Readme",
    "category": "page",
    "text": ""
},

{
    "location": "#BenchmarkTools.jl-1",
    "page": "Readme",
    "title": "BenchmarkTools.jl",
    "category": "section",
    "text": "(Image: Build Status) (Image: Coverage Status)BenchmarkTools makes performance tracking of Julia code easy by supplying a framework for writing and running groups of benchmarks as well as comparing benchmark results.This package is used to write and run the benchmarks found in BaseBenchmarks.jl.The CI infrastructure for automated performance testing of the Julia language is not in this package, but can be found in Nanosoldier.jl."
},

{
    "location": "#Installation-1",
    "page": "Readme",
    "title": "Installation",
    "category": "section",
    "text": "To install BenchmarkTools, you can run the following:Pkg.add(\"BenchmarkTools\")"
},

{
    "location": "#Documentation-1",
    "page": "Readme",
    "title": "Documentation",
    "category": "section",
    "text": "If you\'re just getting started, check out the manual for a thorough explanation of BenchmarkTools.If you want to explore the BenchmarkTools API, see the reference document.If you want a short example of a toy benchmark suite, see the sample file in this repo (benchmark/benchmarks.jl).If you want an extensive example of a benchmark suite being used in the real world, you can look at the source code of BaseBenchmarks.jl.If you\'re benchmarking on Linux, I wrote up a series of tips and tricks to help eliminate noise during performance tests."
},

{
    "location": "#Quick-Start-1",
    "page": "Readme",
    "title": "Quick Start",
    "category": "section",
    "text": "The simplest usage is via the @btime macro, which is analogous to Julia\'s built-in @time macro but is often more accurate (by collecting results over multiple runs):julia> using BenchmarkTools\n\njulia> @btime sin(1)\n  15.081 ns (0 allocations: 0 bytes)\n0.8414709848078965If the expression you want to benchmark depends on external variables, you should use $ to \"interpolate\" them into the benchmark expression to avoid the problems of benchmarking with globals.  Essentially, any interpolated variable $x or expression $(...) is \"pre-computed\" before benchmarking begins:julia> A = rand(3,3);\n\njulia> @btime inv($A);            # we interpolate the global variable A with $A\n  1.191 μs (10 allocations: 2.31 KiB)\n  \njulia> @btime inv($(rand(3,3)));  # interpolation: the rand(3,3) call occurs before benchmarking\n  1.192 μs (10 allocations: 2.31 KiB)\n  \njulia> @btime inv(rand(3,3));     # the rand(3,3) call is included in the benchmark time\n  1.295 μs (11 allocations: 2.47 KiB)As described the manual, the BenchmarkTools package supports many other features, both for additional output and for more fine-grained control over the benchmarking process."
},

{
    "location": "#Why-does-this-package-exist?-1",
    "page": "Readme",
    "title": "Why does this package exist?",
    "category": "section",
    "text": "Our story begins with two packages, \"Benchmarks\" and \"BenchmarkTrackers\". The Benchmarks package implemented an execution strategy for collecting and summarizing individual benchmark results, while BenchmarkTrackers implemented a framework for organizing, running, and determining regressions of groups of benchmarks. Under the hood, BenchmarkTrackers relied on Benchmarks for actual benchmark execution.For a while, the Benchmarks + BenchmarkTrackers system was used for automated performance testing of Julia\'s Base library. It soon became apparent that the system suffered from a variety of issues:Individual sample noise could significantly change the execution strategy used to collect further samples.\nThe estimates used to characterize benchmark results and to detect regressions were statistically vulnerable to noise (i.e. not robust).\nDifferent benchmarks have different noise tolerances, but there was no way to tune this parameter on a per-benchmark basis.\nRunning benchmarks took a long time - an order of magnitude longer than theoretically necessary for many functions.\nUsing the system in the REPL (for example, to reproduce regressions locally) was often cumbersome.The BenchmarkTools package is a response to these issues, designed by examining user reports and the benchmark data generated by the old system. BenchmarkTools offers the following solutions to the corresponding issues above:Benchmark execution parameters are configured separately from the execution of the benchmark itself. This means that subsequent experiments are performed more consistently, avoiding branching \"substrategies\" based on small numbers of samples.\nA variety of simple estimators are supported, and the user can pick which one to use for regression detection.\nNoise tolerance has been made a per-benchmark configuration parameter.\nBenchmark configuration parameters can be easily cached and reloaded, significantly reducing benchmark execution time.\nThe API is simpler, more transparent, and overall easier to use."
},

{
    "location": "#Acknowledgements-1",
    "page": "Readme",
    "title": "Acknowledgements",
    "category": "section",
    "text": "This package was authored primarily by Jarrett Revels (@jrevels). Additionally, I\'d like to thank the following people:John Myles White, for authoring the original Benchmarks package, which greatly inspired BenchmarkTools\nAndreas Noack, for statistics help and investigating weird benchmark time distributions\nOscar Blumberg, for discussions on noise robustness\nJiahao Chen, for discussions on error analysis"
},

{
    "location": "autodocs/#BenchmarkTools.@belapsed",
    "page": "Docstrings",
    "title": "BenchmarkTools.@belapsed",
    "category": "macro",
    "text": "@belapsed expression [other parameters...]\n\nSimilar to the @elapsed macro included with Julia, this returns the elapsed time (in seconds) to execute a given expression.   It uses the @benchmark macro, however, and accepts all of the same additional parameters as @benchmark.  The returned time is the minimum elapsed time measured during the benchmark.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#BenchmarkTools.@btime",
    "page": "Docstrings",
    "title": "BenchmarkTools.@btime",
    "category": "macro",
    "text": "@btime expression [other parameters...]\n\nSimilar to the @time macro included with Julia, this executes an expression, printing the time it took to execute and the memory allocated before returning the value of the expression.\n\nUnlike @time, it uses the @benchmark macro, and accepts all of the same additional parameters as @benchmark.  The printed time is the minimum elapsed time measured during the benchmark.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#",
    "page": "Docstrings",
    "title": "Docstrings",
    "category": "page",
    "text": "BenchmarkTools.@belapsedBenchmarkTools.@benchmarkBenchmarkTools.@benchmarkableBenchmarkTools.@btimeBenchmarkTools.@taggedBenchmarkTools.BENCHMARKTOOLS_VERSIONBenchmarkTools.BenchmarkBenchmarkTools.BenchmarkGroupBenchmarkTools.BenchmarkToolsBenchmarkTools.DEFAULT_PARAMETERSBenchmarkTools.EVALSBenchmarkTools.ParametersBenchmarkTools.RESOLUTIONBenchmarkTools.SUPPORTED_TYPESBenchmarkTools.TagFilterBenchmarkTools.TrialBenchmarkTools.TrialEstimateBenchmarkTools.TrialJudgementBenchmarkTools.TrialRatioBenchmarkTools.VERSIONSBenchmarkTools._lineartrialBenchmarkTools._runBenchmarkTools._showBenchmarkTools._summaryBenchmarkTools.addgroup!BenchmarkTools.allocsBenchmarkTools.andexprBenchmarkTools.andreduceBenchmarkTools.badextBenchmarkTools.benchmarkable_partsBenchmarkTools.collectvarsBenchmarkTools.colormapBenchmarkTools.createchild!BenchmarkTools.estimate_overheadBenchmarkTools.evalBenchmarkTools.filtervalsBenchmarkTools.filtervals!BenchmarkTools.gcratioBenchmarkTools.gcscrubBenchmarkTools.gctimeBenchmarkTools.generate_benchmark_definitionBenchmarkTools.guessevalsBenchmarkTools.hasevalsBenchmarkTools.improvementsBenchmarkTools.includeBenchmarkTools.invariantsBenchmarkTools.isimprovementBenchmarkTools.isinvariantBenchmarkTools.isregressionBenchmarkTools.judgeBenchmarkTools.keyunionBenchmarkTools.leavesBenchmarkTools.leaves!BenchmarkTools.lineartrialBenchmarkTools.loadBenchmarkTools.loadparams!BenchmarkTools.loadtagged!BenchmarkTools.logisticBenchmarkTools.mapvalsBenchmarkTools.mapvals!BenchmarkTools.memoryBenchmarkTools.nullfuncBenchmarkTools.overhead_sampleBenchmarkTools.paramsBenchmarkTools.prettydiffBenchmarkTools.prettymemoryBenchmarkTools.prettypercentBenchmarkTools.prettytimeBenchmarkTools.printmemoryjudgeBenchmarkTools.printtimejudgeBenchmarkTools.prunekwargsBenchmarkTools.quasiquote!BenchmarkTools.ratioBenchmarkTools.recoverBenchmarkTools.regressionsBenchmarkTools.rmskewBenchmarkTools.rmskew!BenchmarkTools.run_resultBenchmarkTools.sampleBenchmarkTools.saveBenchmarkTools.skewcutoffBenchmarkTools.tagpredicate!BenchmarkTools.tagreprBenchmarkTools.tagunionBenchmarkTools.trimBenchmarkTools.tune!BenchmarkTools.warmupBenchmarkTools.withtypename"
},

]}
