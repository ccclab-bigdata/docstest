<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · BenchmarkTools.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>BenchmarkTools.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Documentation-1">Documentation</a></li><li><a class="toctext" href="#Quick-Start-1">Quick Start</a></li><li><a class="toctext" href="#Why-does-this-package-exist?-1">Why does this package exist?</a></li><li><a class="toctext" href="#Acknowledgements-1">Acknowledgements</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="BenchmarkTools.jl-1" href="#BenchmarkTools.jl-1">BenchmarkTools.jl</a></h1><p><a href="https://travis-ci.org/JuliaCI/BenchmarkTools.jl"><img src="https://travis-ci.org/JuliaCI/BenchmarkTools.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/JuliaCI/BenchmarkTools.jl?branch=master"><img src="https://coveralls.io/repos/github/JuliaCI/BenchmarkTools.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><p>BenchmarkTools makes <strong>performance tracking of Julia code easy</strong> by supplying a framework for <strong>writing and running groups of benchmarks</strong> as well as <strong>comparing benchmark results</strong>.</p><p>This package is used to write and run the benchmarks found in <a href="https://github.com/JuliaCI/BaseBenchmarks.jl">BaseBenchmarks.jl</a>.</p><p>The CI infrastructure for automated performance testing of the Julia language is not in this package, but can be found in <a href="https://github.com/JuliaCI/Nanosoldier.jl">Nanosoldier.jl</a>.</p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>To install BenchmarkTools, you can run the following:</p><pre><code class="language-julia">Pkg.add(&quot;BenchmarkTools&quot;)</code></pre><h2><a class="nav-anchor" id="Documentation-1" href="#Documentation-1">Documentation</a></h2><p>If you&#39;re just getting started, check out the <a href="doc/manual.md">manual</a> for a thorough explanation of BenchmarkTools.</p><p>If you want to explore the BenchmarkTools API, see the <a href="doc/reference.md">reference document</a>.</p><p>If you want a short example of a toy benchmark suite, see the sample file in this repo (<a href="benchmark/benchmarks.jl">benchmark/benchmarks.jl</a>).</p><p>If you want an extensive example of a benchmark suite being used in the real world, you can look at the source code of <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/tree/nanosoldier">BaseBenchmarks.jl</a>.</p><p>If you&#39;re benchmarking on Linux, I wrote up a series of <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/linuxtips.md">tips and tricks</a> to help eliminate noise during performance tests.</p><h2><a class="nav-anchor" id="Quick-Start-1" href="#Quick-Start-1">Quick Start</a></h2><p>The simplest usage is via the <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#benchmarking-basics"><code>@btime</code> macro</a>, which is analogous to Julia&#39;s built-in <a href="https://docs.julialang.org/en/stable/stdlib/base/#Base.@time"><code>@time</code> macro</a> but is often more accurate (by collecting results over multiple runs):</p><pre><code class="language-julia">julia&gt; using BenchmarkTools

julia&gt; @btime sin(1)
  15.081 ns (0 allocations: 0 bytes)
0.8414709848078965</code></pre><p>If the expression you want to benchmark depends on external variables, you should use <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#interpolating-values-into-benchmark-expressions"><code>$</code> to &quot;interpolate&quot;</a> them into the benchmark expression to <a href="https://docs.julialang.org/en/stable/manual/performance-tips/#Avoid-global-variables-1">avoid the problems of benchmarking with globals</a>.  Essentially, any interpolated variable <code>$x</code> or expression <code>$(...)</code> is &quot;pre-computed&quot; before benchmarking begins:</p><pre><code class="language-julia">julia&gt; A = rand(3,3);

julia&gt; @btime inv($A);            # we interpolate the global variable A with $A
  1.191 μs (10 allocations: 2.31 KiB)
  
julia&gt; @btime inv($(rand(3,3)));  # interpolation: the rand(3,3) call occurs before benchmarking
  1.192 μs (10 allocations: 2.31 KiB)
  
julia&gt; @btime inv(rand(3,3));     # the rand(3,3) call is included in the benchmark time
  1.295 μs (11 allocations: 2.47 KiB)</code></pre><p>As described the <a href="doc/manual.md">manual</a>, the BenchmarkTools package supports many other features, both for additional output and for more fine-grained control over the benchmarking process.</p><h2><a class="nav-anchor" id="Why-does-this-package-exist?-1" href="#Why-does-this-package-exist?-1">Why does this package exist?</a></h2><p>Our story begins with two packages, &quot;Benchmarks&quot; and &quot;BenchmarkTrackers&quot;. The Benchmarks package implemented an execution strategy for collecting and summarizing individual benchmark results, while BenchmarkTrackers implemented a framework for organizing, running, and determining regressions of groups of benchmarks. Under the hood, BenchmarkTrackers relied on Benchmarks for actual benchmark execution.</p><p>For a while, the Benchmarks + BenchmarkTrackers system was used for automated performance testing of Julia&#39;s Base library. It soon became apparent that the system suffered from a variety of issues:</p><ol><li>Individual sample noise could significantly change the execution strategy used to collect further samples.</li><li>The estimates used to characterize benchmark results and to detect regressions were statistically vulnerable to noise (i.e. not robust).</li><li>Different benchmarks have different noise tolerances, but there was no way to tune this parameter on a per-benchmark basis.</li><li>Running benchmarks took a long time - an order of magnitude longer than theoretically necessary for many functions.</li><li>Using the system in the REPL (for example, to reproduce regressions locally) was often cumbersome.</li></ol><p>The BenchmarkTools package is a response to these issues, designed by examining user reports and the benchmark data generated by the old system. BenchmarkTools offers the following solutions to the corresponding issues above:</p><ol><li>Benchmark execution parameters are configured separately from the execution of the benchmark itself. This means that subsequent experiments are performed more consistently, avoiding branching &quot;substrategies&quot; based on small numbers of samples.</li><li>A variety of simple estimators are supported, and the user can pick which one to use for regression detection.</li><li>Noise tolerance has been made a per-benchmark configuration parameter.</li><li>Benchmark configuration parameters can be easily cached and reloaded, significantly reducing benchmark execution time.</li><li>The API is simpler, more transparent, and overall easier to use.</li></ol><h2><a class="nav-anchor" id="Acknowledgements-1" href="#Acknowledgements-1">Acknowledgements</a></h2><p>This package was authored primarily by Jarrett Revels (@jrevels). Additionally, I&#39;d like to thank the following people:</p><ul><li>John Myles White, for authoring the original Benchmarks package, which greatly inspired BenchmarkTools</li><li>Andreas Noack, for statistics help and investigating weird benchmark time distributions</li><li>Oscar Blumberg, for discussions on noise robustness</li><li>Jiahao Chen, for discussions on error analysis</li></ul><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
