<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial Â· ReinforcementLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li class="current"><a class="toctext" href>Tutorial</a><ul class="internal"><li><a class="toctext" href="#Write-your-own-learner-1">Write your own learner</a></li><li><a class="toctext" href="#api_environments-1">Bind your own environment</a></li><li><a class="toctext" href="#Preprocessors-1">Preprocessors</a></li><li><a class="toctext" href="#Policies-1">Policies</a></li><li><a class="toctext" href="#Callbacks-1">Callbacks</a></li><li><a class="toctext" href="#Stopping-Criteria-1">Stopping Criteria</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../preprocessors/">Preprocessors</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tutorial</a></li></ul></nav><hr/><div id="topbar"><span>Tutorial</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h1><p>You would like to test existing reinforcement learning methods on your environment or try your method on existing environments? Extending this package is a piece of cake. Please consider registering the binding to your own environment as a new package (see e.g. <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentAtari.jl">ReinforcementLearningEnvironmentAtari</a>) and open a <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pulls">pull request</a> for any other extension.</p><h2><a class="nav-anchor" id="Write-your-own-learner-1" href="#Write-your-own-learner-1">Write your own learner</a></h2><p>For a new learner you need to implement the functions</p><pre><code class="language-none">update!(learner, buffer)                          # returns nothing
defaultpolicy(learner, actionspace, buffer)       # returns a policy
defaultbuffer(learner, environment, preprocessor) # returns a buffer</code></pre><p>Let&#39;s assume you want to implement plain, simple Q-learning (you don&#39;t need to do this; it is already implemented. Your file <code>qlearning.jl</code> could contain</p><pre><code class="language-julia">import ReinforcementLearning: update!, defaultpolicy, defaultbuffer, Buffer

struct MyQLearning
    Q::Array{Float64, 2} # number of actions x number of states
    alpha::Float64       # learning rate
end

function update!(learner::MyQLearning, buffer)
    s = buffer.states[1]
    snext = buffer.states[2]
    r = buffer.rewards[1]
    a = buffer.actions[1]
    Q = learner.Q
    Q[a, s] += learner.alpha * (r + maximum(Q[:, snext]) - Q[a, s])
end

function defaultpolicy(learner::MyQLearning, actionspace, buffer)
    EpsilonGreedyPolicy(.1, actionspace, s -&gt; getvalue(learner.params, s))
end

function defaultbuffer(learner::MyQLearning, environment, preprocessor)
    state, done = getstate(environment)
    processedstate = preprocessstate(preprocessor, state)
    Buffer(statetype = typeof(processedstate), capacity = 2)
end</code></pre><p>The functions <code>defaultpolicy</code> and <code>defaultbuffer</code> get called during the construction of an <code>RLSetup</code>. <code>defaultbuffer</code> returns a buffer that is filled with states, actions and rewards during interaction with the environment. Currently there are three types of Buffers implemented</p><pre><code class="language-julia">import ReinforcementLearning: Buffer, EpisodeBuffer, ArrayStateBuffer
?Buffer</code></pre><h2><a class="nav-anchor" id="api_environments-1" href="#api_environments-1">Bind your own environment</a></h2><p>For new environments you need to implement the functions</p><pre><code class="language-none">interact!(action, environment)          # returns state, reward done
getstate(environment)                   # returns state, done
reset!(environment)                     # returns state</code></pre><p>Optionally you may also implement the function</p><pre><code class="language-none">plotenv(environment)</code></pre><p>Please have a look at the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentClassicControl.jl/blob/master/src/cartpole.jl">cartpole</a> for an example.</p><h2><a class="nav-anchor" id="Preprocessors-1" href="#Preprocessors-1">Preprocessors</a></h2><pre><code class="language-none">preprocessstate(preprocessor, state)    # returns the preprocessed state</code></pre><p>Optional:</p><pre><code class="language-none">preprocess(preprocessor, reward, state, done) # returns a preprocessed (state, reward done) tuple.</code></pre><h2><a class="nav-anchor" id="Policies-1" href="#Policies-1">Policies</a></h2><p>Policies are function-like objects. To implement for example a policy that returns (the action) <code>42</code> for every possible input <code>state</code> one could write</p><pre><code class="language-none">struct MyPolicy end
(p::MyPolicy)(state) = 42</code></pre><h2><a class="nav-anchor" id="Callbacks-1" href="#Callbacks-1">Callbacks</a></h2><pre><code class="language-none">callback!(callback, rlsetup, state, action, reward, done) # returns nothing</code></pre><h2><a class="nav-anchor" id="Stopping-Criteria-1" href="#Stopping-Criteria-1">Stopping Criteria</a></h2><pre><code class="language-none">isbreak!(stoppingcriterion, state, action, reward, done) # returns true of false</code></pre><footer><hr/><a class="previous" href="../usage/"><span class="direction">Previous</span><span class="title">Usage</span></a><a class="next" href="../comparison/"><span class="direction">Next</span><span class="title">Comparison</span></a></footer></article></body></html>
