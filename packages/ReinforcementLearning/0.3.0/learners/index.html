<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learners · ReinforcementLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li class="current"><a class="toctext" href>Learners</a><ul class="internal"><li><a class="toctext" href="#TD-Learner-1">TD Learner</a></li><li><a class="toctext" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></li><li><a class="toctext" href="#N-step-Learner-1">N-step Learner</a></li><li><a class="toctext" href="#Model-Based-Learner-1">Model Based Learner</a></li><li><a class="toctext" href="#Deep-Reinforcement-Learning-1">Deep Reinforcement Learning</a></li></ul></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../preprocessors/">Preprocessors</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Learners</a></li></ul></nav><hr/><div id="topbar"><span>Learners</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="learners-1" href="#learners-1">Learners</a></h1><h2><a class="nav-anchor" id="TD-Learner-1" href="#TD-Learner-1">TD Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ExpectedSarsa-Tuple{}" href="#ReinforcementLearning.ExpectedSarsa-Tuple{}"><code>ReinforcementLearning.ExpectedSarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ExpectedSarsa(; kargs...) = TDLearner(; endvaluepolicy = ExpectedSarsaEndPolicy(VeryOptimisticEpsilonGreedyPolicy(.1)), kargs...)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.QLearning-Tuple{}" href="#ReinforcementLearning.QLearning-Tuple{}"><code>ReinforcementLearning.QLearning</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">QLearning(; kargs...) = TDLearner(; endvaluepolicy = QLearningEndPolicy(), kargs...)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Sarsa-Tuple{}" href="#ReinforcementLearning.Sarsa-Tuple{}"><code>ReinforcementLearning.Sarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">Sarsa(; kargs...) = TDLearner(; kargs...)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AccumulatingTraces" href="#ReinforcementLearning.AccumulatingTraces"><code>ReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct AccumulatingTraces &lt;: AbstractTraces
    λ::Float64
    γλ::Float64
    trace::Array{Float64, 2}
    minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to <span>$e(a, s) ←  1 + e(a, s)$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AccumulatingTraces-Tuple{}" href="#ReinforcementLearning.AccumulatingTraces-Tuple{}"><code>ReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">AccumulatingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.NoTraces" href="#ReinforcementLearning.NoTraces"><code>ReinforcementLearning.NoTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct NoTraces &lt;: AbstractTraces</code></pre><p>No eligibility traces, i.e. <span>$e(a, s) = 1$</span> for current action <span>$a$</span> and state <span>$s$</span> and zero otherwise.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ReplacingTraces" href="#ReinforcementLearning.ReplacingTraces"><code>ReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct ReplacingTraces &lt;: AbstractTraces
    λ::Float64
    γλ::Float64
    trace::Array{Float64, 2}
    minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to <span>$e(a, s) ←  1$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ReplacingTraces-Tuple{}" href="#ReinforcementLearning.ReplacingTraces-Tuple{}"><code>ReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ReplacingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div></div></section><h3><a class="nav-anchor" id="initunseen-1" href="#initunseen-1">Initial values, novel actions and unseen values</a></h3><p>For td-error dependent methods, the exploration-exploitation trade-off depends on the <code>initvalue</code> and the <code>unseenvalue</code>.  To distinguish actions that were never choosen before, i.e. novel actions, the default initial Q-value (field <code>param</code>) is <code>initvalue = Inf64</code>. In a state with novel actions, the <a href="../policies/#policies-1">policy</a> determines how to deal with novel actions. To compute the td-error the <code>unseenvalue</code> is used for states with novel actions.  One way to achieve agressively exploratory behavior is to assure that <code>unseenvalue</code> (or <code>initvalue</code>) is larger than the largest possible Q-value.</p><h2><a class="nav-anchor" id="Policy-Gradient-Learner-1" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Critic" href="#ReinforcementLearning.Critic"><code>ReinforcementLearning.Critic</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct Critic &lt;: AbstractBiasCorrector
    α::Float64
    V::Array{Float64, 1}</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Critic-Tuple{}" href="#ReinforcementLearning.Critic-Tuple{}"><code>ReinforcementLearning.Critic</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">Critic(; γ = .9, α = .1, ns = 10, initvalue = 0.)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.NoBiasCorrector" href="#ReinforcementLearning.NoBiasCorrector"><code>ReinforcementLearning.NoBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct NoBiasCorrector &lt;: AbstractBiasCorrector</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.RewardLowpassFilterBiasCorrector" href="#ReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>ReinforcementLearning.RewardLowpassFilterBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct RewardLowpassFilterBiasCorrector &lt;: AbstractBiasCorrector
λ::Float64
rmean::Float64</code></pre><p>Filters the reward with factor λ and uses effective reward (r - rmean) to update the parameters.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ActorCriticPolicyGradient-Tuple{}" href="#ReinforcementLearning.ActorCriticPolicyGradient-Tuple{}"><code>ReinforcementLearning.ActorCriticPolicyGradient</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">ActorCriticPolicyGradient(; nsteps = 1, γ = .9, ns = 10, na = 4, 
                            α = .1, αcritic = .1, initvalue = Inf64)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.EpisodicReinforce-Tuple{}" href="#ReinforcementLearning.EpisodicReinforce-Tuple{}"><code>ReinforcementLearning.EpisodicReinforce</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">EpisodicReinforce(; kwargs...) = PolicyGradientForward(; kwargs...)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractPolicyGradient" href="#ReinforcementLearning.AbstractPolicyGradient"><code>ReinforcementLearning.AbstractPolicyGradient</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct PolicyGradientBackward &lt;: AbstractPolicyGradient
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    α::Float64 = .1
    initvalue::Float64 = 0.
    params::Array{Float64, 2} = zeros(na, ns) + initvalue
    traces::AccumulatingTraces = AccumulatingTraces(ns, na, 1., γ, 
                                                    trace = zeros(na, ns))
    biascorrector::T = NoBiasCorrector()</code></pre><p>Policy gradient learning in the backward view.</p><p>The parameters are updated according to <span>$params[a, s] += α * r_{eff} * e[a, s]$</span> where <span>$r_{eff} =  r$</span> for <a href="#ReinforcementLearning.NoBiasCorrector"><code>NoBiasCorrector</code></a>, <span>$r_{eff} =  r - rmean$</span> for <a href="#ReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>RewardLowpassFilterBiasCorrector</code></a> and e[a, s] is the eligibility trace.</p></div></div></section><h2><a class="nav-anchor" id="N-step-Learner-1" href="#N-step-Learner-1">N-step Learner</a></h2><h2><a class="nav-anchor" id="Model-Based-Learner-1" href="#Model-Based-Learner-1">Model Based Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.policy_iteration!-Tuple{MDPLearner}" href="#ReinforcementLearning.policy_iteration!-Tuple{MDPLearner}"><code>ReinforcementLearning.policy_iteration!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">policy_iteration!(mdplearner::MDPLearner)</code></pre><p>Solve MDP with policy iteration using <a href="@ref"><code>MDPLearner</code></a>.</p></div></div></section><h2><a class="nav-anchor" id="Deep-Reinforcement-Learning-1" href="#Deep-Reinforcement-Learning-1">Deep Reinforcement Learning</a></h2><footer><hr/><a class="previous" href="../learning/"><span class="direction">Previous</span><span class="title">Learning</span></a><a class="next" href="../buffers/"><span class="direction">Next</span><span class="title">Buffers</span></a></footer></article></body></html>
