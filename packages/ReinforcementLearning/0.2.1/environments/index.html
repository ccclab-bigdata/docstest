<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Environments · ReinforcementLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li class="current"><a class="toctext" href>Environments</a><ul class="internal"><li><a class="toctext" href="#MDPs-1">MDPs</a></li></ul></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../preprocessors/">Preprocessors</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Environments</a></li></ul></nav><hr/><div id="topbar"><span>Environments</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="environments-1" href="#environments-1">Environments</a></h1><p>The following environments can be added with </p><pre><code class="language-julia">(v1.0) pkg&gt; add ReinforcementLearningEnvironmentXZY</code></pre><p>or</p><pre><code class="language-julia">Pkg.add(&quot;ReinforcementLearningEnvironmentXYZ&quot;)</code></pre><p>Examples can be found in the example folders of these repositories.</p><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentAtari.jl">ReinforcementLearningEnvironmentAtari</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentClassicControl.jl">ReinforcementLearningEnvironmentClassicControl</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentDiscrete.jl">ReinforcementLearningEnvironmentDiscrete</a>. <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentViZDoom.jl">ReinforcementLearningEnvironmentViZDoom</a>. <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironmentGym.jl">ReinforcementLearningEnvironmentViZGym</a>.</p><h2><a class="nav-anchor" id="MDPs-1" href="#MDPs-1">MDPs</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.MDP" href="#ReinforcementLearning.MDP"><code>ReinforcementLearning.MDP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct MDP 
    ns::Int64
    na::Int64
    state::Int64
    trans_probs::Array{AbstractArray, 2}
    reward::Array{Float64, 2}
    initialstates::Array{Int64, 1}
    isterminal::Array{Int64, 1}</code></pre><p>A Markov Decision Process with <code>ns</code> states, <code>na</code> actions, current <code>state</code>, <code>na</code>x<code>ns</code> - array of transition probabilites <code>trans_props</code> which consists for every (action, state) pair of a (potentially sparse) array that sums to 1 (see <a href="#ReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>getprobvecrandom</code></a>, <a href="#ReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>getprobvecuniform</code></a>, <a href="#ReinforcementLearning.getprobvecdeterministic"><code>getprobvecdeterministic</code></a> for helpers to constract the transition probabilities) <code>na</code>x<code>ns</code> - array of <code>reward</code>, array of initial states <code>initialstates</code>, and <code>ns</code> - array of 0/1 indicating if a state is terminal.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.MDP-Tuple{Any,Any}" href="#ReinforcementLearning.MDP-Tuple{Any,Any}"><code>ReinforcementLearning.MDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">MDP(ns, na; init = &quot;random&quot;)
MDP(; ns = 10, na = 4, init = &quot;random&quot;)</code></pre><p>Return MDP with <code>init in (&quot;random&quot;, &quot;uniform&quot;, &quot;deterministic&quot;)</code>, where the keyword init determines how to construct the transition probabilites (see also  <a href="#ReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>getprobvecrandom</code></a>, <a href="#ReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>getprobvecuniform</code></a>, <a href="#ReinforcementLearning.getprobvecdeterministic"><code>getprobvecdeterministic</code></a>).</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbsorbingDetMDP-Tuple{}" href="#ReinforcementLearning.AbsorbingDetMDP-Tuple{}"><code>ReinforcementLearning.AbsorbingDetMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">AbsorbingDetMDP(;ns = 10^3, na = 10)</code></pre><p>Returns a random deterministic absorbing MDP</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DetMDP-Tuple{}" href="#ReinforcementLearning.DetMDP-Tuple{}"><code>ReinforcementLearning.DetMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">DetMDP(; ns = 10^4, na = 10)</code></pre><p>Returns a random deterministic MDP.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DetTreeMDP-Tuple{}" href="#ReinforcementLearning.DetTreeMDP-Tuple{}"><code>ReinforcementLearning.DetTreeMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">DetTreeMDP(; na = 4, depth = 5)</code></pre><p>Returns a treeMDP with random rewards at the leaf nodes.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DetTreeMDPwithinrew-Tuple{}" href="#ReinforcementLearning.DetTreeMDPwithinrew-Tuple{}"><code>ReinforcementLearning.DetTreeMDPwithinrew</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">DetTreeMDPwithinrew(; na = 4, depth = 5)</code></pre><p>Returns a treeMDP with random rewards.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.StochMDP-Tuple{}" href="#ReinforcementLearning.StochMDP-Tuple{}"><code>ReinforcementLearning.StochMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">StochMDP(; na = 10, ns = 50) = MDP(ns, na)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.StochTreeMDP-Tuple{}" href="#ReinforcementLearning.StochTreeMDP-Tuple{}"><code>ReinforcementLearning.StochTreeMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">StochTreeMDP(; na = 4, depth = 4, bf = 2)</code></pre><p>Returns a random stochastic treeMDP with branching factor <code>bf</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.run!-Tuple{MDP,Array{Int64,1}}" href="#ReinforcementLearning.run!-Tuple{MDP,Array{Int64,1}}"><code>ReinforcementLearning.run!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">run!(mdp::MDP, policy::Array{Int64, 1}) = run!(mdp, policy[mdp.state])</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.run!-Tuple{MDP,Int64}" href="#ReinforcementLearning.run!-Tuple{MDP,Int64}"><code>ReinforcementLearning.run!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">run!(mdp::MDP, action::Int64)</code></pre><p>Transition to a new state given <code>action</code>. Returns the new state.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.setterminalstates!-Tuple{Any,Any}" href="#ReinforcementLearning.setterminalstates!-Tuple{Any,Any}"><code>ReinforcementLearning.setterminalstates!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">setterminalstates!(mdp, range)</code></pre><p>Sets <code>mdp.isterminal[range] .= 1</code>, empties the table of transition probabilities for terminal states and sets the reward for all actions in the terminal state to the same value.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.treeMDP-Tuple{Any,Any}" href="#ReinforcementLearning.treeMDP-Tuple{Any,Any}"><code>ReinforcementLearning.treeMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">treeMDP(na, depth; init = &quot;random&quot;, branchingfactor = 3)</code></pre><p>Returns a tree structured MDP with na actions and <code>depth</code> of the tree. If <code>init</code> is random, the <code>branchingfactor</code> determines how many possible states a (action, state) pair has. If <code>init = &quot;deterministic&quot;</code> the <code>branchingfactor = na</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.getprobvecdeterministic" href="#ReinforcementLearning.getprobvecdeterministic"><code>ReinforcementLearning.getprobvecdeterministic</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">getprobvecdeterministic(n, min = 1, max = n)</code></pre><p>Returns a <code>SparseVector</code> of length <code>n</code> where one element in <code>min</code>:<code>max</code> has  value 1.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}" href="#ReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>ReinforcementLearning.getprobvecrandom</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">getprobvecrandom(n, min, max)</code></pre><p>Returns an array of length <code>n</code> that sums to 1 where all elements outside of <code>min</code>:<code>max</code> are zero.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.getprobvecrandom-Tuple{Any}" href="#ReinforcementLearning.getprobvecrandom-Tuple{Any}"><code>ReinforcementLearning.getprobvecrandom</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">getprobvecrandom(n)</code></pre><p>Returns an array of length <code>n</code> that sums to 1. More precisely, the array is a sample of a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> with <code>n</code> categories and <span>$α_1 = ⋯  = α_n = 1$</span>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.getprobvecuniform-Tuple{Any}" href="#ReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>ReinforcementLearning.getprobvecuniform</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">getprobvecuniform(n)  = fill(1/n, n)</code></pre></div></div></section><footer><hr/><a class="previous" href="../buffers/"><span class="direction">Previous</span><span class="title">Buffers</span></a><a class="next" href="../stop/"><span class="direction">Next</span><span class="title">Stopping Criteria</span></a></footer></article></body></html>
