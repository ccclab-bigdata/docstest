<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· NLopt.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NLopt.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Using-with-MathProgBase-1">Using with MathProgBase</a></li><li><a class="toctext" href="#Tutorial-1">Tutorial</a></li><li><a class="toctext" href="#Reference-1">Reference</a></li><li><a class="toctext" href="#Author-1">Author</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="The-NLopt-module-for-Julia-1" href="#The-NLopt-module-for-Julia-1">The NLopt module for Julia</a></h1><p><a href="https://travis-ci.org/JuliaOpt/NLopt.jl"><img src="https://travis-ci.org/JuliaOpt/NLopt.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://ci.appveyor.com/project/StevenGJohnson/nlopt-jl"><img src="https://ci.appveyor.com/api/projects/status/eqw9yb2cyn8sxvo9?svg=true" alt="Build status"/></a></p><p><a href="http://pkg.julialang.org/?pkg=NLopt&amp;ver=0.3"><img src="http://pkg.julialang.org/badges/NLopt_0.3.svg" alt="NLopt"/></a> <a href="http://pkg.julialang.org/?pkg=NLopt&amp;ver=0.4"><img src="http://pkg.julialang.org/badges/NLopt_0.4.svg" alt="NLopt"/></a> <a href="http://pkg.julialang.org/?pkg=NLopt&amp;ver=0.5"><img src="http://pkg.julialang.org/badges/NLopt_0.5.svg" alt="NLopt"/></a></p><p>This module provides a <a href="http://julialang.org/">Julia-language</a> interface to the free/open-source <a href="http://ab-initio.mit.edu/nlopt">NLopt library</a> for nonlinear optimization. NLopt provides a common interface for many different optimization algorithms, including:</p><ul><li>Both global and local optimization</li><li>Algorithms using function values only (derivative-free) and also algorithms exploiting user-supplied gradients.</li><li>Algorithms for unconstrained optimization, bound-constrained optimization, and general nonlinear inequality/equality constraints.</li></ul><p>See the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Introduction">NLopt introduction</a> for a further overview of the types of problems it addresses.</p><p>NLopt can be used either by accessing it&#39;s specialized API or by using the generic <a href="https://github.com/JuliaOpt/MathProgBase.jl">MathProgBase</a> interface for nonlinear optimization. Both methods are documented below.</p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>Within Julia, you can install the NLopt.jl package with the package manager: <code>Pkg.add(&quot;NLopt&quot;)</code></p><p>On Windows and OS X platforms, NLopt binaries will be automatically installed. On other platforms, Julia will attempt to build NLopt from source; be sure to have a compiler installed.</p><h2><a class="nav-anchor" id="Using-with-MathProgBase-1" href="#Using-with-MathProgBase-1">Using with MathProgBase</a></h2><p>NLopt implements the <a href="http://mathprogbasejl.readthedocs.org/en/latest/nlp.html">MathProgBase interface</a> for nonlinear optimization, which means that it can be used interchangeably with other optimization packages from modeling packages like <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a> or when providing hand-written derivatives. Note that NLopt does not exploit sparsity of Jacobians.</p><p>The NLopt solver is named <span>$NLoptSolver$</span> and takes parameters:</p><ul><li><span>$algorithm$</span></li><li><span>$stopval$</span></li><li><span>$ftol_rel$</span></li><li><span>$ftol_abs$</span></li><li><span>$xtol_rel$</span></li><li><span>$xtol_abs$</span></li><li><span>$constrtol_abs$</span></li><li><span>$maxeval$</span></li><li><span>$maxtime$</span></li><li><span>$initial_step$</span></li><li><span>$population$</span></li><li><span>$seed$</span></li><li><span>$vector_storage$</span></li></ul><p>The <span>$algorithm$</span> parameter is required, and all others are optional. The meaning and acceptable values of all parameters, except <span>$constrtol_abs$</span>, match the descriptions below from the specialized NLopt API. The <span>$constrtol_abs$</span> parameter is an absolute feasibility tolerance applied to all constraints.</p><h2><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h2><p>The following example code solves the nonlinearly constrained minimization problem from the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Tutorial">NLopt Tutorial</a>:</p><pre><code class="language-julia">using NLopt

function myfunc(x::Vector, grad::Vector)
    if length(grad) &gt; 0
        grad[1] = 0
        grad[2] = 0.5/sqrt(x[2])
    end
    return sqrt(x[2])
end

function myconstraint(x::Vector, grad::Vector, a, b)
    if length(grad) &gt; 0
        grad[1] = 3a * (a*x[1] + b)^2
        grad[2] = -1
    end
    (a*x[1] + b)^3 - x[2]
end

opt = Opt(:LD_MMA, 2)
lower_bounds!(opt, [-Inf, 0.])
xtol_rel!(opt,1e-4)

min_objective!(opt, myfunc)
inequality_constraint!(opt, (x,g) -&gt; myconstraint(x,g,2,0), 1e-8)
inequality_constraint!(opt, (x,g) -&gt; myconstraint(x,g,-1,1), 1e-8)

(minf,minx,ret) = optimize(opt, [1.234, 5.678])
count = opt.numevals # the number of function evaluations
println(&quot;got $minf at $minx after $count iterations (returned $ret)&quot;)</code></pre><p>The output should be:</p><pre><code class="language-none">got 0.5443310476200902 at [0.3333333346933468,0.29629628940318486] after 11 iterations (returned XTOL_REACHED)</code></pre><p>Much like the NLopt interfaces in other languages, you create an <code>Opt</code> object (analogous to <code>nlopt_opt</code> in C) which encapsulates the dimensionality of your problem (here, 2) and the algorithm to be used (here, <code>LD_MMA</code>) and use various functions to specify the constraints and stopping criteria (along with any other aspects of the problem).</p><p>The same problem can be solved by using the JuMP interface to NLopt:</p><pre><code class="language-julia">using JuMP
using NLopt

m = Model(solver=NLoptSolver(algorithm=:LD_MMA))

a1 = 2
b1 = 0
a2 = -1
b2 = 1

@variable(m, x1)
@variable(m, x2 &gt;= 0)

@NLobjective(m, Min, sqrt(x2))
@NLconstraint(m, x2 &gt;= (a1*x1+b1)^3)
@NLconstraint(m, x2 &gt;= (a2*x1+b2)^3)

setvalue(x1, 1.234)
setvalue(x2, 5.678)

status = solve(m)

println(&quot;got &quot;, getobjectivevalue(m), &quot; at &quot;, [getvalue(x1),getvalue(x2)])</code></pre><p>The output should be:</p><pre><code class="language-none">got 0.5443310477213124 at [0.3333333342139688,0.29629628951338166]</code></pre><p>Note that the MathProgBase interface sets slightly different convergence tolerances by default, so the outputs from the two problems are not identical.</p><h2><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h2><p>The main purpose of this section is to document the syntax and unique features of the Julia interface; for more detail on the underlying features, please refer to the C documentation in the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference">NLopt Reference</a>.</p><h3><a class="nav-anchor" id="Using-the-Julia-API-1" href="#Using-the-Julia-API-1">Using the Julia API</a></h3><p>To use NLopt in Julia, your Julia program should include the line:</p><pre><code class="language-julia">using NLopt</code></pre><p>which imports the NLopt module and its symbols.  (Alternatively, you can use <code>import NLopt</code> if you want to keep all the NLopt symbols in their own namespace.  You would then prefix all functions below with <code>NLopt.</code>, e.g. <code>NLopt.Opt</code> and so on.)</p><h3><a class="nav-anchor" id="The-Opt-type-1" href="#The-Opt-type-1">The <code>Opt</code> type</a></h3><p>The NLopt API revolves around an object of type <code>Opt</code>. Via functions acting on this object, all of the parameters of the optimization are specified (dimensions, algorithm, stopping criteria, constraints, objective function, etcetera), and then one finally calls the <code>optimize</code> function in order to perform the optimization. The object should normally be created via the constructor:</p><pre><code class="language-julia">opt = Opt(algorithm, n)</code></pre><p>given an algorithm (see <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms">NLopt Algorithms</a> for possible values) and the dimensionality of the problem (<code>n</code>, the number of optimization parameters). Whereas in C the algorithms are specified by <code>nlopt_algorithm</code> constants of the form <code>NLOPT_LD_MMA</code>, <code>NLOPT_LN_COBYLA</code>, etcetera, the Julia <code>algorithm</code> values are symbols of the form <code>:LD_MMA</code>, <code>:LN_COBYLA</code>, etcetera (with the <code>NLOPT_</code> prefix replaced by <code>:</code> to create a Julia symbol).</p><p>There is also a <code>copy(opt::Opt)</code> function to make a copy of a given object (equivalent to <code>nlopt_copy</code> in the C API).</p><p>If there is an error in these functions, an exception is thrown.</p><p>The algorithm and dimension parameters of the object are immutable (cannot be changed without constructing a new object), but you can query them for a given object by the functions:</p><pre><code class="language-julia">ndims(opt::Opt)
algorithm(opt::Opt)</code></pre><p>You can get a string description of the algorithm via:</p><pre><code class="language-julia">algorithm_name(opt::Opt)</code></pre><h3><a class="nav-anchor" id="Objective-function-1" href="#Objective-function-1">Objective function</a></h3><p>The objective function is specified by calling one of the functions:</p><pre><code class="language-julia">min_objective!(opt::Opt, f::Function)
max_objective!(opt::Opt, f::Function)</code></pre><p>depending on whether one wishes to minimize or maximize the objective function <code>f</code>, respectively. The function <code>f</code> should be of the form:</p><pre><code class="language-julia">function f(x::Vector, grad::Vector):
    if length(grad) &gt; 0:
        ...set grad to gradient, in-place...
    return ...value of f(x)...
end</code></pre><p>The return value should be the value of the function at the point <code>x</code>, where <code>x</code> is a (<code>Float64</code>) array of length <code>n</code> of the optimization parameters (the same as the dimension passed to the <code>Opt</code> constructor).</p><p>In addition, if the argument <code>grad</code> is not empty [i.e. <code>length(grad)</code>&gt;0], then <code>grad</code> is a (<code>Float64</code>) array of length <code>n</code> which should (upon return) be set to the gradient of the function with respect to the optimization parameters at <code>x</code>. That is, <code>grad[i]</code> should upon return contain the partial derivative &amp;part;<code>f</code>/&amp;part;<code>x</code>&lt;sub&gt;<code>i</code>&lt;/sub&gt;, for 1&amp;le;<code>i</code>&amp;le;<code>n</code>, if <code>grad</code> is non-empty. Not all of the optimization algorithms (below) use the gradient information: for algorithms listed as &quot;derivative-free,&quot; the <code>grad</code> argument will always be empty and need never be computed. (For algorithms that do use gradient information, however, <code>grad</code> may still be empty for some calls.)</p><p>Note that <code>grad</code> must be modified <em>in-place</em> by your function <code>f</code>. Generally, this means using indexing operations <code>grad[...] = ...</code> to overwrite the contents of <code>grad</code>.  For example <code>grad = 2x</code> will <em>not</em> work, because it points <code>grad</code> to a new array <code>2x</code> rather than overwriting the old contents; instead, use an explicit loop or use <code>grad[:] = 2x</code>.</p><h3><a class="nav-anchor" id="Bound-constraints-1" href="#Bound-constraints-1">Bound constraints</a></h3><p>The bound constraints can be specified by calling the functions:</p><pre><code class="language-julia">lower_bounds!(opt::Opt, lb::AbstractVector)
upper_bounds!(opt::Opt, ub::AbstractVector)</code></pre><p>where <code>lb</code> and <code>ub</code> are real arrays of length <code>n</code> (the same as the dimension passed to the <code>Opt</code> constructor). For convenience, these are overloaded with functions that take a single number as arguments, in order to set the lower/upper bounds for all optimization parameters to a single constant.</p><p>To retrieve the values of the lower/upper bounds, you can call one of:</p><pre><code class="language-julia">lower_bounds(opt::Opt)
upper_bounds(opt::Opt)</code></pre><p>both of which return <code>Vector{Float64}</code> arrays.</p><p>To specify an unbounded dimension, you can use &amp;plusmn;<code>Inf</code>.</p><h3><a class="nav-anchor" id="Nonlinear-constraints-1" href="#Nonlinear-constraints-1">Nonlinear constraints</a></h3><p>Just as for nonlinear constraints in C, you can specify nonlinear inequality and equality constraints by the functions:</p><pre><code class="language-julia">inequality_constraint!(opt::Opt, fc, tol=0)
equality_constraint!(opt::Opt, h, tol=0)</code></pre><p>where the arguments <code>fc</code> and <code>h</code> have the same form as the objective function above. The optional <code>tol</code> arguments specify a tolerance (which defaults to zero) in judging feasibility for the purposes of stopping the optimization, as in C.</p><p>To remove all of the inequality and equality constraints from a given problem, you can call the following functions:</p><pre><code class="language-julia">remove_constraints!(opt::Opt)</code></pre><h3><a class="nav-anchor" id="Vector-valued-constraints-1" href="#Vector-valued-constraints-1">Vector-valued constraints</a></h3><p>Just as for nonlinear constraints in C, you can specify vector-valued nonlinear inequality and equality constraints by the functions</p><pre><code class="language-julia">inequality_constraint!(opt::Opt, c, tol::AbstractVector)
equality_constraint!(opt::Opt, c, tol::AbstractVector)</code></pre><p>Here, <code>tol</code> is an array of the tolerances in each constraint dimension; the dimensionality <code>m</code> of the constraint is determined by <code>length(tol)</code>. The constraint function <code>c</code> must be of the form:</p><pre><code class="language-julia">function c(result::Vector, x::Vector, grad::Matrix):
    if length(grad) &gt; 0:
        ...set grad to gradient, in-place...
    result[1] = ...value of c1(x)...
    result[2] = ...value of c2(x)...
    ...</code></pre><p><code>result</code> is a (<code>Float64</code>) array whose length equals the dimensionality <code>m</code> of the constraint (same as the length of <code>tol</code> above), which upon return should be set <em>in-place</em> (see also above) to the constraint results at the point <code>x</code> (a <code>Float64</code> array whose length <code>n</code> is the same as the dimension passed to the <code>Opt</code> constructor). Any return value of the function is ignored.</p><p>In addition, if the argument <code>grad</code> is not empty [i.e. <code>length(grad)</code>&gt;0], then <code>grad</code> is a 2d array of size <code>n</code>&amp;times;<code>m</code> which should (upon return) be set in-place (see above) to the gradient of the function with respect to the optimization parameters at <code>x</code>. That is, <code>grad[j,i]</code> should upon return contain the partial derivative &amp;part;c&lt;sub&gt;<code>i</code>&lt;/sub&gt;/&amp;part;x&lt;sub&gt;<code>j</code>&lt;/sub&gt; if <code>grad</code> is non-empty. Not all of the optimization algorithms (below) use the gradient information: for algorithms listed as &quot;derivative-free,&quot; the <code>grad</code> argument will always be empty and need never be computed. (For algorithms that do use gradient information, however, <code>grad</code> may still be empty for some calls.)</p><p>An inequality constraint corresponds to c&lt;sub&gt;<code>i</code>&lt;/sub&gt;&amp;le;0 for 1&amp;le;<code>i</code>&amp;le;<code>m</code>, and an equality constraint corresponds to c&lt;sub&gt;i&lt;/sub&gt;=0, in both cases with tolerance <code>tol[i]</code> for purposes of termination criteria.</p><p>(You can add multiple vector-valued constraints and/or scalar constraints in the same problem.)</p><h3><a class="nav-anchor" id="Stopping-criteria-1" href="#Stopping-criteria-1">Stopping criteria</a></h3><p>As explained in the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference">C API Reference</a> and the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Introduction">Introduction</a>), you have multiple options for different stopping criteria that you can specify. (Unspecified stopping criteria are disabled; i.e., they have innocuous defaults.)</p><p>For each stopping criteria, there are (at least) two functions: a &quot;set&quot; function (ending with <code>!</code>) to specify the stopping criterion, and a &quot;get&quot; function to retrieve the current value for that criterion. The meanings of each criterion are exactly the same as in the C API.</p><pre><code class="language-julia">stopval!(o::Opt, stopval::Real)
stopval(o::Opt)</code></pre><p>Stop when an objective value of at least <code>stopval</code> is found.</p><pre><code class="language-julia">ftol_rel!(o::Opt, tol::Real)
ftol_rel(o::Opt)</code></pre><p>Set relative tolerance on function value.</p><pre><code class="language-julia">ftol_abs!(o::Opt, tol::Real)
ftol_abs(o::Opt)</code></pre><p>Set absolute tolerance on function value.</p><pre><code class="language-julia">xtol_rel!(o::Opt, tol::Real)
xtol_rel(o::Opt)</code></pre><p>Set relative tolerance on optimization parameters.</p><pre><code class="language-julia">xtol_abs!(o::Opt, tol)
xtol_abs(o::Opt)</code></pre><p>Set absolute tolerances on optimization parameters. The <code>tol</code> input must be an array of length <code>n</code> (the dimension specified in the <code>Opt</code> constructor); alternatively, you can pass a single number in order to set the same tolerance for all optimization parameters. <code>xtol_abs(o)</code> returns the tolerances as a array.</p><pre><code class="language-julia">maxeval!(o::Opt, mev::Integer)
maxeval(o::Opt)</code></pre><p>Stop when the number of function evaluations exceeds <code>mev</code>. (0 or negative for no limit.)</p><pre><code class="language-julia">maxtime!(o::Opt, t::Real)
maxtime(o::Opt)</code></pre><p>Stop when the optimization time (in seconds) exceeds <code>t</code>. (0 or negative for no limit.)</p><h3><a class="nav-anchor" id="Forced-termination-1" href="#Forced-termination-1">Forced termination</a></h3><p>In certain cases, the caller may wish to force the optimization to halt, for some reason unknown to NLopt. For example, if the user presses Ctrl-C, or there is an error of some sort in the objective function. You can do this by throwing any exception inside your objective/constraint functions: the optimization will be halted gracefully, and the same exception will be thrown to the caller. See below regarding exceptions. The Julia equivalent of <code>nlopt_forced_stop</code> from the C API is to throw a <code>ForcedStop</code> exception.</p><h3><a class="nav-anchor" id="Performing-the-optimization-1" href="#Performing-the-optimization-1">Performing the optimization</a></h3><p>Once all of the desired optimization parameters have been specified in a given object <code>opt::Opt</code>, you can perform the optimization by calling:</p><pre><code class="language-julia">(optf,optx,ret) = optimize(opt::Opt, x::AbstractVector)</code></pre><p>On input, <code>x</code> is an array of length <code>n</code> (the dimension of the problem from the <code>Opt</code> constructor) giving an initial guess for the optimization parameters. The return value <code>optx</code> is a array containing the optimized values of the optimization parameters. <code>optf</code> contains the optimized value of the objective function, and <code>ret</code> contains a symbol indicating the NLopt return code (below).</p><p>Alternatively,</p><pre><code class="language-julia">(optf,optx,ret) = optimize!(opt::Opt, x::Vector{Float64})</code></pre><p>is the same but modifies <code>x</code> in-place (as well as returning <code>optx=x</code>).</p><p>On failure (negative return codes), optimize() throws an exception (see Exceptions, below).</p><h3><a class="nav-anchor" id="Return-values-1" href="#Return-values-1">Return values</a></h3><p>The possible return values are the same as the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference#Return_values">return values in the C API</a>, except that the <code>NLOPT_</code> prefix is replaced with <code>:</code>.  That is, the return values are <code>:SUCCESS</code>, <code>:XTOL_REACHED</code>, etcetera (instead of <code>NLOPT_SUCCESS</code> etcetera).</p><h3><a class="nav-anchor" id="Exceptions-1" href="#Exceptions-1">Exceptions</a></h3><p>The error codes in the C API are replaced in the Julia API by thrown exceptions. The following exceptions are thrown by the various routines:</p><p>If your objective/constraint functions throw any exception during the execution of <code>optimize</code>, it will be caught by NLopt and the optimization will be halted gracefully, and opt.optimize will re-throw the same exception to its caller.</p><h3><a class="nav-anchor" id="Local/subsidiary-optimization-algorithm-1" href="#Local/subsidiary-optimization-algorithm-1">Local/subsidiary optimization algorithm</a></h3><p>Some of the algorithms, especially MLSL and AUGLAG, use a different optimization algorithm as a subroutine, typically for local optimization. You can change the local search algorithm and its tolerances by calling:</p><pre><code class="language-julia">local_optimizer!(opt::Opt, local_opt::Opt)</code></pre><p>Here, <code>local_opt</code> is another <code>Opt</code> object whose parameters are used to determine the local search algorithm, its stopping criteria, and other algorithm parameters. (However, the objective function, bounds, and nonlinear-constraint parameters of <code>local_opt</code> are ignored.) The dimension <code>n</code> of <code>local_opt</code> must match that of <code>opt</code>.</p><p>This function makes a copy of the <code>local_opt</code> object, so you can freely change your original <code>local_opt</code> afterwards without affecting <code>opt</code>.</p><h3><a class="nav-anchor" id="Initial-step-size-1" href="#Initial-step-size-1">Initial step size</a></h3><p>Just <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference#Initial_step_size">as in the C API</a>, you can get and set the initial step sizes for derivative-free optimization algorithms. The Julia equivalents of the C functions are the following functions:</p><pre><code class="language-julia">initial_step!(opt::Opt, dx::AbstractVector)</code></pre><p>Here, dx is an array of the (nonzero) initial steps for each dimension, or a single number if you wish to use the same initial steps for all dimensions. <code>get_initial_step(opt::Opt, x::AbstractVector)</code> returns the initial step that will be used for a starting guess of <code>x</code> in <code>optimize(opt,x)</code>.</p><h3><a class="nav-anchor" id="Stochastic-population-1" href="#Stochastic-population-1">Stochastic population</a></h3><p>Just <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference#Stochastic_population">as in the C API</a>, you can get and set the initial population for stochastic optimization algorithms, by the functions:</p><pre><code class="language-julia">population!(opt::Opt, pop::Integer)
population(opt::Opt)</code></pre><p>(A <code>pop</code> of zero implies that the heuristic default will be used.)</p><h3><a class="nav-anchor" id="Pseudorandom-numbers-1" href="#Pseudorandom-numbers-1">Pseudorandom numbers</a></h3><p>For stochastic optimization algorithms, NLopt uses pseudorandom numbers generated by the Mersenne Twister algorithm, based on code from Makoto Matsumoto. By default, the seed for the random numbers is generated from the system time, so that you will get a different sequence of pseudorandom numbers each time you run your program. If you want to use a &quot;deterministic&quot; sequence of pseudorandom numbers, i.e. the same sequence from run to run, you can set the seed by calling:</p><pre><code class="language-julia">NLopt.srand(seed::Integer)</code></pre><p>To reset the seed based on the system time, you can call <code>NLopt.srand_time()</code>.</p><p>(Normally, you don&#39;t need to call this as it is called automatically. However, it might be useful if you want to &quot;re-randomize&quot; the pseudorandom numbers after calling <code>nlopt.srand</code> to set a deterministic seed.)</p><h3><a class="nav-anchor" id="Vector-storage-for-limited-memory-quasi-Newton-algorithms-1" href="#Vector-storage-for-limited-memory-quasi-Newton-algorithms-1">Vector storage for limited-memory quasi-Newton algorithms</a></h3><p>Just <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Reference#Vector_storage_for_limited-memory_quasi-Newton_algorithms">as in the C API</a>, you can get and set the number M of stored vectors for limited-memory quasi-Newton algorithms, via the functions:</p><pre><code class="language-julia">vector_storage!(opt::Opt, M::Integer)
vector_storage(opt::Opt)</code></pre><p>(The default is <code>M</code>=0, in which case NLopt uses a heuristic nonzero value.)</p><h3><a class="nav-anchor" id="Version-number-1" href="#Version-number-1">Version number</a></h3><p>The version number of NLopt is given by the global variable:</p><pre><code class="language-julia">NLOPT_VERSION::VersionNumber</code></pre><p>where <code>VersionNumber</code> is a built-in Julia type from the Julia standard library.</p><h2><a class="nav-anchor" id="Author-1" href="#Author-1">Author</a></h2><p>This module was initially written by <a href="http://math.mit.edu/~stevenj/">Steven G. Johnson</a>, with subsequent contributions by several other authors (see the git history).</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
