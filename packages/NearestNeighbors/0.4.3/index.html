<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · NearestNeighbors.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NearestNeighbors.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Creating-a-tree-1">Creating a tree</a></li><li><a class="toctext" href="#k-Nearest-Neighbor-(kNN)-searches-1">k Nearest Neighbor (kNN) searches</a></li><li><a class="toctext" href="#Range-searches-1">Range searches</a></li><li><a class="toctext" href="#Using-on-disk-data-sets-1">Using on-disk data sets</a></li><li><a class="toctext" href="#Author-1">Author</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="NearestNeighbors.jl-1" href="#NearestNeighbors.jl-1">NearestNeighbors.jl</a></h1><p><a href="https://travis-ci.org/KristofferC/NearestNeighbors.jl"><img src="https://travis-ci.org/KristofferC/NearestNeighbors.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://ci.appveyor.com/project/KristofferC/nearestneighbors-jl"><img src="https://ci.appveyor.com/api/projects/status/lj0lk3c0pgwn06xe?svg=true" alt="Build status"/></a>  <a href="https://codecov.io/github/KristofferC/NearestNeighbors.jl?branch=master"><img src="https://codecov.io/github/KristofferC/NearestNeighbors.jl/coverage.svg?branch=master" alt="codecov.io"/></a></p><p><code>NearestNeighbors.jl</code> is a package written in Julia to perform high performance nearest neighbor searches in  arbitrarily high dimensions.</p><hr/><h2><a class="nav-anchor" id="Creating-a-tree-1" href="#Creating-a-tree-1">Creating a tree</a></h2><p>The abstract tree type that the trees in this package are a subtype of is called a <code>NNTree</code>. A <code>NNTree</code> is created by the function:</p><pre><code class="language-jl">NNTree(data, metric; leafsize, reorder)</code></pre><ul><li><code>data</code>: This parameter represents the points to build up the tree from. It can either be a matrix of size <code>nd × np</code> with the points to insert in the tree where <code>nd</code> is the dimensionality of the points, <code>np</code> is the number of points or it can be a <code>Vector{V}</code> where <code>V</code> is itself a subtype of an <code>AbstractVector</code> and such that <code>eltype(V)</code> and <code>length(V)</code> is defined.</li><li><code>metric</code>: The metric to use, defaults to <code>Euclidean</code>. This is one of the <code>Metric</code> types defined in the <code>Distances.jl</code> packages. It is possible to define your own metrics by simply creating new types that are subtypes of <code>Metric</code>.</li><li><code>leafsize</code> (keyword argument): Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.</li><li><code>reorder</code> (keyword argument): While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to <code>true</code>.</li></ul><p>There are currently three types of trees available:</p><ul><li><code>BruteTree</code>: Not actually a tree. It linearly searches all points in a brute force fashion. Works with any <code>Metric</code>.</li><li><code>KDTree</code>: In a kd tree the points are recursively split into groups using hyper-planes.</li></ul><p>Therefore a <code>KDTree</code> only works with axis aligned metrics which are: <code>Euclidean</code>, <code>Chebyshev</code>, <code>Minkowski</code> and <code>Cityblock</code>.</p><ul><li><code>BallTree</code>: Points are recursively split into groups bounded by hyper-spheres. Works with any <code>Metric</code>.</li></ul><p>All trees in <code>NearestNeighbors.jl</code> are static which means that points can not be added or removed from an already created tree.</p><p>Here are a few examples of creating trees:</p><pre><code class="language-jl">using NearestNeighbors
data = rand(3, 10^4)

# Create trees
kdtree = KDTree(data; leafsize = 10)
balltree = BallTree(data, Minkowski(3.5); reorder = false)
brutetree = BruteTree(data)</code></pre><h2><a class="nav-anchor" id="k-Nearest-Neighbor-(kNN)-searches-1" href="#k-Nearest-Neighbor-(kNN)-searches-1">k Nearest Neighbor (kNN) searches</a></h2><p>A kNN search finds the <code>k</code> nearest neighbors to given point(s). This is done with the method:</p><pre><code class="language-jl">knn(tree, points, k, sortres = false, skip = always_false) -&gt; idxs, dists</code></pre><ul><li><code>tree</code>: The tree instance</li><li><code>points</code>: A vector or matrix of points to find the <code>k</code> nearest neighbors to. If <code>points</code> is a vector of numbers then this represents a single point, if <code>points</code> is a matrix then the <code>k</code> nearest neighbors to each point (column) will be computed. <code>points</code> can also be a vector of other vectors where each element in the outer vector is considered a point.</li><li><code>sortres</code> (optional): Determines if the results should be sorted before returning.</li></ul><p>In this case the results will be sorted in order of increasing distance to the point.</p><ul><li><code>skip</code> (optional): A predicate to determine if a given point should be skipped, for</li></ul><p>example if iterating over points and a point has already been visited.</p><p>It is generally better for performance to query once with a large number of points than to query multiple times with one point per query.</p><p>Some examples:</p><pre><code class="language-jl">using NearestNeighbors
data = rand(3, 10^4)
k = 3
point = rand(3)

kdtree = KDTree(data)
idxs, dists = knn(kdtree, point, k, true)

idxs
# 3-element Array{Int64,1}:
#  4683
#  6119
#  3278

dists
# 3-element Array{Float64,1}:
#  0.039032201026256215
#  0.04134193711411951 
#  0.042974090446474184

# Multiple points
points = rand(3, 4);

idxs, dists = knn(kdtree, points, k, true);

idxs
# 4-element Array{Array{Int64,1},1}:
#  [3330, 4072, 2696]
#  [1825, 7799, 8358]
#  [3497, 2169, 3737]
#  [1845, 9796, 2908]

# dists
# 4-element Array{Array{Float64,1},1}:
#  [0.0298932, 0.0327349, 0.0365979]
#  [0.0348751, 0.0498355, 0.0506802]
#  [0.0318547, 0.037291, 0.0421208] 
#  [0.03321, 0.0360935, 0.0411951]
 
# Static vectors
v = @SVector[0.5, 0.3, 0.2];

idxs, dists = knn(kdtree, v, k, true);

idxs
# 3-element Array{Int64,1}:
#   842
#  3075
#  3046

dists
# 3-element Array{Float64,1}:
#  0.04178677766255837 
#  0.04556078331418939 
#  0.049967238112417205</code></pre><h2><a class="nav-anchor" id="Range-searches-1" href="#Range-searches-1">Range searches</a></h2><p>A range search finds all neighbors within the range <code>r</code> of given point(s). This is done with the method:</p><pre><code class="language-jl">inrange(tree, points, r, sortres = false) -&gt; idxs</code></pre><p>Note that for performance reasons the distances are not returned. The arguments to <code>inrange</code> are the same as for <code>knn</code> except that <code>sortres</code> just sorts the returned index vector.</p><p>An example:</p><pre><code class="language-jl">using NearestNeighbors
data = rand(3, 10^4)
r = 0.05
point = rand(3)

balltree = BallTree(data)
idxs = inrange(balltree, point, r, true)

# 4-element Array{Int64,1}:
#  317
#  983
# 4577
# 8675</code></pre><h2><a class="nav-anchor" id="Using-on-disk-data-sets-1" href="#Using-on-disk-data-sets-1">Using on-disk data sets</a></h2><p>By default, the trees store a copy of the <code>data</code> provided during construction. This is problematic in case you want to work on data sets which are larger than available memory, thus wanting to <code>mmap</code> the data or want to store the data in a different place, outside the tree.</p><p><code>DataFreeTree</code> can be used to strip a constructed tree of its data field and re-link it with that data at a later stage. An example of using a large on-disk data set looks like this:</p><pre><code class="language-jl">using Mmap
ndim = 2; ndata = 10_000_000_000
data = Mmap.mmap(datafilename, Matrix{Float32}, (ndim, ndata))
data[:] = rand(Float32, ndim, ndata)  # create example data
dftree = DataFreeTree(KDTree, data)</code></pre><p><code>dftree</code> now only stores the indexing data structures. It can be passed around, saved and reloaded independently of <code>data</code>.</p><p>To perform look-ups, <code>dftree</code> is re-linked to the underlying data:</p><pre><code class="language-jl">tree = injectdata(dftree, data)  # yields a KDTree
knn(tree, data[:,1], 3)  # perform operations as usual</code></pre><h2><a class="nav-anchor" id="Author-1" href="#Author-1">Author</a></h2><p>Kristoffer Carlsson -  @KristofferC - kristoffer.carlsson@chalmers.se</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
