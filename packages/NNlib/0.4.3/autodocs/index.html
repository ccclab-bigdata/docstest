<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · NNlib.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NNlib.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">NNlib.@fix</code></pre><pre><code class="language-none">NNlib.Dims</code></pre><pre><code class="language-none">NNlib.MLN2</code></pre><pre><code class="language-none">NNlib.NNlib</code></pre><pre><code class="language-none">NNlib.__init__</code></pre><pre><code class="language-none">NNlib.__inits__</code></pre><pre><code class="language-none">NNlib.cdims</code></pre><pre><code class="language-none">NNlib.col2im2d!</code></pre><pre><code class="language-none">NNlib.col2im3d!</code></pre><pre><code class="language-none">NNlib.col2im_2d!</code></pre><pre><code class="language-none">NNlib.col2im_3d!</code></pre><pre><code class="language-none">NNlib.conv</code></pre><pre><code class="language-none">NNlib.conv!</code></pre><pre><code class="language-none">NNlib.conv2d!</code></pre><pre><code class="language-none">NNlib.conv2d_grad_w!</code></pre><pre><code class="language-none">NNlib.conv2d_grad_x!</code></pre><pre><code class="language-none">NNlib.conv3d!</code></pre><pre><code class="language-none">NNlib.conv3d_grad_w!</code></pre><pre><code class="language-none">NNlib.conv3d_grad_x!</code></pre><pre><code class="language-none">NNlib.crosscor</code></pre><pre><code class="language-none">NNlib.crosscor!</code></pre><pre><code class="language-none">NNlib.dcdims</code></pre><pre><code class="language-none">NNlib.depthwiseconv</code></pre><pre><code class="language-none">NNlib.depthwiseconv!</code></pre><pre><code class="language-none">NNlib.depthwiseconv2d!</code></pre><pre><code class="language-none">NNlib.depthwiseconv2d_grad_w!</code></pre><pre><code class="language-none">NNlib.depthwiseconv2d_grad_x!</code></pre><pre><code class="language-none">NNlib.depthwisecrosscor</code></pre><pre><code class="language-none">NNlib.depthwisecrosscor!</code></pre><pre><code class="language-none">NNlib.dilation_dims</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.elu" href="#NNlib.elu"><code>NNlib.elu</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">elu(x, α = 1) =
  x &gt; 0 ? x : α * (exp(x) - 1)</code></pre><p>Exponential Linear Unit activation function. See <a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units</a>. You can also specify the coefficient explicitly, e.g. <code>elu(x, 1)</code>.</p></div></div></section><pre><code class="language-none">NNlib.eval</code></pre><pre><code class="language-none">NNlib.expand</code></pre><pre><code class="language-none">NNlib.float2integer</code></pre><pre><code class="language-none">NNlib.gemm!</code></pre><pre><code class="language-none">NNlib.head</code></pre><pre><code class="language-none">NNlib.ilogb2k</code></pre><pre><code class="language-none">NNlib.im2col2d!</code></pre><pre><code class="language-none">NNlib.im2col3d!</code></pre><pre><code class="language-none">NNlib.im2col_2d!</code></pre><pre><code class="language-none">NNlib.im2col_3d!</code></pre><pre><code class="language-none">NNlib.im2col_dims</code></pre><pre><code class="language-none">NNlib.include</code></pre><pre><code class="language-none">NNlib.ldexp3k</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.leakyrelu" href="#NNlib.leakyrelu"><code>NNlib.leakyrelu</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">leakyrelu(x) = max(0.01x, x)</code></pre><p>Leaky <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function. You can also specify the coefficient explicitly, e.g. <code>leakyrelu(x, 0.01)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.log_fast" href="#NNlib.log_fast"><code>NNlib.log_fast</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">log_fast(x)</code></pre><p>Compute the natural logarithm of <code>x</code>. The inverse of the natural logarithm is the natural expoenential function <code>exp(x)</code></p></div></div></section><pre><code class="language-none">NNlib.log_fast_kernel</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.logsigmoid" href="#NNlib.logsigmoid"><code>NNlib.logsigmoid</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">logσ(x)</code></pre><p>Return <code>log(σ(x))</code> which is computed in a numerically stable way.</p><pre><code class="language-none">julia&gt; logσ(0.)
-0.6931471805599453
julia&gt; logσ.([-100, -10, 100.])
3-element Array{Float64,1}:
 -100.0
  -10.0
   -0.0</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.logsoftmax" href="#NNlib.logsoftmax"><code>NNlib.logsoftmax</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">logsoftmax(xs) = log.(exp.(xs) ./ sum(exp.(xs)))</code></pre><p>logsoftmax computes the log of softmax(xs) and it is more numerically stable than softmax function in computing the cross entropy loss.</p></div></div></section><pre><code class="language-none">NNlib.logsoftmax!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.logσ" href="#NNlib.logσ"><code>NNlib.logσ</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">logσ(x)</code></pre><p>Return <code>log(σ(x))</code> which is computed in a numerically stable way.</p><pre><code class="language-none">julia&gt; logσ(0.)
-0.6931471805599453
julia&gt; logσ.([-100, -10, 100.])
3-element Array{Float64,1}:
 -100.0
  -10.0
   -0.0</code></pre></div></div></section><pre><code class="language-none">NNlib.max_pooling2d_bwd!</code></pre><pre><code class="language-none">NNlib.max_pooling2d_fwd!</code></pre><pre><code class="language-none">NNlib.max_pooling3d_bwd!</code></pre><pre><code class="language-none">NNlib.max_pooling3d_fwd!</code></pre><pre><code class="language-none">NNlib.maxpool</code></pre><pre><code class="language-none">NNlib.maxpool!</code></pre><pre><code class="language-none">NNlib.maxpool2d!</code></pre><pre><code class="language-none">NNlib.maxpool2d_grad!</code></pre><pre><code class="language-none">NNlib.maxpool3d!</code></pre><pre><code class="language-none">NNlib.maxpool3d_grad!</code></pre><pre><code class="language-none">NNlib.maxpool_cpu!</code></pre><pre><code class="language-none">NNlib.mean_pooling2d_bwd!</code></pre><pre><code class="language-none">NNlib.mean_pooling2d_fwd!</code></pre><pre><code class="language-none">NNlib.mean_pooling3d_bwd!</code></pre><pre><code class="language-none">NNlib.mean_pooling3d_fwd!</code></pre><pre><code class="language-none">NNlib.meanpool</code></pre><pre><code class="language-none">NNlib.meanpool!</code></pre><pre><code class="language-none">NNlib.meanpool2d!</code></pre><pre><code class="language-none">NNlib.meanpool2d_grad!</code></pre><pre><code class="language-none">NNlib.meanpool3d!</code></pre><pre><code class="language-none">NNlib.meanpool3d_grad!</code></pre><pre><code class="language-none">NNlib.meanpool_cpu!</code></pre><pre><code class="language-none">NNlib.padtuple</code></pre><pre><code class="language-none">NNlib.pdims</code></pre><pre><code class="language-none">NNlib.psize</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.relu" href="#NNlib.relu"><code>NNlib.relu</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">relu(x) = max(0, x)</code></pre><p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.selu" href="#NNlib.selu"><code>NNlib.selu</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))

λ ≈ 1.0507
α ≈ 1.6733</code></pre><p>Scaled exponential linear units. See <a href="https://arxiv.org/pdf/1706.02515.pdf">Self-Normalizing Neural Networks</a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.sigmoid" href="#NNlib.sigmoid"><code>NNlib.sigmoid</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">σ(x) = 1 / (1 + exp(-x))</code></pre><p>Classic <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> activation function.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.softmax" href="#NNlib.softmax"><code>NNlib.softmax</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">softmax(xs) = exp.(xs) ./ sum(exp.(xs))</code></pre><p><a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> takes log-probabilities (any real vector) and returns a probability distribution that sums to 1.</p><p>If given a matrix it will treat it as a batch of vectors, with each column independent.</p><pre><code class="language-none">julia&gt; softmax([1,2,3.])
3-element Array{Float64,1}:
  0.0900306
  0.244728
  0.665241</code></pre></div></div></section><pre><code class="language-none">NNlib.softmax!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.softplus" href="#NNlib.softplus"><code>NNlib.softplus</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">softplus(x) = log(exp(x) + 1)</code></pre><p>See <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.softsign" href="#NNlib.softsign"><code>NNlib.softsign</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">softsign(x) = x / (1 + |x|)</code></pre><p>See <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205">Quadratic Polynomials Learn Better Image Features</a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.swish" href="#NNlib.swish"><code>NNlib.swish</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">swish(x) = x * σ(x)</code></pre><p>Self-gated actvation function. See <a href="https://arxiv.org/pdf/1710.05941.pdf">Swish: a Self-Gated Activation Function</a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.σ" href="#NNlib.σ"><code>NNlib.σ</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">σ(x) = 1 / (1 + exp(-x))</code></pre><p>Classic <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> activation function.</p></div></div></section><pre><code class="language-none">NNlib.σ_stable</code></pre><pre><code class="language-none">NNlib.∇conv_data</code></pre><pre><code class="language-none">NNlib.∇conv_data!</code></pre><pre><code class="language-none">NNlib.∇conv_filter</code></pre><pre><code class="language-none">NNlib.∇conv_filter!</code></pre><pre><code class="language-none">NNlib.∇depthwiseconv_data</code></pre><pre><code class="language-none">NNlib.∇depthwiseconv_data!</code></pre><pre><code class="language-none">NNlib.∇depthwiseconv_filter</code></pre><pre><code class="language-none">NNlib.∇depthwiseconv_filter!</code></pre><pre><code class="language-none">NNlib.∇logsoftmax</code></pre><pre><code class="language-none">NNlib.∇maxpool</code></pre><pre><code class="language-none">NNlib.∇maxpool!</code></pre><pre><code class="language-none">NNlib.∇maxpool_cpu!</code></pre><pre><code class="language-none">NNlib.∇meanpool</code></pre><pre><code class="language-none">NNlib.∇meanpool!</code></pre><pre><code class="language-none">NNlib.∇meanpool_cpu!</code></pre><pre><code class="language-none">NNlib.∇softmax</code></pre><pre><code class="language-none">NNlib.∇softmax!</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
