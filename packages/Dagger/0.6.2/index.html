<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · Dagger.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Dagger.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#DAG-creation-interface-1">DAG creation interface</a></li><li><a class="toctext" href="#Rough-high-level-description-of-scheduling-1">Rough high level description of scheduling</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Dagger-1" href="#Dagger-1">Dagger</a></h1><p><strong>A framework for out-of-core and parallel computing</strong>.</p><p><a href="https://travis-ci.org/JuliaParallel/Dagger.jl"><img src="https://travis-ci.org/JuliaParallel/Dagger.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/JuliaParallel/Dagger.jl?branch=master"><img src="https://coveralls.io/repos/github/JuliaParallel/Dagger.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><p>At the core of Dagger.jl is a scheduler heavily inspired by <a href="http://dask.pydata.org/en/latest/">Dask</a>. It can run computations represented as <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed-acyclic-graphs</a> (DAGs) efficiently on many Julia worker processes.</p><h1><a class="nav-anchor" id="DAG-creation-interface-1" href="#DAG-creation-interface-1">DAG creation interface</a></h1><p>Here is an example DAG:</p><pre><code class="language-julia">using Dagger

p = delayed(f; options...)(42)
q = delayed(g)(p)
r = delayed(h)(53)
s = delayed(combine)(p, q, r)</code></pre><p>The connections between nodes <code>p</code>, <code>q</code>, <code>r</code> and <code>s</code> is represented by this dependency graph:</p><p><img src="https://user-images.githubusercontent.com/25916/26920104-7b9b5fa4-4c55-11e7-97fb-fe5b9e73cae6.png" alt="graph"/></p><p><code>delayed(f; options...)</code></p><p>Returns a function which when called creates a <code>Thunk</code> object representing a call to function <code>f</code> with the given arguments. If it is called with other thunks as input, then they form a graph with input nodes directed at the output. The function <code>f</code> get the result of evaluating the input thunks.</p><p>To compute and fetch the result of a thunk (say <code>s</code>), you can call <code>collect(s)</code>. <code>collect</code> will fetch the result of the computation to the master process. Alternatively, if you want to compute but not fetch the result you can call <code>compute</code> on the thunk. This will return a <code>Chunk</code> object which references the result. If you pass in a <code>Chunk</code> objects as an input to a delayed function, then the function will get executed with the value of the <code>Chunk</code> – this evaluation will likely happen where the input chunks are, to reduce communication.</p><p>Options to <code>delayed</code> are:</p><ul><li><code>get_result::Bool</code> – return the actual result to the scheduler instead of <code>Chunk</code> objects. Used when <code>f</code> explicitly constructs a Chunk or when return value is small (e.g. in case of reduce)</li><li><code>meta::Bool</code> – pass the input “Chunk” objects themselves to <code>f</code> and not the value contained in them - this is always run on the master process</li><li><code>persist::Bool</code> – the result of this Thunk should not be released after it becomes unused in the DAG</li><li><code>cache::Bool</code> – cache the result of this Thunk such that if the thunk is evaluated again, one can just reuse the cached value. If it’s been removed from cache, recompute the value.</li></ul><h2><a class="nav-anchor" id="Rough-high-level-description-of-scheduling-1" href="#Rough-high-level-description-of-scheduling-1">Rough high level description of scheduling</a></h2><ul><li>First picks the leaf Thunks and distributes them to available workers. Each worker is given at most 1 task at a time. If input to the node is a Chunk, then workers which already have the chunk are preferred.</li><li>When a worker finishes a thunk it will return a <code>Chunk</code> object to the scheduler.</li><li>Once the worker has returned a Chunk, scheduler picks the next task for the worker – this is usually the task the worker immediately made available (if possible). In the small example above, if worker 2 finished <code>p</code> it will be given <code>q</code> since it will already have the result of <code>p</code> which is input to <code>q</code>.</li><li>The scheduler also issues &quot;release&quot; Commands to chunks that are no longer required by nodes in the DAG: for example, when s is computed all of p, q, r are released to free up memory. This can be prevented by passing <code>persist</code> or <code>cache</code> options to <code>delayed</code>.</li></ul><h3><a class="nav-anchor" id="Acknowledgements-1" href="#Acknowledgements-1">Acknowledgements</a></h3><p>We thank DARPA, Intel, and the NIH for supporting this work at MIT.</p><footer><hr/></footer></article></body></html>
