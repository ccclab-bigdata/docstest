<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · NetworkLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NetworkLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Introduction-1">Introduction</a></li><li><a class="toctext" href="#Features-1">Features</a></li><li><a class="toctext" href="#Observation-based-network-learning-example-1">Observation-based network learning example</a></li><li><a class="toctext" href="#Entity-based-network-learning-example-1">Entity-based network learning example</a></li><li><a class="toctext" href="#Documentation-1">Documentation</a></li><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#License-1">License</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="NetworkLearning-1" href="#NetworkLearning-1">NetworkLearning</a></h1><p>A Julia package for network learning.</p><p><a href="LICENSE.md"><img src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" alt="License"/></a>  <a href="http://pkg.julialang.org/detail/NetworkLearning"><img src="http://pkg.julialang.org/badges/NetworkLearning_0.6.svg" alt="NetworkLearning"/></a> <a href="https://travis-ci.org/zgornel/NetworkLearning.jl"><img src="https://travis-ci.org/zgornel/NetworkLearning.jl.svg?branch=master" alt="Build Status"/></a>  <a href="https://coveralls.io/github/zgornel/NetworkLearning.jl?branch=master"><img src="https://coveralls.io/repos/github/zgornel/NetworkLearning.jl/badge.svg?branch=master" alt="Coverage Status"/></a></p><h2><a class="nav-anchor" id="Introduction-1" href="#Introduction-1">Introduction</a></h2><p>NetworkLearning implements a generic framework for network classification. It could in theory be used to provide additional functionality (i.e. semi-supervised learning, regression), provided that appropriate methods for relational learning (i.e. relational variable generation) and collective inference are added. The framework is designed to make as little assumptions as possible on the elements involved in the process.  </p><p>Two scenarios or usecases are covered:</p><ul><li><p><em>Observation-based learning</em>, in which the network structure is pertinent to the observations and consequently, estimates (i.e. class probabilities) are associated to the observations; in the estimation process, relational structures can either make use the training data (in-graph learning) or not (out-of-graph learning). For example, in the case of document classifcation, an observation would correspond to a publication that has to be classified into an arbitrary category, given a representation of its local content-based information as well on the its relational information (links to other documents, citations etc.).  </p></li><li><p><em>Entity-based learning</em>, in which observations are pertinent to one or more abstract entities for which estimates are calculated. In entity-based network learning, observations can modify either the local or relational information of one or more entities.</p></li></ul><h2><a class="nav-anchor" id="Features-1" href="#Features-1">Features</a></h2><ul><li><strong>Learner type</strong></li></ul><pre><code class="language-none">- observation-based
- entity-based</code></pre><ul><li><strong>Relational learners</strong></li></ul><pre><code class="language-none">- simple relational neighbour
- weighted/probabilistic relational neighbour
- naive bayes
- class distribution</code></pre><ul><li><strong>Collective inference</strong></li></ul><pre><code class="language-none">- relaxation labeling
- collective classification
- gibbs sampling (EXPERIMENTAL, slow)</code></pre><ul><li><strong>Adjacency strucures</strong></li></ul><pre><code class="language-none">- matrices
- graphs
- tuples containing functions and data from which adjacency matrices or graphs can be computed</code></pre><h2><a class="nav-anchor" id="Observation-based-network-learning-example-1" href="#Observation-based-network-learning-example-1">Observation-based network learning example</a></h2><pre><code class="language-julia">import DecisionTree
using NetworkLearning, MLDataPattern, MLLabelUtils, LossFunctions

# Download the CORA dataset, and return data and citing/cited papers indices (relative to order in X,y)
(X,y), cited_papers, citing_papers = NetworkLearning.grab_cora_data()
n = nobs(X)
yᵤ = sort(unique(y))
C = length(yᵤ)

# Split data
idx = collect(1:n);
shuffle!(idx)
p = 0.5
idxtr,idxts = splitobs(idx,p)
Xtr = X[:,idxtr]
ytr = y[idxtr]
Xts = X[:,idxts]

# Build adjacency matrices
Atr = NetworkLearning.generate_partial_adjacency(cited_papers, citing_papers, idxtr);

# Construct necessary arguments for training the network learner
fl_train = (X::Tuple{Matrix{Float64}, Vector{Int}})-&gt;  DecisionTree.build_tree(X[2],X[1]&#39;)
fl_exec(C) = (m,X)-&gt; DecisionTree.apply_tree_proba(m, X&#39;, collect(1:C))&#39;

fr_train = (X)-&gt;  DecisionTree.build_tree(X[2],X[1]&#39;)
fr_exec(C) = (m,X)-&gt; DecisionTree.apply_tree_proba(m, X&#39;, collect(1:C))&#39;

AV = [adjacency(Atr)]

# Train
info(&quot;Training ...&quot;)
nlmodel = NetworkLearning.fit(NetworkLearnerObs, 
	      Xtr,
	      ytr,
	      AV,
	      fl_train, fl_exec(C),
	      fr_train, fr_exec(C);
	      learner = :wrn,
	      inference = :ic,
	      use_local_data = false, # use only relational variables
	      f_targets = x-&gt;targets(indmax,x),
	      normalize = true,
	      maxiter = 10,
	      α = 0.95
	  )



#########################
# Out-of-Graph learning #
#########################

# Add adjacency pertinent to the test data
Ats = NetworkLearning.generate_partial_adjacency(cited_papers, citing_papers, idxts);
add_adjacency!(nlmodel, [Ats])

# Make predictions
info(&quot;Predicting (out-of-graph) ...&quot;)
ŷts = predict(nlmodel, Xts)

# Squared loss
yts = convertlabel(LabelEnc.OneOfK(C), y[idxts], yᵤ)
println(&quot;\tL2 loss (out-of-graph):&quot;, value(L2DistLoss(), yts, ŷts, AvgMode.Mean()))
println(&quot;\tAverage error (out-of-graph):&quot;, mean(targets(indmax,yts).!=targets(indmax,ŷts)))



#####################
# In-Graph learning #
#####################

# Initialize output structure
Xo = zeros(C,nobs(X))
Xo[:,idxtr] = convertlabel(LabelEnc.OneOfK(C), y[idxtr] ,yᵤ)

# Add adjacency pertinent to the test data
Ats = NetworkLearning.generate_partial_adjacency(cited_papers, citing_papers, collect(1:nobs(X)));
add_adjacency!(nlmodel, [Ats])

# Make predictions
info(&quot;Predicting (in-graph) ...&quot;)
update_mask = trues(nobs(X));
update_mask[idxtr] = false # estimates for training samples will not be used
predict!(Xo, nlmodel, X, update_mask)

# Squared loss
ŷts = Xo[:,idxts]
yts = convertlabel(LabelEnc.OneOfK(C), y[idxts], yᵤ)
println(&quot;\tL2 loss (in-graph):&quot;, value(L2DistLoss(), yts, ŷts, AvgMode.Mean()))
println(&quot;\tAverage error (in-graph):&quot;, mean(targets(indmax,yts).!=targets(indmax,ŷts)))</code></pre><p>The output of the above code is:</p><pre><code class="language-julia">#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
#                                  Dload  Upload   Total   Spent    Left  Speed
# 100  163k  100  163k    0     0   163k      0  0:00:01  0:00:01 --:--:-- 86695
# cora/
# cora/README
# cora/cora.content
# cora/cora.cites
# INFO: Training ...
# INFO: Predicting (out-of-graph) ...
# 	  L2 loss (out-of-graph):0.13011389156615571
# 	  Average error (out-of-graph):0.5310192023633677
# INFO: Predicting (in-graph) ...
# 	  L2 loss (in-graph):0.06473003424857691
#	  Average error (in-graph):0.24963072378138848</code></pre><h2><a class="nav-anchor" id="Entity-based-network-learning-example-1" href="#Entity-based-network-learning-example-1">Entity-based network learning example</a></h2><pre><code class="language-julia">import DecisionTree
using NetworkLearning, MLDataPattern, MLLabelUtils, LossFunctions

# Download the CORA dataset, and return data and citing/cited papers indices (relative to order in X,y)
(X,y), cited_papers, citing_papers = NetworkLearning.grab_cora_data()
n = nobs(X)
yᵤ = sort(unique(y))
C = length(yᵤ)

# Split data
idx = collect(1:n);
shuffle!(idx)
p = 0.5
idxtr,idxts = splitobs(idx,p)

### !!! ###### 	
sort!(idxtr) # It is important to sort the indices, 
sort!(idxts) # because of the use of the update mask
### !!! ######

Xtr = X[:,idxtr]
ytr = y[idxtr]
Xts = X[:,idxts]


############### Remove 70% of the citations to papers in the test data ################## 	
removed_citations = Vector{Int}()							#
for (i, (ic,oc)) in enumerate(zip(cited_papers,citing_papers))				#
	if ic in idxts &amp;&amp; rand() &gt; 0.3 							#
		push!(removed_citations, i)						#	
	end										#
end											#
											#
cited_incomplete = cited_papers[setdiff(1:nobs(cited_papers), removed_citations)]	#
citing_incomplete = citing_papers[setdiff(1:nobs(citing_papers), removed_citations)]	#
											#
cited_removed = cited_papers[removed_citations]						#
citing_removed = citing_papers[removed_citations]					#
#########################################################################################


# Construct adjacencies, local model, etc
Am = sparse(NetworkLearning.generate_partial_adjacency(cited_incomplete, citing_incomplete, collect(1:n)));
AV = [adjacency(Am)]
Ml = DecisionTree.build_tree(ytr,Xtr&#39;)

# Initialize output estimates and update mask
Xo = zeros(C,n)
update = falses(n);
update[idx[findin(idx,idxts)]] = true # mark only test entities i.e. unknown as updateable
Xo[:,.!update] = convertlabel(LabelEnc.OneOfK(C), ytr ,yᵤ)
Xo[:,update] = DecisionTree.apply_tree_proba(Ml, Xts&#39;, yᵤ)&#39;
ŷ_tree = copy(Xo[:, update])

# Construct necessary arguments for training the entity network learner
fr_train = (X)-&gt;  DecisionTree.build_tree(X[2],X[1]&#39;)
fr_exec(C) = (m,X)-&gt; DecisionTree.apply_tree_proba(m, X&#39;, collect(1:C))&#39;


# Train
info(&quot;Training ...&quot;)
nlmodel = NetworkLearning.fit(NetworkLearnerEnt, 
	      Xo,
	      update,
	      AV,
	      fr_train, fr_exec(C);
	      learner = :wrn,
	      inference = :ic,
	      f_targets = x-&gt;targets(indmax,x),
	      normalize = true,
	      maxiter = 10,
	      α = 0.95
	  )


# Squared loss (with just a few citations)
ŷts = nlmodel.state.ê[:,update]
yts = convertlabel(LabelEnc.OneOfK(C), y[idxts], yᵤ)
println(&quot;\tL2 loss (few citations):&quot;, value(L2DistLoss(), yts, ŷts, AvgMode.Mean()))
println(&quot;\tAverage error (few citations):&quot;, mean(targets(indmax,yts).!=targets(indmax,ŷts)))

# Add citations (i.e. update adjacency matrices of the model)

# Function that updates an adjacency matrix given two vectors (of same length)
# of cited and citing paper; the function may be more complicated depending on
# how easy the corresponding adjacency matrix coordinates can be determined
# from the input data
function add_citations!(Am, cited, citing)
	for i in 1:nobs(cited)
		Am[cited[i],citing[i]] += 1
		Am[citing[i],cited[i]] += 1
	end
	return Am
end

info(&quot;Updating adjacencies ...&quot;)
f_update(ic,oc) = x-&gt;add_citations!(x, ic, oc)
update_adjacency!(nlmodel.Adj[1], f_update(cited_removed, citing_removed))

# Run again collective inference
info(&quot;Re-running collective inference ...&quot;)
infer!(nlmodel)

# Squared loss (with all citations)
ŷts = nlmodel.state.ê[:,update]
yts = convertlabel(LabelEnc.OneOfK(C), y[idxts], yᵤ)
println(&quot;\tL2 loss (all citations):&quot;, value(L2DistLoss(), yts, ŷts, AvgMode.Mean()))
println(&quot;\tAverage error (all citations):&quot;, mean(targets(indmax,yts).!=targets(indmax,ŷts)))</code></pre><p>The output of the above code is:</p><pre><code class="language-julia">#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
#                                  Dload  Upload   Total   Spent    Left  Speed
# 100  163k  100  163k    0     0   163k      0  0:00:01  0:00:01 --:--:-- 86382
# cora/
# cora/README
# cora/cora.content
# cora/cora.cites
# INFO: Training ...
# 	  L2 loss (few citations):0.061311528508626575
#	  Average error (few citations):0.27843426883308714
# INFO: Updating adjacencies ...
# INFO: Re-running collective inference ...
#	  L2 loss (all citations):0.04990883428571481
#	  Average error (all citations):0.2119645494830133</code></pre><h2><a class="nav-anchor" id="Documentation-1" href="#Documentation-1">Documentation</a></h2><p>The documentation is provided in Julia&#39;s native docsystem. </p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>The package can be installed by running <code>Pkg.add(&quot;NetworkLearning&quot;)</code> or, to check out the latest version, <code>Pkg.checkout(&quot;NetworkLearning.jl&quot;)</code> in the Julia REPL. From <code>v0.1.0</code>, only versions of Julia above 0.7  are supported. Julia v.0.6 support can be found in the <code>support_julia_v0.6</code> branch (currently unmantained).</p><h2><a class="nav-anchor" id="License-1" href="#License-1">License</a></h2><p>This code has an MIT license and therefore it is free.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><p>[1] S.A. Macskassy, F. Provost &quot;Classification in networked data: A toolkit and a univariate case study&quot;, Journal of Machine learning Research 8, 2007, 935-983</p><p>[2] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, T. Eliassi-Rad &quot;Collective classification in network data&quot;, AI Magazine 29(3), 2008</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
