<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · TensorOperations.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>TensorOperations.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">TensorOperations.@dividebody</code></pre><pre><code class="language-none">TensorOperations.@optimalcontractiontree</code></pre><pre><code class="language-none">TensorOperations.@stridedloops</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.@tensor" href="#TensorOperations.@tensor"><code>TensorOperations.@tensor</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@tensor(block)</code></pre><p>Specify one or more tensor operations using Einstein&#39;s index notation. Indices can be chosen to be arbitrary Julia variable names, or integers. When contracting several tensors together, this will be evaluated as pairwise contractions in left to right order, unless the so-called NCON style is used (positive integers for contracted indices and negative indices for open indices).</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.@tensoropt" href="#TensorOperations.@tensoropt"><code>TensorOperations.@tensoropt</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@tensoropt(optex, block)
@tensoropt(block)</code></pre><p>Specify one or more tensor operations using Einstein&#39;s index notation. Indices can be chosen to be arbitrary Julia variable names, or integers. When contracting several tensors together, the macro will determine (at compile time) the optimal contraction order depending on the cost associated to the individual indices. If no <code>optex</code> is provided, all indices are assumed to have an abstract scaling <code>χ</code> which is optimized in the asympotic limit of large <code>χ</code>.</p><p>The cost can be specified in the following ways:</p><pre><code class="language-julia">@tensoropt (a=&gt;χ,b=&gt;χ^2,c=&gt;2*χ,e=&gt;5) C[a,b,c,d] := A[a,e,c,f,h]*B[f,g,e,b]*C[g,d,h]
# asymptotic cost as specified for listed indices, unlisted indices have cost 1 (any symbol for χ can be used)
@tensoropt (a,b,c,e) C[a,b,c,d] := A[a,e,c,f,h]*B[f,g,e,b]*C[g,d,h]
# asymptotic cost χ for indices a,b,c,e, other indices (d,f) have cost 1
@tensoropt !(a,b,c,e) C[a,b,c,d] := A[a,e,c,f,h]*B[f,g,e,b]*C[g,d,h]
# cost 1 for indices a,b,c,e; other indices (d,f) have asymptotic cost χ
@tensoropt C[a,b,c,d] := A[a,e,c,f,h]*B[f,g,e,b]*C[g,d,h]
# asymptotic cost χ for all indices (a,b,c,d,e,f)</code></pre><p>Note that <code>@tensoropt</code> will optimize any tensor contraction sequence it encounters in the (block of) expressions. It will however not break apart expressions that have been explicitly grouped with parenthesis, i.e. in</p><pre><code class="language-julia">@tensoroptC[a,b,c,d] := A[a,e,c,f,h]*(B[f,g,e,b]*C[g,d,h])</code></pre><p>it will always contract <code>B</code> and <code>C</code> first. For a single tensor contraction sequence, the optimal contraction order and associated (asymptotic) cost can be obtained using <code>@optimalcontractiontree</code>.</p></div></div></section><pre><code class="language-none">TensorOperations.AbstractPoly</code></pre><pre><code class="language-none">TensorOperations.BASELENGTH</code></pre><pre><code class="language-none">TensorOperations.ConjugatedStridedData</code></pre><pre><code class="language-none">TensorOperations.IndexError</code></pre><pre><code class="language-none">TensorOperations.IndexTuple</code></pre><pre><code class="language-none">TensorOperations.NormalStridedData</code></pre><pre><code class="language-none">TensorOperations.One</code></pre><pre><code class="language-none">TensorOperations.Poly</code></pre><pre><code class="language-none">TensorOperations.Power</code></pre><pre><code class="language-none">TensorOperations.StaticLength</code></pre><pre><code class="language-none">TensorOperations.StridedData</code></pre><pre><code class="language-none">TensorOperations.StridedSubArray</code></pre><pre><code class="language-none">TensorOperations.TensorOperations</code></pre><pre><code class="language-none">TensorOperations.Zero</code></pre><pre><code class="language-none">TensorOperations._dividebody</code></pre><pre><code class="language-none">TensorOperations._filterdims</code></pre><pre><code class="language-none">TensorOperations._findfirst</code></pre><pre><code class="language-none">TensorOperations._findlast</code></pre><pre><code class="language-none">TensorOperations._findnext</code></pre><pre><code class="language-none">TensorOperations._indmax</code></pre><pre><code class="language-none">TensorOperations._intersect</code></pre><pre><code class="language-none">TensorOperations._isemptyset</code></pre><pre><code class="language-none">TensorOperations._memjumps</code></pre><pre><code class="language-none">TensorOperations._ncontree!</code></pre><pre><code class="language-none">TensorOperations._one</code></pre><pre><code class="language-none">TensorOperations._optimaltree</code></pre><pre><code class="language-none">TensorOperations._permute</code></pre><pre><code class="language-none">TensorOperations._scale!</code></pre><pre><code class="language-none">TensorOperations._setdiff</code></pre><pre><code class="language-none">TensorOperations._sreplace</code></pre><pre><code class="language-none">TensorOperations._stridedloops</code></pre><pre><code class="language-none">TensorOperations._union</code></pre><pre><code class="language-none">TensorOperations._zero</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.add!" href="#TensorOperations.add!"><code>TensorOperations.add!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">add!(α, A, conjA, β, C, indCinA)</code></pre><p>Implements <code>C = β*C+α*permute(op(A))</code> where <code>A</code> is permuted according to <code>indCinA</code> and <code>op</code> is <code>conj</code> if <code>conjA=Val{:C}</code> or the identity map if <code>conjA=Val{:N}</code>. The indexable collection <code>indCinA</code> contains as nth entry the dimension of <code>A</code> associated with the nth dimension of <code>C</code>.</p></div></div></section><pre><code class="language-none">TensorOperations.add_indices</code></pre><pre><code class="language-none">TensorOperations.add_micro!</code></pre><pre><code class="language-none">TensorOperations.add_rec!</code></pre><pre><code class="language-none">TensorOperations.add_strides</code></pre><pre><code class="language-none">TensorOperations.axpby</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.checkindices" href="#TensorOperations.checkindices"><code>TensorOperations.checkindices</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">checkindices(A, IA)</code></pre><p>Checks whether the indices in <code>IA</code> are compatible with the object <code>A</code>. The fallback method checks <code>length(IA) == numind(A)</code>.</p></div></div></section><pre><code class="language-none">TensorOperations.computecost</code></pre><pre><code class="language-none">TensorOperations.connectedcomponents</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.contract!" href="#TensorOperations.contract!"><code>TensorOperations.contract!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">contract!(α, A, conjA, B, conjB, β, C, oindA, cindA, oindB, cindB, indCinoAB, [method])</code></pre><p>Implements <code>C = β*C+α*contract(op(A),op(B))</code> where <code>A</code> and <code>B</code> are contracted according to <code>oindA</code>, <code>cindA</code>, <code>oindB</code>, <code>cindB</code> and <code>indCinoAB</code>. The operation <code>op</code> acts as <code>conj</code> if <code>conjA</code> or <code>conjB</code> equal <code>Val{:C}</code> or as the identity map if <code>conjA</code> (<code>conjB</code>) equal <code>Val{:N}</code>. The dimension <code>cindA[i]</code> of <code>A</code> is contracted with dimension <code>cindB[i]</code> of <code>B</code>. The <code>n</code>th dimension of C is associated with an uncontracted (open) dimension of <code>A</code> or <code>B</code> according to <code>indCinoAB[n] &lt; NoA ? oindA[indCinoAB[n]] : oindB[indCinoAB[n]-NoA]</code> with <code>NoA=length(oindA)</code> the number of open dimensions of <code>A</code>.</p><p>The optional argument <code>method</code> specifies whether the contraction is performed using BLAS matrix multiplication by specifying <code>Val{:BLAS}</code> (default), or using a native algorithm by specifying <code>Val{:native}</code>. The native algorithm does not copy the data but is typically slower.</p></div></div></section><pre><code class="language-none">TensorOperations.contract_indices</code></pre><pre><code class="language-none">TensorOperations.contract_micro!</code></pre><pre><code class="language-none">TensorOperations.contract_rec!</code></pre><pre><code class="language-none">TensorOperations.contract_strides</code></pre><pre><code class="language-none">TensorOperations.degree</code></pre><pre><code class="language-none">TensorOperations.deindexify</code></pre><pre><code class="language-none">TensorOperations.deindexify!</code></pre><pre><code class="language-none">TensorOperations.eval</code></pre><pre><code class="language-none">TensorOperations.getallindices</code></pre><pre><code class="language-none">TensorOperations.getindices</code></pre><pre><code class="language-none">TensorOperations.getlhsrhs</code></pre><pre><code class="language-none">TensorOperations.hastraceindices</code></pre><pre><code class="language-none">TensorOperations.include</code></pre><pre><code class="language-none">TensorOperations.isassignment</code></pre><pre><code class="language-none">TensorOperations.isdefinition</code></pre><pre><code class="language-none">TensorOperations.isgeneraltensor</code></pre><pre><code class="language-none">TensorOperations.isindex</code></pre><pre><code class="language-none">TensorOperations.isnconstyle</code></pre><pre><code class="language-none">TensorOperations.isscalarexpr</code></pre><pre><code class="language-none">TensorOperations.istensor</code></pre><pre><code class="language-none">TensorOperations.istensorexpr</code></pre><pre><code class="language-none">TensorOperations.makegeneraltensor</code></pre><pre><code class="language-none">TensorOperations.makeindex</code></pre><pre><code class="language-none">TensorOperations.makescalar</code></pre><pre><code class="language-none">TensorOperations.maketensor</code></pre><pre><code class="language-none">TensorOperations.memjumps</code></pre><pre><code class="language-none">TensorOperations.ncontree</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.numind" href="#TensorOperations.numind"><code>TensorOperations.numind</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">numind(A)</code></pre><p>Returns the number of indices of a tensor-like object <code>A</code>, i.e. for a multidimensional array (<code>&lt;:AbstractArray</code>) we have <code>numind(A) = ndims(A)</code>. Also works in type domain.</p></div></div></section><pre><code class="language-none">TensorOperations.optdata</code></pre><pre><code class="language-none">TensorOperations.optimaltree</code></pre><pre><code class="language-none">TensorOperations.parsecost</code></pre><pre><code class="language-none">TensorOperations.prime</code></pre><pre><code class="language-none">TensorOperations.processcontractorder</code></pre><pre><code class="language-none">TensorOperations.rangeexpr</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.scalar" href="#TensorOperations.scalar"><code>TensorOperations.scalar</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">scalar(C)</code></pre><p>Returns the single element of a tensor-like object with zero dimensions, i.e. if <code>numind(C)==0</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.similar_from_indices" href="#TensorOperations.similar_from_indices"><code>TensorOperations.similar_from_indices</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">similar_from_indices(::Type{T}, indices::NTuple{N,Int}, A, conjA=Val{:N}) where {T,N}</code></pre><p>Returns an object similar to <code>A</code> which has an <code>eltype</code> given by <code>T</code> and dimensions/sizes corresponding to a selection of those of <code>op(A)</code>, where the selection is specified by <code>indices</code> (which contains integer between <code>1</code> and <code>numind(A)</code>) and <code>op</code> is <code>conj</code> if <code>conjA=Val{:C}</code> or does nothing if <code>conjA=Val{:N}</code> (default).</p></div></div><div><div><pre><code class="language-none">similar_from_indices(::Type{T}, indices::NTuple{N,Int}, A, B, conjA=Val{:N}, conjB={:N}) where {T,N}</code></pre><p>Returns an object similar to <code>A</code> which has an <code>eltype</code> given by <code>T</code> and dimensions/sizes corresponding to a selection of those of <code>op(A)</code> and <code>op(B)</code> concatenated, where the selection is specified by <code>indices</code> (which contains integers between <code>1</code> and     <code>numind(A)+numind(B)</code> and <code>op</code> is <code>conj</code> if <code>conjA</code> or <code>conjB</code> equal <code>Val{:C}</code>     or does nothing if <code>conjA</code> or <code>conjB</code> equal <code>Val{:N}</code> (default).</p></div></div></section><pre><code class="language-none">TensorOperations.storeset</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.tensoradd" href="#TensorOperations.tensoradd"><code>TensorOperations.tensoradd</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">tensoradd(A, IA::Tuple, B, IB::Tuple, IC::Tuple=IA)</code></pre><p>Returns the result of adding arrays <code>A</code> and <code>B</code> where the iterabels <code>IA</code> and <code>IB</code> denote how the array data should be permuted in order to be added. More specifically, the result of this method is equivalent to</p><pre><code class="language-julia">tensorcopy(A, IA, IC) + tensorcopy(B, IB, IC)</code></pre><p>but without creating the temporary permuted arrays.</p></div></div></section><pre><code class="language-none">TensorOperations.tensoradd!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.tensorcontract" href="#TensorOperations.tensorcontract"><code>TensorOperations.tensorcontract</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">tensorcontract(A, IA::Tuple, B, IB::Tuple, IC::Tuple; method=:BLAS)</code></pre><p>Contract indices of array <code>A</code> with corresponding indices in array <code>B</code> by assigning them identical labels in the iterables <code>IA</code> and <code>IB</code>. The indices of the resulting array correspond to the indices that only appear in either <code>IA</code> or <code>IB</code> and can be ordered by specifying the optional argument <code>IC</code>. The default is to have all open indices of array <code>A</code> followed by all open indices of array <code>B</code>. Note that inner contractions of an array should be handled first with <code>tensortrace</code>, so that every label can appear only once in <code>labelsA</code> or <code>labelsB</code> seperately, and once (for open index) or twice (for contracted index) in the union of <code>labelsA</code> and <code>labelsB</code>.</p><p>The keyword argument <code>method</code> can take the values <code>:BLAS</code> or <code>:native</code>. The first option creates temporary copies of <code>A</code>, <code>B</code> and the resulting tensor where the indices are permuted such that the contractions become equivalent to a single matrix multiplication, which is then handled by BLAS. This is often the fastest approach and therefore the default value, but it does require sufficient memory and there is some overhead in allocating new memory (e.g. when doing this many times in a loop). In case <code>method</code> is set to <code>:native</code>, a native Julia function is called that performs the contraction without creating tempories, with special attention to cache-friendliness for maximal efficiency. This can be faster for small tensors or when the contraction pattern is non-generic, e.g. when it amounts to a scalar (no open indices) or a tensor product (no contraction indices).</p></div></div></section><pre><code class="language-none">TensorOperations.tensorcontract!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.tensorcopy" href="#TensorOperations.tensorcopy"><code>TensorOperations.tensorcopy</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">tensorcopy(A, IA, IC=IA)</code></pre><p>Creates a copy of <code>A</code>, where the dimensions of <code>A</code> are assigned indices from the iterable <code>IA</code> and the indices of the copy are contained in <code>IC</code>. Both iterables should contain the same elements in a different order.</p><p>The result of this method is equivalent to <code>permutedims(A, p)</code> where p is the permutation such that <code>IC=IA[p]</code>. The implementation of tensorcopy is however more efficient on average.</p></div></div></section><pre><code class="language-none">TensorOperations.tensorcopy!</code></pre><pre><code class="language-none">TensorOperations.tensorify</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.tensorproduct" href="#TensorOperations.tensorproduct"><code>TensorOperations.tensorproduct</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">tensorproduct(A, IA::Tuple, B, IB::Tuple, IC::Tuple = (IA..., IB...))</code></pre><p>Computes the tensor product of two arrays <code>A</code> and <code>B</code>, i.e. returns a new array <code>C</code> with <code>ndims(C)=ndims(A)+ndims(B)</code>. The indices of the output tensor are related to those of the input tensors by the pattern specified by the indices. Essentially, this is a special case of <code>tensorcontract</code> with no indices being contracted over. On top of just calling <code>tensorcontract</code> with <code>method=:native</code>, this method checks whether the indices indeed specify a tensor product instead of a genuine contraction.</p></div></div></section><pre><code class="language-none">TensorOperations.tensorproduct!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.tensortrace" href="#TensorOperations.tensortrace"><code>TensorOperations.tensortrace</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">tensortrace(A, IA::Tuple, IC::Tuple)</code></pre><p>Trace or contract pairs of indices of array <code>A</code>, by assigning them an identical indices in the iterable <code>IA</code>. The untraced indices, which are assigned a unique index, can be reordered according to the optional argument <code>IC</code>. The default value corresponds to the order in which they appear. Note that only pairs of indices can be contracted, so that every index in <code>IA</code> can appear only once (for an untraced index) or twice (for an index in a contracted pair).</p></div></div></section><pre><code class="language-none">TensorOperations.tensortrace!</code></pre><pre><code class="language-none">TensorOperations.tinvperm</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.trace!" href="#TensorOperations.trace!"><code>TensorOperations.trace!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">trace!(α, A, conjA, β, C, indCinA, cindA1, cindA2)</code></pre><p>Implements <code>C = β*C+α*partialtrace(op(A))</code> where <code>A</code> is permuted and partially traced, according to <code>indCinA</code>, <code>cindA1</code> and <code>cindA2</code>, and <code>op</code> is <code>conj</code> if <code>conjA=Val{:C}</code> or the identity map if <code>conjA=Val{:N}</code>. The indexable collection <code>indCinA</code> contains as nth entry the dimension of <code>A</code> associated with the nth dimension of <code>C</code>. The partial trace is performed by contracting dimension <code>cindA1[i]</code> of <code>A</code> with dimension <code>cindA2[i]</code> of <code>A</code> for all <code>i in 1:length(cindA1)</code>.</p></div></div></section><pre><code class="language-none">TensorOperations.trace_indices</code></pre><pre><code class="language-none">TensorOperations.trace_micro!</code></pre><pre><code class="language-none">TensorOperations.trace_rec!</code></pre><pre><code class="language-none">TensorOperations.trace_strides</code></pre><pre><code class="language-none">TensorOperations.tree2expr</code></pre><pre><code class="language-none">TensorOperations.tsetdiff</code></pre><pre><code class="language-none">TensorOperations.tunique</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorOperations.unique2" href="#TensorOperations.unique2"><code>TensorOperations.unique2</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">unique2(itr)</code></pre><p>Returns an array containing only those elements that appear exactly once in itr, and without any elements that appear more than once.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
