<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · SparseRegression.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SparseRegression.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li class="current"><a class="toctext" href>Algorithms</a><ul class="internal"><li><a class="toctext" href="#ProxGrad-1">ProxGrad</a></li><li><a class="toctext" href="#Fista-1">Fista</a></li><li><a class="toctext" href="#AdaptiveProxGrad-1">AdaptiveProxGrad</a></li><li><a class="toctext" href="#GradientDescent-1">GradientDescent</a></li><li><a class="toctext" href="#Sweep-1">Sweep</a></li><li><a class="toctext" href="#LinRegCholesky-1">LinRegCholesky</a></li><li><a class="toctext" href="#LineSearch-1">LineSearch</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Algorithms</a></li></ul></nav><hr/><div id="topbar"><span>Algorithms</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Algorithms-1" href="#Algorithms-1">Algorithms</a></h1><p>The first argument of an <code>Algorithm</code>&#39;s constructor is an <code>SModel</code>.  This is to ensure storage buffers are the correct size.</p><h2><a class="nav-anchor" id="ProxGrad-1" href="#ProxGrad-1">ProxGrad</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.ProxGrad" href="#SparseRegression.ProxGrad"><code>SparseRegression.ProxGrad</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ProxGrad(model, step = 1.0)</code></pre><p>Proximal gradient method with step size <code>step</code>.  Works for any loss and any penalty with a <code>prox</code> method.</p><p><strong>Example</strong></p><pre><code class="language-none">x, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)
s = SModel(x, y, L2DistLoss())
strat = strategy(MaxIter(50), ProxGrad(s))
learn!(s, strat)</code></pre></div></div></section><h2><a class="nav-anchor" id="Fista-1" href="#Fista-1">Fista</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.Fista" href="#SparseRegression.Fista"><code>SparseRegression.Fista</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Fista(model, step = 1.0)</code></pre><p>Accelerated proximal gradient method.  Works for any loss and any penalty with a <code>prox</code> method.</p></div></div></section><h2><a class="nav-anchor" id="AdaptiveProxGrad-1" href="#AdaptiveProxGrad-1">AdaptiveProxGrad</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.AdaptiveProxGrad" href="#SparseRegression.AdaptiveProxGrad"><code>SparseRegression.AdaptiveProxGrad</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AdaptiveProxGrad(s, divisor = 1.5, init = 1.0)</code></pre><p>Proximal gradient method with adaptive step sizes.  AdaptiveProxGrad uses element-wise  learning rates.  Every time the sign of a coefficient switches, the step size for that coefficient is divided by <code>divisor</code>.</p></div></div></section><h2><a class="nav-anchor" id="GradientDescent-1" href="#GradientDescent-1">GradientDescent</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.GradientDescent" href="#SparseRegression.GradientDescent"><code>SparseRegression.GradientDescent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">GradientDescent(model, step = 1.0)</code></pre><p>Gradient Descent.  Works for any loss and any penalty.</p><p><strong>Example</strong></p><pre><code class="language-none">x, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)
s = SModel(x, y, L2DistLoss())
strat = strategy(MaxIter(50), GradientDescent(s))
learn!(s, strat)</code></pre></div></div></section><h2><a class="nav-anchor" id="Sweep-1" href="#Sweep-1">Sweep</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.Sweep" href="#SparseRegression.Sweep"><code>SparseRegression.Sweep</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Sweep(model)</code></pre><p>Linear/ridge regression via sweep operator.  Works for (scaled) L2DistLoss with NoPenalty or L2Penalty.  The <code>Sweep</code> algorithm has a closed form solution and is complete after one iteration.  It therefore doesn&#39;t need additional learning strategies such as <code>MaxIter</code>, <code>Converged</code>, etc.</p><p><strong>Example</strong></p><pre><code class="language-none">x, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)
s = SModel(x, y, L2DistLoss())
learn!(s, Sweep(s))</code></pre></div></div></section><h2><a class="nav-anchor" id="LinRegCholesky-1" href="#LinRegCholesky-1">LinRegCholesky</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.LinRegCholesky" href="#SparseRegression.LinRegCholesky"><code>SparseRegression.LinRegCholesky</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LinRegCholesky(model)</code></pre><p>Linear/ridge regression via cholesky decomposition.  Works for (scaled) L2DistLoss with NoPenalty or L2Penalty.  The <code>LinRegCholesky</code> algorithm has a closed form solution  and is complete after one iteration.  It therefore doesn&#39;t need additional learning strategies such as <code>MaxIter</code>, <code>Converged</code>, etc.</p><p><strong>Example</strong></p><pre><code class="language-none">x, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)
s = SModel(x, y, L2DistLoss())
learn!(s, Sweep(s))</code></pre></div></div></section><h2><a class="nav-anchor" id="LineSearch-1" href="#LineSearch-1">LineSearch</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="SparseRegression.LineSearch" href="#SparseRegression.LineSearch"><code>SparseRegression.LineSearch</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LineSearch(algorithm)</code></pre><p>Use a line search in the <code>update!</code> of <code>algorithm</code>.  Currently, <a href="#SparseRegression.ProxGrad"><code>ProxGrad</code></a>, <a href="#SparseRegression.Fista"><code>Fista</code></a>, and <a href="#SparseRegression.GradientDescent"><code>GradientDescent</code></a> are supported.</p><p><strong>Example</strong></p><pre><code class="language-none">x, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)
s = SModel(x, y, L2DistLoss())
strat = strategy(MaxIter(50), LineSearch(ProxGrad(s)))
learn!(s, strat)</code></pre></div></div></section><footer><hr/><a class="previous" href="../usage/"><span class="direction">Previous</span><span class="title">Usage</span></a></footer></article></body></html>
