var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Readme",
    "title": "Readme",
    "category": "page",
    "text": ""
},

{
    "location": "#TopicModelsVB.jl-1",
    "page": "Readme",
    "title": "TopicModelsVB.jl",
    "category": "section",
    "text": "A Julia Package for Variational Bayesian Topic Modeling.Topic Modeling is concerned with discovering the latent low-dimensional thematic structure within corpora.  Modeling this latent structure is done using either Markov chain Monte Carlo (MCMC) methods, or variational Bayesian (VB) methods.  The former approach is slower, but unbiased.  Given infinite time, MCMC will fit the desired model exactly.  The latter method is faster (often much faster), but biased, since one must approximate distributions in order to ensure tractability.  This package takes the latter approach to topic modeling."
},

{
    "location": "#Dependencies-1",
    "page": "Readme",
    "title": "Dependencies",
    "category": "section",
    "text": "Pkg.add(\"Distributions\")\nPkg.add(\"OpenCL\")"
},

{
    "location": "#Install-1",
    "page": "Readme",
    "title": "Install",
    "category": "section",
    "text": "Pkg.clone(\"git://github.com/esproff/TopicModelsVB.jl.git\")"
},

{
    "location": "#Datasets-1",
    "page": "Readme",
    "title": "Datasets",
    "category": "section",
    "text": "Included in TopicModelsVB.jl are three datasets:National Science Foundation Abstracts 1989 - 2003:128804 documents\n25319 lexiconCiteULike Science Article Database:16980 documents\n8000 lexicon\n5551 usersMacintosh Magazine Article Collection 1984 - 2005:75011 documents\n15113 lexicon"
},

{
    "location": "#Corpus-1",
    "page": "Readme",
    "title": "Corpus",
    "category": "section",
    "text": "Let\'s begin with the Corpus data structure.  The Corpus data structure has been designed for maximum ease-of-use.  Datasets must still be cleaned and put into the appropriate format, but once a dataset is in the proper format and read into a corpus, it can easily be modified to meet the user\'s needs.There are four plaintext files that make up a corpus:docfile\nlexfile\nuserfile\ntitlefileNone of these files are mandatory to read a corpus, and in fact reading no files will result in an empty corpus.  However in order to train a model a docfile will be necessary, since it contains all quantitative data known about the documents.  On the other hand, the lex, user and title files are used solely for interpreting output.The docfile should be a plaintext file containing lines of delimited numerical values.  Each document is a block of lines, the number of which depends on what information is known about the documents.  Since a document is at its essence a list of terms, each document must contain at least one line containing a nonempty list of delimited positive integer values corresponding to the terms of which it is composed.  Any further lines in a document block are optional, however if they are present they must be present for all documents and must come in the following order:terms: A line of delimited positive integers corresponding to the terms which make up the document (this line is mandatory).\ncounts: A line of delimited positive integers, equal in length to the term line, corresponding to the number of times a particular term appears in a document (defaults to ones(length(terms))).\nreaders: A line of delimited positive integers corresponding to those users which have read the document.\nratings: A line of delimited positive integers, equal in length to the readers line, corresponding to the rating each reader gave the document (defaults to ones(length(readers))).\nstamp: A numerical value in the range [-inf, inf] denoting the timestamp of the document.An example of a single doc block from a docfile with all possible lines included:...\n4,10,3,100,57\n1,1,2,1,3\n1,9,10\n1,1,5\n19990112.0\n...The lex and user files are dictionaries mapping positive integers to terms and usernames (resp.).  For example,1    this\n2    is\n3    a\n4    lex\n5    fileA userfile is identitcal to a lexfile, except usernames will appear in place of vocabulary terms.Finally, a titlefile is simply a list of titles, not a dictionary, and is of the form:title1\ntitle2\ntitle3\ntitle4\ntitle5The order of these titles correspond to the order of document blocks in the associated docfile.To read a corpus into TopicModelsVB.jl, use the following function:readcorp(;docfile=\"\", lexfile=\"\", userfile=\"\", titlefile=\"\", delim=\',\', counts=false, readers=false, ratings=false, stamps=false)The file keyword arguments indicate the path where the respective file is located.It is often the case that even once files are correctly formatted and read, the corpus will still contain formatting defects which prevent it from being loaded into a model.  Therefore, before loading a corpus into a model, it is very important that one of the following is run:fixcorp!(corp; kwargs...)orpadcorp!(corp; kwargs...)\nfixcorp!(corp; kwargs...)Padding a corpus before fixing it will ensure that any documents which contain lex or user keys not in the lex or user dictionaries are not removed.  Instead, generic lex and user keys will be added as necessary to the lex and user dictionaries (resp.).Important: A corpus is only a container for documents.  Whenever you load a corpus into a model, a copy of that corpus is made, such that if you modify the original corpus at corpus-level (remove documents, re-order lex keys, etc.), this will not affect any corpus attached to a model.  However!  Since corpora are containers for their documents, modifying an individual document will affect this document in all corpora which contain it.  Be very careful whenever modifying the internals of documents themselves, either manually or through the use of corp! functions. "
},

{
    "location": "#Models-1",
    "page": "Readme",
    "title": "Models",
    "category": "section",
    "text": "The available models are as follows:"
},

{
    "location": "#Models-2",
    "page": "Readme",
    "title": "Models",
    "category": "section",
    "text": "LDA(corp, K)\n# Latent Dirichlet Allocation model with K topics.\n\nfLDA(corp, K)\n# Filtered latent Dirichlet allocation model with K topics.\n\nCTM(corp, K)\n# Correlated topic model with K topics.\n\nfCTM(corp, K)\n# Filtered correlated topic model with K topics.\n\nDTM(corp, K, delta, basemodel)\n# Dynamic topic model with K topics and ∆ = delta.\n\nCTPF(corp, K, basemodel)\n# Collaborative topic Poisson factorization model with K topics."
},

{
    "location": "#GPU-Accelerated-Models-1",
    "page": "Readme",
    "title": "GPU Accelerated Models",
    "category": "section",
    "text": "gpuLDA(corp, K, batchsize)\n# GPU accelerated latent Dirichlet allocation model with K topics.\n\ngpufLDA(corp, K, batchsize)\n# Coming soon...\n\ngpuCTM(corp, K, batchsize)\n# GPU accelerated correlated topic model with K topics.\n\ngpufCTM(corp, K, batchsize)\n# Coming soon...\n\ngpuDTM(corp, K, delta, batchsize, basemodel)\n# Coming soon...\n\ngpuCTPF(corp, K, batchsize, basemodel)\n# GPU accelerated collaborative topic Poisson factorization model with K topics."
},

{
    "location": "#Tutorial-1",
    "page": "Readme",
    "title": "Tutorial",
    "category": "section",
    "text": ""
},

{
    "location": "#LDA-1",
    "page": "Readme",
    "title": "LDA",
    "category": "section",
    "text": "Let\'s begin our tutorial with a simple latent Dirichlet allocation (LDA) model with 9 topics, trained on the first 5000 documents from the NSF corpus.using TopicModelsVB\n\nsrand(1)\n\nnsfcorp = readcorp(:nsf) \n\nnsfcorp.docs = nsfcorp[1:5000]\nfixcorp!(nsfcorp)\n\n# Notice that the post-fix lexicon is smaller after removing all but the first 5000 docs.\n\nnsflda = LDA(nsfcorp, 9)\ntrain!(nsflda, iter=150, tol=0.0) # Setting tol=0.0 will ensure that all 150 iterations are completed.\n                                  # If you don\'t want to watch the ∆elbo, set chkelbo=151.\n# training...\n\nshowtopics(nsflda, cols=9)topic 1         topic 2         topic 3          topic 4        topic 5       topic 6      topic 7          topic 8         topic 9\ndata            research        species          research       research      cell         research         theory          chemistry\nproject         study           research         systems        university    protein      project          problems        research\nresearch        experimental    plant            system         support       cells        data             study           metal\nstudy           high            study            design         students      proteins     study            research        reactions\nearthquake      systems         populations      data           program       gene         economic         equations       chemical\nocean           theoretical     genetic          algorithms     science       plant        important        work            study\nwater           phase           plants           based          scientists    genes        social           investigator    studies\nstudies         flow            evolutionary     control        award         studies      understanding    geometry        program\nmeasurements    physics         population       project        dr            molecular    information      project         organic\nfield           quantum         data             computer       project       research     work             principal       structure\nprovide         materials       dr               performance    scientific    specific     development      algebraic       molecular\ntime            properties      studies          parallel       sciences      function     theory           mathematical    dr\nmodels          temperature     patterns         techniques     conference    system       provide          differential    compounds\nresults         model           relationships    problems       national      study        analysis         groups          surface\nprogram         dynamics        determine        models         projects      important    policy           space           moleculesNow that we\'ve trained our LDA model we can, if we want, take a look at the topic proportions for individual documents.  For instance, document 1 has topic breakdown:nsflda.gamma[1] # = [0.036, 0.030, 189.312, 0.036, 0.049, 0.022, 8.728, 0.027, 0.025]This vector of topic weights suggests that document 1 is mostly about biology, and in fact looking at the document text confirms this observation:showdocs(nsflda, 1) # Could also have done showdocs(nsfcorp, 1). ●●● Doc: 1\n ●●● CRB: Genetic Diversity of Endangered Populations of Mysticete Whales: Mitochondrial DNA and Historical Demography\ncommercial exploitation past hundred years great extinction variation sizes\npopulations prior minimal population size current permit analyses effects \ndiffering levels species distributions life history...On the other hand, some documents will be a combination of topics.  Consider the topic breakdown for document 25:nsflda.gamma[25] # = [11.575, 44.889, 0.0204, 0.036, 0.049, 0.022, 0.020, 66.629, 0.025]\n\nshowdocs(nsflda, 25) ●●● Doc: 25\n ●●● Mathematical Sciences: Nonlinear Partial Differential Equations from Hydrodynamics\nwork project continues mathematical research nonlinear elliptic problems arising perfect\nfluid hydrodynamics emphasis analytical study propagation waves stratified media techniques\nanalysis partial differential equations form basis studies primary goals understand nature \ninternal presence vortex rings arise density stratification due salinity temperature...We see that in this case document 25 appears to be about applications of mathematical physics to ocean currents, which corresponds precisely to a combination of topics 2 and 8, with a smaller but not insignificant weight on topic 1.Furthermore, if we want to, we can also generate artificial corpora by using the gencorp function.  Generating artificial corpora will in turn run the underlying probabilistic graphical model as a generative process in order to produce entirely new collections of documents, let\'s try it out:artifnsfcorp = gencorp(nsflda, 5000, 1e-5) # The third argument governs the amount of Laplace smoothing (defaults to 0.0).\n\nartifnsflda = LDA(artifnsfcorp, 9)\ntrain!(artifnsflda, iter=150, tol=0.0, chkelbo=15)\n\n# training...\n\nshowtopics(artifnsflda, cols=9)topic 1       topic 2          topic 3       topic 4          topic 5         topic 6         topic 7      topic 8         topic 9\ncell          research         research      species          theory          data            chemistry    research        research\nprotein       project          university    plant            problems        project         research     study           systems\ncells         study            students      research         study           research        reactions    systems         design\ngene          data             support       study            research        earthquake      metal        phase           system\nproteins      economic         program       evolutionary     equations       study           chemical     experimental    data\nplant         social           science       genetic          work            studies         organic      flow            algorithms\nstudies       important        scientists    population       project         water           structure    theoretical     based\ngenes         understanding    scientific    plants           investigator    ocean           program      materials       parallel\nresearch      work             award         populations      principal       measurements    study        high            performance\nmolecular     information      sciences      dr               geometry        program         dr           quantum         techniques\nspecific      theory           projects      data             differential    important       molecular    physics         computer\nmechanisms    provide          dr            patterns         mathematical    time            synthesis    properties      problems\nsystem        development      project       relationships    algebraic       models          compounds    temperature     control\nrole          human            national      evolution        methods         seismic         surface      dynamics        project\nstudy         political        provide       variation        analysis        field           studies      proposed        methodsOne thing we notice so far is that despite producing what are clearly coherent topics, many of the top words in each topic are words such as research, study, data, etc.  While such terms would be considered informative in a generic corpus, they are effectively stop words in a corpus composed of science article abstracts.  Such corpus-specific stop words will be missed by most generic stop word lists, and they can be difficult to pinpoint and individually remove prior to training.  Thus let\'s change our model to a filtered latent Dirichlet allocation (fLDA) model.srand(1)\n\nnsfflda = fLDA(nsfcorp, 9)\ntrain!(nsfflda, iter=150, tol=0.0)\n\n# training...\n\nshowtopics(nsfflda, cols=9)topic 1         topic 2         topic 3          topic 4          topic 5        topic 6       topic 7      topic 8         topic 9\nearthquake      flow            species          design           university     cell          economic     theory          chemistry\nocean           theoretical     plant            algorithms       support        protein       social       equations       reactions\nwater           phase           populations      computer         students       cells         theory       geometry        metal\nmeasurements    physics         genetic          performance      program        proteins      policy       algebraic       chemical\nprogram         quantum         plants           parallel         science        gene          human        differential    program\nsoil            properties      evolutionary     processing       award          plant         change       mathematical    organic\nseismic         temperature     population       applications     scientists     genes         political    groups          molecular\nclimate         effects         patterns         networks         scientific     molecular     public       space           compounds\neffects         phenomena       variation        network          sciences       function      examine      mathematics     surface\nglobal          numerical       effects          software         conference     dna           science      finite          properties\nsea             laser           food             computational    national       regulation    decision     solutions       molecules\nsurface         measurements    ecology          efficient        projects       expression    people       spaces          university\nresponse        experiments     environmental    program          engineering    plants        labor        dimensional     reaction\nsolar           award           test             distributed      year           mechanisms    effects      functions       synthesis\nearth           liquid          ecological       power            workshop       membrane      market       questions       complexesWe can now see that many of the most troublesome corpus-specific stop words have been automatically filtered out, while those that remain are those which tend to cluster within their own, more generic, topic."
},

{
    "location": "#CTM-1",
    "page": "Readme",
    "title": "CTM",
    "category": "section",
    "text": "For our final example using the NSF corpus, let\'s upgrade our model to a filtered correlated topic model (fCTM).srand(1)\n\nnsffctm = fCTM(nsfcorp, 9)\ntrain!(nsffctm, iter=150, tol=0.0)\n\n# training...\n\nshowtopics(nsffctm, 20, cols=9)topic 1         topic 2         topic 3          topic 4          topic 5         topic 6       topic 7      topic 8         topic 9\ndata            flow            species          system           university      cell          data         theory          chemistry\nearthquake      numerical       plant            design           support         protein       social       problems        chemical\nocean           theoretical     populations      data             program         cells         economic     geometry        materials\nwater           models          genetic          algorithms       students        plant         theory       investigator    reactions\nmeasurements    model           evolutionary     control          science         proteins      policy       algebraic       properties\nprogram         physics         plants           problems         dr              gene          human        equations       metal\nclimate         theory          population       models           award           molecular     political    groups          surface\nmodels          nonlinear       data             parallel         scientists      genes         models       differential    electron\nseismic         dynamics        dr               computer         scientific      dna           public       space           program\nsoil            experimental    patterns         performance      sciences        system        change       mathematical    molecular\nearth           equations       relationships    model            national        function      model        mathematics     organic\nglobal          particle        evolution        processing       projects        regulation    science      spaces          dr\nsea             phenomena       variation        applications     engineering     plants        people       functions       university\nresponse        quantum         group            network          conference      expression    decision     questions       compounds\ndamage          heat            ecology          networks         year            mechanisms    issues       manifolds       temperature\nsolar           fluid           ecological       approach         researchers     dr            labor        finite          molecules\npacific         particles       forest           software         workshop        membrane      market       dimensional     laser\nice             waves           environmental    efficient        mathematical    genetic       case         properties      reaction\nsurface         problems        food             computational    months          binding       women        group           optical\nsystem          award           experiments      distributed      equipment       cellular      factors      operators       measurementsBecause the topics in the fLDA model were already so well defined, there\'s little room to improve topic coherence by upgrading to the fCTM model, however what\'s most interesting about the CTM and fCTM models is the ability to look at topic correlations.Based on the top 20 terms in each topic, we might tentatively assign the following topic labels:topic 1: Earth Science\ntopic 2: Physics\ntopic 3: Sociobiology\ntopic 4: Computer Science\ntopic 5: Academia\ntopic 6: Microbiology\ntopic 7: Economics\ntopic 8: Mathematics\ntopic 9: ChemistryNow let\'s take a look at the topic-covariance matrix:nsffctm.sigma\n\n# Top 3 off-diagonal positive entries, sorted in descending order:\nnsffctm.sigma[4,8] # 9.532\nnsffctm.sigma[3,6] # 7.362\nnsffctm.sigma[2,9] # 4.531\n\n# Top 3 negative entries, sorted in ascending order:\nnsffctm.sigma[7,9] # -14.627\nnsffctm.sigma[3,8] # -12.464\nnsffctm.sigma[1,8] # -11.775According to the list above, the most closely related topics are topics 4 and 8, which correspond to the Computer Science and Mathematics topics, followed closely by 3 and 6, corresponding to the topics Sociobiology and Microbiology, and then by 2 and 9, corresponding to Physics and Chemistry.As for the most unlikely topic pairings, first are topics 7 and 9, corresponding to Economics and Chemistry, followed closely by topics 1 and 8, corresponding to Sociobiology and Mathematics, and then third are topics 3 and 8, corresponding to Earth Science and Mathematics.Furthermore, as expected, the topic which is least correlated with all other topics is the Academia topic:sum(abs(nsffctm.sigma[:,5])) - nsffctm.sigma[5,5] # Academia topic, absolute off-diagonal covariance 13.403.Note: Both CTM and fCTM will sometimes have to numerically invert ill-conditioned matrices, thus don\'t be alarmed if the ∆elbo periodically goes negative for stretches, it should always right itself in fairly short order."
},

{
    "location": "#DTM-1",
    "page": "Readme",
    "title": "DTM",
    "category": "section",
    "text": "Now that we have covered static topic models, let\'s transition to the dynamic topic model (DTM).  The dynamic topic model discovers the temporal-dynamics of topics which, nevertheless, remain thematically static.  A good example of a topic which is thematically-static, yet exhibits an evolving lexicon, is Computer Storage.  Methods of data storage have evolved rapidly in the last 40 years, evolving from punch cards, to 5-inch floppy disks, to smaller hard disks, to zip drives and cds, to dvds and platter hard drives, and now to flash drives, solid-state drives and cloud storage, all accompanied by the rise and fall of computer companies which manufacture (or at one time manufactured) these products.As an example, let\'s load the Macintosh corpus of articles, drawn from the magazines MacWorld and MacAddict, published between the years 1984 - 2005.  We sample 400 articles randomly from each year, and break time periods into 2 year intervals.import Distributions.sample\n\nsrand(1)\n\nmaccorp = readcorp(:mac)\n\nmaccorp.docs = vcat([sample(filter(doc -> round(doc.stamp / 100) == y, maccorp.docs), 400, replace=false) for y in 1984:2005]...)\n\nfixcorp!(maccorp, b=100, len=10) # Remove words which appear < 100 times and documents of length < 10.\n\nbasemodel = LDA(maccorp, 9)\ntrain!(basemodel, iter=150, chkelbo=151)\n\n# training...\n\nmacdtm = DTM(maccorp, 9, 200, pmodel)\ntrain!(macdtm, iter=10) # This will likely take about an hour on a personal computer.\n                        # Convergence for all other models is worst-case quadratic,\n                        # while DTM convergence is linear or at best super-linear.\n# training...We can look at a particular topic slice by writing:showtopics(macdtm, topics=4, cols=6) ●●● Topic: 4\ntime 1       time 2       time 3         time 4       time 5       time 6\nboard        macintosh    color          board        board        power\nserial       upgrade      system         video        powerbook    macs\nmemory       port         board          ram          color        price\npower        memory       video          upgrade      upgrade      video\nupgrade      board        display        system       display      speed\nport         power        memory         rom          apple        upgrade\nchips        expansion    boards         simms        scsi         performance\nunit         unit         radius         macintosh    video        apple\nports        digital      upgrade        ethernet     monitor      fast\nchip         connect      power          classic      power        monitors\nexpansion    radius       ram            apple        quadra       powerbook\ndigital      device       monitor        math         memory       radius\nboards       devices      accelerator    rasterops    designed     small\nplug         boards       device         scsi         processor    cpu\nadapter      chip         supermac       digital      macintosh    faster\n\ntime 7         time 8         time 9         time 10      time 11       \npower          g3             usb            g4           ipod          \nram            usb            firewire       power        mini          \nspeed          imac           g4             usb          power         \nprocessor      ram            ram            firewire     usb           \nstandard       apple          apple          apple        apple         \nfast           modem          power          ram          g5            \ndrive          upgrade        palm           imac         firewire      \nperformance    power          g3             powerbook    g4            \nkeyboard       pci            refurbished    drive        ram           \nfaster         speed          powerbook      airport      models        \nupgrade        drive          hardware       port         display       \nmemory         powerbook      machine        processor    memory        \napple          internal       port           models       hard_drive    \nslots          serial         imac           memory       speed         \npowerbook      performance    device         pci          port  or a particular time slice, by writing:showtopics(macdtm, times=11, cols=9) ●●● Time: 11\n ●●● Span: 200405.0 - 200512.0\ntopic 1    topic 2      topic 3     topic 4       topic 5        topic 6      topic 7     topic 8       topic 9\nfile       color        system      ipod          demo           click        apple       drive         mac\nselect     image        files       mini          manager        video        site        express       mouse\nfolder     images       disk        power         future         software     price       backup        cover\nset        photo        osx         usb           director       music        web         drives        fax\nopen       photoshop    utility     apple         usa            audio        products    buffer        software\nmenu       print        install     g5            network        good         contest     prices        ea\nchoose     light        finder      firewire      shareware      game         smart       subject       pad\ntext       photos       user        g4            charts         itunes       year        data          laserwriter\nbutton     printer      terminal    ram           accounts       play         computer    warranty      kensington\nfind       mode         run         models        editor         time         world       mac           stylewriter\nwindow     digital      folders     display       advertising    effects      phone       disk          turbo\ntype       quality      network     memory        entries        pro          group       retrospect    apple\ncreate     elements     classic     hard_drive    production     dvd          product     orders        printer\npress      lens         desktop     speed         california     makes        service     notice        modem\nline       printing     windows     port          marketing      interface    people      shipping      extAs you may have noticed, the dynamic topic model is extremely computationally intensive, hopefully GPGPU support will ameliorate this problem to at least some degree.  However it is likely that running the DTM model on an industry-sized dataset will always require more computational power than can be provided by your standard personal computer.Important: Beware that the DTM algorithm is still a bit buggy, an overhaul of the algorithm itself will likely come alongside the GPU accelerated version."
},

{
    "location": "#CTPF-1",
    "page": "Readme",
    "title": "CTPF",
    "category": "section",
    "text": "For our final model, we take a look at the collaborative topic Poisson factorization (CTPF) model.  CTPF is a collaborative filtering topic model which uses the latent thematic structure of documents to improve the quality of document recommendations beyond what would be achievable using just the document-user matrix.  This blending of thematic structure with known user prefrences not only improves recommendation accuracy, but also mitigates the cold-start problem of recommending to users never-before-seen documents.  As an example, let\'s load the CiteULike dataset into a corpus and then randomly remove a single reader from each of the documents.import Distributions.sample\n\nsrand(1)\n\nciteucorp = readcorp(:citeu)\n\ntestukeys = Int[]\nfor doc in citeucorp\n    index = sample(1:length(doc.readers), 1)[1]\n    push!(testukeys, doc.readers[index])\n    deleteat!(doc.readers, index)\n    deleteat!(doc.ratings, index)\nendImportant: We refrain from fixing our corpus in this case, first because the CiteULike dataset is pre-packaged and thus pre-fixed, but more importantly, because removing user keys from documents and then fixing a corpus may result in a re-ordering of its user dictionary, which would in turn invalidate our test set.After training, we will evaluate model quality by measuring our model\'s success at imputing the correct user back into each of the document libraries.It\'s also worth noting that after removing a single reader from each document, 158 of the documents now have 0 readers:sum([isempty(doc.readers) for doc in citeucorp]) # = 158Fortunately, since CTPF can if need be depend entirely on thematic structure when making recommendations, this poses no problem for the model.Now that we have set up our experiment, we instantiate and train a CTPF model on our corpus.  Furthermore, since we\'re not interested in the interpretability of the topics, we\'ll instantiate our model with a larger than usual number of topics (K=30), and then run it for a relatively short number of iterations (iter=20).srand(1)\n\nciteuctpf = CTPF(citeucorp, 30) # Note: If no \'pmodel\' is entered then parameters will be initialized at random.\ntrain!(citeuctpf, iter=20)\n\n# training...Finally, we evaluate the accuracy of our model against the test set, where baseline for mean accuracy is 0.5.acc = Float64[]\nfor (d, u) in enumerate(testukeys)\n    rank = findin(citeuctpf.drecs[d], u)[1]\n    nrlen = length(citeuctpf.drecs[d])\n    push!(acc, (nrlen - rank) / (nrlen - 1))\nend\n\n@show mean(acc) # mean(acc) = 0.908Not bad, but let\'s see if we can\'t improve our accuracy at least a percentage point or two by priming our CTPF model with a 100 iteration LDA model.In the interest of time, let\'s use the GPU accelerated verions of LDA and CTPF:srand(1)\n\nbasemodel = gpuLDA(citeucorp, 30)\ntrain!(basemodel, iter=100, chkelbo=101)\n\n# training...\n\nciteuctpf = gpuCTPF(citeucorp, 30, basemodel)\ntrain!(citeuctpf, iter=20, chkelbo=21)\n\n# training...Again we evaluate the accuracy of our model against the test set:acc = Float64[]\nfor (d, u) in enumerate(testukeys)\n    rank = findin(citeuctpf.drecs[d], u)[1]\n    nrlen = length(citeuctpf.drecs[d])\n    push!(acc, (nrlen - rank) / (nrlen - 1))\nend\n\n@show mean(acc) # mean(acc) = 0.920We can see that, on average, our model ranks the true hidden reader in the top 8% of all non-readers for each document.Let\'s also take a look at the top recommendations for a particular document(s):testukeys[1] # = 216\nacc[1] # = 0.945\n\nshowdrecs(citeuctpf, 1, 307, cols=1) ●●● Doc: 1\n ●●● The metabolic world of Escherichia coli is not small\n...\n303. #user5159\n304. #user5486\n305. #user261\n306. #user4999\n307. #user216as well as those for a particular user(s):showurecs(citeuctpf, 216, 1745) ●●● User: 216\n...\n1741. {Characterizing gene sets with FuncAssociate}\n1742. Network Data and Measurement\n1743. Analysis of genomic context: prediction of functional associations from conserved bidirectionally transcribed gene pairs.\n1744. The public road to high-quality curated biological pathways\n1745. The metabolic world of Escherichia coli is not smallWe can also take a more holistic and informal approach to evaluating model quality.Since large heterogenous libraries make the qualitative assessment of recommendations difficult, let\'s search for a user with a modestly sized relatively focused library: showlibs(citeuctpf, 1741) ●●● User: 1741\n • Region-Based Memory Management\n • A Syntactic Approach to Type Soundness\n • Imperative Functional Programming\n • The essence of functional programming\n • Representing monads\n • The marriage of effects and monads\n • A Taste of Linear Logic\n • Monad transformers and modular interpreters\n • Comprehending Monads\n • Monads for functional programming\n • Building interpreters by composing monads\n • Typed memory management via static capabilities\n • Computational Lambda-Calculus and Monads\n • Why functional programming matters\n • Tackling the Awkward Squad: monadic input/output, concurrency, exceptions, and foreign-language calls in Haskell\n • Notions of Computation and Monads\n • Recursion schemes from comonads\n • There and back again: arrows for invertible programming\n • Composing monads using coproducts\n • An Introduction to Category Theory, Category Theory Monads, and Their Relationship to Functional ProgrammingThe 20 articles in user 1741\'s library suggest that he or she is interested in the interplay of foundational mathematics and functional programming.  Now compare this with the top 20 recommendations made by our model:showurecs(citeuctpf, 1741, 20) ●●● User: 1741\n1.  Sets for Mathematics\n2.  On Understanding Types, Data Abstraction, and Polymorphism\n3.  Can programming be liberated from the von {N}eumann style? {A} functional style and its algebra of programs\n4.  Dynamic Logic\n5.  Principles of programming with complex objects and collection types\n6.  Functional programming with bananas, lenses, envelopes and barbed wire\n7.  Haskell\'s overlooked object system\n8.  Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints\n9.  Lectures on the Curry-Howard isomorphism\n10. Parsing expression grammars: a recognition-based syntactic foundation\n11. Type Classes with Functional Dependencies\n12. Macros as multi-stage computations: type-safe, generative, binding macros in MacroML\n13. Types, abstraction and parametric polymorphism\n14. Functional pearl: implicit configurations--or, type classes reflect the values of types\n15. The Zipper\n16. Monadic Parsing in Haskell\n17. Ownership types for safe programming: preventing data races and deadlocks\n18. Monadic Parser Combinators\n19. The essence of compiling with continuations\n20. The {Calculus of Constructions}"
},

{
    "location": "#GPU-Acceleration-1",
    "page": "Readme",
    "title": "GPU Acceleration",
    "category": "section",
    "text": "GPU accelerating your model runs its performance bottlenecks on the GPU rather than the CPU.Currently the LDA, CTM and CTPF models are supported, however GPU accelerated versions of the remaining three models are in the works.  There\'s no reason to instantiate the GPU models directly, instead you can simply instantiate the normal version of a supported model, and then use the @gpu macro to train it on the GPU:nsfcorp = readcorp(:nsf)\n\nnsflda = LDA(nsfcorp, 16)\n@time @gpu train!(nsflda, iter=150, chkelbo=151) # Let\'s time it as well to get an exact benchmark. \n\n# training...\n\n# 238.117185 seconds (180.46 M allocations: 11.619 GB, 4.39% gc time)\n# On a 2.5 GHz Intel Core i5 2012 Macbook Pro with 4GB of RAM and an Intel HD Graphics 4000 1536 MB GPU.This algorithm just crunched through a 16 topic 128,804 document topic model in under 4 minutes.Important: Notice that we didn\'t check the ELBO at all during training.  While you can check the ELBO if you wish, it\'s recommended that you do so infrequently since checking the ELBO for GPU models requires expensive transfers between GPU and CPU memory.Here is the benchmark of our above model against the equivalent NSF LDA model run on the CPU: (Image: GPU Benchmark)As we can see, the GPU LDA model is approximatey 1.35 orders of magnitude faster than the equivalent CPU LDA model.It\'s often the case that one does not have sufficient VRAM to hold the entire GPU model at one time.  Thus we provide the option of batching the GPU model in order to fit much larger models than would otherwise be possible:citeucorp = readcorp(:citeu)\n\nciteuctm = CTM(citeucorp, 7)\n@gpu 4250 train!(citeuctm, iter=150, chkelbo=25) # batchsize = 4250 documents.\n\n# training...It\'s important to understand that GPGPU is still the wild west of computer programming.  The performance of batched models depends on many architecture dependent factors, including but not limited to the memory, the GPU, the manufacturer, the type of computer, what other applications are running, whether a display is connected, etc.While non-batched models will usually be the fasted (for those GPUs which can handle them), it\'s not necessarily the case that reducing the batch size will result in a degredation in performance.  Thus it\'s always a good idea to experiment with different batch sizes, to see which sizes work best for your computer.Important: If Julia crashes or throws an error when trying to run one of your models on the GPU, your best course of action is to reduce the batch size and retrain your model.Finally, expect your computer to lag when training on your GPU, since you\'re effectively siphoning off its rendering resources to fit your model."
},

{
    "location": "#Types-1",
    "page": "Readme",
    "title": "Types",
    "category": "section",
    "text": "VectorList{T}\n# Array{Array{T,1},1}\n\nMatrixList{T}\n# Array{Array{T,2},1}\n\nDocument(terms::Vector{Integer}; counts::Vector{Integer}=ones(length(terms)), readers::Vector{Integer}=Int[], ratings::Vector{Integer}=ones(length(readers)), stamp::Real=-Inf, title::UTF8String=\"\")\n# FIELDNAMES:\n# terms::Vector{Int}\n# counts::Vector{Int}\n# readers::Vector{Int}\n# ratings::Vector{Int}\n# stamp::Float64\n# title::UTF8String\n\nCorpus(;docs::Vector{Document}=Document[], lex::Union{Vector{UTF8String}, Dict{Integer, UTF8String}}=[], users::Union{Vector{UTF8String}, Dict{Integer, UTF8String}}=[])\n# FIELDNAMES:\n# docs::Vector{Document}\n# lex::Dict{Int, UTF8String}\n# users::Dict{Int, UTF8String}\n\nTopicModel\n# abstract\n\nGPUTopicModel <: TopicModel\n# abstract\n\nBaseTopicModel\n# Union{LDA, fLDA, CTM, fCTM, gpuLDA, gpufLDA, gpuCTM, gpufCTM}\n\nAbstractLDA\n# Union{LDA, gpuLDA}\n\nAbstractfLDA\n# Union{fLDA, gpufLDA}\n\nAbstractCTM\n# Union{CTM, gpuCTM}\n\nAbstractfCTM\n# Union{fCTM, gpufCTM}\n\nAbstractDTM\n# Union{DTM, gpuDTM}\n\nAbstractCTPF\n# Union{CTPF, gpuCTPF}\n\nLDA(corp::Corpus, K::Integer) <: TopicModel\n# Latent Dirichlet allocation\n# \'K\' - number of topics.\n\nfLDA(corp::Corpus, K::Integer) <: TopicModel\n# Filtered latent Dirichlet allocation\n\nCTM(corp::Corpus, K::Integer) <: TopicModel\n# Correlated topic model\n\nfCTM(corp::Corpus, K::Integer) <: TopicModel\n# Filtered correlated topic model\n\nDTM(corp::Corpus, K::Integer, delta::Real, basemodel::BaseTopicModel) <: TopicModel\n# Dynamic topic model\n# \'delta\'     - time-interval size.\n# \'basemodel\' - pre-trained model of type BaseTopicModel (optional).\n\nCTPF(corp::Corpus, K::Integer, basemodel::BaseTopicModel) <: GPUTopicModel\n# Collaborative topic Poisson factorization\n# \'basemodel\' - pre-trained model of type BaseTopicModel (optional).\n\ngpuLDA(corp::Corpus, K::Integer, batchsize::Integer) <: GPUTopicModel\n# GPU accelerated latent Dirichlet allocation\n# \'batchsize\' defaults to \'length(corp)\'.\n\ngpufLDA(corp::Corpus, K::Integer, batchsize::Integer) <: GPUTopicModel\n# GPU accelerated filtered latent Dirichlet allocation\n# \'batchsize\' defaults to \'length(corp)\'.\n\ngpuCTM(corp::Corpus, K::Integer, batchsize::Integer) <: GPUTopicModel\n# GPU accelerated correlated topic model\n# \'batchsize\' defaults to \'length(corp)\'.\n\ngpufCTM(corp::Corpus, K::Integer, batchsize::Integer) <: GPUTopicModel\n# GPU accelerated filtered correlated topic model\n# \'batchsize\' defaults to \'length(corp)\'.\n\ngpuDTM(corp::Corpus, K::Integer, delta::Real, batchsize::Integer, basemodel::BaseTopicModel) <: GPUTopicModel\n# GPU accelerated dynamic topic model\n# \'delta\'     - time-interval size.\n# \'batchsize\' defaults to \'length(corp)\'.\n# \'basemodel\' - pre-trained model of type BaseTopicModel (optional).\n\ngpuCTPF(corp::Corpus, K::Integer, batchsize::Integer, basemodel::BaseTopicModel) <: GPUTopicModel\n# GPU accelerated collaborative topic Poission factorization\n# \'batchsize\' defaults to \'length(corp)\'.\n# \'basemodel\' - pre-trained model of type BaseTopicModel (optional)."
},

{
    "location": "#Functions-1",
    "page": "Readme",
    "title": "Functions",
    "category": "section",
    "text": ""
},

{
    "location": "#Generic-Functions-1",
    "page": "Readme",
    "title": "Generic Functions",
    "category": "section",
    "text": "isnegative(x::Union{Real, Array{Real}})\n# Take Real or Array{Real} and return Bool or Array{Bool} (resp.).\n\nispositive(x::Union{Real, Array{Real}})\n# Take Real or Array{Real} and return Bool or Array{Bool} (resp.).\n\nlogsumexp(x::Array{Real})\n# Overflow safe log(sum(exp(x))).\n\naddlogistic(x::Array{Real}, region::Integer)\n# Overflow safe additive logistic function.\n# \'region\' is optional, across columns: \'region\' = 1, rows: \'region\' = 2.\n\npartition(xs::Union{Vector, UnitRange}, n::Integer)\n# \'n\' must be positive.\n# Return VectorList containing contiguous portions of xs of length n (includes remainder).\n# e.g. partition([1,-7.1,\"HI\",5,5], 2) == Vector[[1,-7.1],[\"HI\",5],[5]]"
},

{
    "location": "#Document/Corpus-Functions-1",
    "page": "Readme",
    "title": "Document/Corpus Functions",
    "category": "section",
    "text": "checkdoc(doc::Document)\n# Verify that all Document fields have legal values.\n\ncheckcorp(corp::Corpus)\n# Verify that all Corpus fields have legal values.\n\nreadcorp(;docfile::AbstractString=\"\", lexfile::AbstractString=\"\", userfile::AbstractString=\"\", titlefile::AbstractString=\"\", delim::Char=\',\', counts::Bool=false, readers::Bool=false, ratings::Bool=false, stamps::Bool=false)\n# Read corpus from plaintext files.\n# readcorp(:nsf)   - National Science Foundation corpus.\n# readcorp(:citeu) - CiteULike corpus.\n# readcorp(:mac)   - Macintosh Magazine corpus.\n# readcorp(:cmag)  - Full Computer Magazine corpus.\n\nwritecorp(corp::Corpus; docfile::AbstractString=\"\", lexfile::AbstractString=\"\", userfile::AbstractString=\"\", titlefile::AbstractString=\"\", delim::Char=\',\', counts::Bool=false, readers::Bool=false, ratings::Bool=false, stamps::Bool=false)\n# Write corpus to plaintext files.\n\nabridgecorp!(corp::Corpus; stop::Bool=false, order::Bool=true, abr::Integer=1)\n# Abridge corpus.\n# If stop = true, stop words are removed.\n# If order = false, order is ignored and multiple seperate occurrences of words are stacked and the associated counts increased.\n# All terms which appear < abr times are removed from documents.\n\ntrimcorp!(corp::Corpus; lex::Bool=true, terms::Bool=true, users::Bool=true, readers::Bool=true)\n# Those values which appear in the indicated fields of documents, yet don\'t appear in the corpus dictionaries, are removed.\n\ncompactcorp!(corp::Corpus; lex::Bool=true, users::Bool=true, alphabetize::Bool=true)\n# Compact a corpus by relabeling lex and/or userkeys so that they form a unit range.\n# If alphabetize=true the lex and/or user dictionaries are alphabetized.\n\npadcorp!(corp::Corpus; lex::Bool=true, users::Bool=true)\n# Pad a corpus by entering generic values for lex and/or userkeys which appear in documents but not in the lex/user dictionaries.\n\ncullcorp!(corp::Corpus; terms::Bool=false, readers::Bool=false, len::Integer=1)\n# Culls the corpus of documents which contain lex and/or user keys in a document\'s terms/readers (resp.) fields yet don\'t appear in the corpus dictionaries.\n# All documents of length < len are removed.\n\nfixcorp!(corp::Corpus; lex::Bool=true, terms::Bool=true, users::Bool=true, readers::Bool=true, stop::Bool=false, order::Bool=true, abr::Int=1, len::Int=1, alphabetize::Bool=true)\n# Fixes a corp by running the following four functions in order:\n# abridgecorp!(corp, stop=stop, order=order, abr=abr)\n# trimcorp!(corp, lex=lex, terms=terms, users=users, readers=readers)\n# cullcorp!(corp, len=len)	\n# compactcorp!(corp, lex=lex, users=users, alphabetize=alphabetize)\n\nshowdocs(corp::Corpus, docs::Union{Document, Vector{Document}, Integer, Vector{Integer}, UnitRange{Integer}})\n# Display the text and title of a document(s).\n\ngetlex(corp::Corpus)\n# Collect sorted values from the lex dictionary.\n\ngetusers(corp::Corpus)\n# Collect sorted values from the user dictionary."
},

{
    "location": "#Model-Functions-1",
    "page": "Readme",
    "title": "Model Functions",
    "category": "section",
    "text": "showdocs(model::TopicModel, docs::Union{Document, Vector{Document}, Int, Vector{Int}, UnitRange{Int}})\n\nfixmodel!(model::TopicModel; check::Bool=true)\n# If \'check == true\', verify the legality of the model\'s primary data.\n# Align any auxiliary parameters with their associated parent parameters.\n\ntrain!(model::BaseTopicModel; iter::Integer=150, tol::Real=1.0, niter::Integer=1000, ntol::Real=1/model.K^2, viter::Integer=10, vtol::Real=1/model.K^2, chkelbo::Integer=1)\n# Train one of the following models: LDA, fLDA, CTM, fCTM.\n# \'iter\'    - maximum number of iterations through the corpus.\n# \'tol\'     - absolute tolerance for ∆elbo as a stopping criterion.\n# \'niter\'   - maximum number of iterations for Newton\'s and interior-point Newton\'s methods.\n# \'ntol\'    - tolerance for change in function value as a stopping criterion for Newton\'s and interior-point Newton\'s methods.\n# \'viter\'   - maximum number of iterations for optimizing variational parameters (at the document level).\n# \'vtol\'    - tolerance for change in variational parameter values as stopping criterion.\n# \'chkelbo\' - number of iterations between ∆elbo checks (for both evaluation and convergence checking).\n\ntrain!(dtm::AbstractDTM; iter::Integer=150, tol::Real=1.0, niter::Integer=1000, ntol::Real=1/dtm.K^2, cgiter::Integer=10, cgtol::Real=1/dtm.T^2, chkelbo::Integer=1)\n# Train DTM.\n# \'cgiter\' - maximum number of iterations for the Polak-Ribière conjugate gradient method.\n# \'cgtol\'  - tolerance for change in function value as a stopping criterion for the Polak-Ribière conjugate gradient method.\n\ntrain!(ctpf::AbstractCTPF; iter::Integer=150, tol::Real=1.0, viter::Integer=10, vtol::Real=1/ctpf.K^2, chkelbo::Integer=1)\n# Train CTPF.\n\n@gpu train!(model; kwargs...)\n# Train model on GPU.\n\ngendoc(model::BaseTopicModel, a::Real=0.0)\n# Generate a generic document from model parameters by running the associated graphical model as a generative process.\n# \'a\' - amount of Laplace smoothing to apply to the topic-term distributions (\'a\' must be nonnegative).\n\ngencorp(model::BaseTopicModel, corpsize::Int, a::Real=0.0)\n# Generate a generic corpus of size \'corpsize\' from model parameters.\n\nshowtopics(model::TopicModel, N::Integer=min(15, model.V); topics::Union{Integer, Vector{Integer}}=collect(1:model.K), cols::Integer=4)\n# Display the top \'N\' words for each topic in \'topics\', defaults to 4 columns per line.\n\nshowtopics(dtm::AbstractDTM, N::Integer=min(15, dtm.V); topics::Union{Integer, Vector{Integer}}=collect(1:dtm.K), times::Union{Integer, Vector{Integer}}=collect(1:dtm.T), cols::Integer=4)\n# Display the top \'N\' words for each topic in \'topics\' and each time interval in \'times\', defaults to 4 columns per line.\n\nshowlibs(ctpf::AbstractCTPF, users::Union{Integer, Vector{Integer}})\n# Show the document(s) in a user\'s library.\n\nshowdrecs(ctpf::AbstractCTPF, docs::Union{Integer, Vector{Integer}}, U::Integer=min(16, ctpf.U); cols::Integer=4)\n# Show the top \'U\' user recommendations for a document(s), defaults to 4 columns per line.\n\nshowurecs(ctpf::AbstractCTPF, users::Union{Integer, Vector{Integer}}, M::Integer=min(10, ctpf.M); cols::Integer=1)\n# Show the top \'M\' document recommendations for a user(s), defaults to 1 column per line.\n# If a document has no title, the document\'s index in the corpus will be shown instead."
},

{
    "location": "#Bibliography-1",
    "page": "Readme",
    "title": "Bibliography",
    "category": "section",
    "text": "Latent Dirichlet Allocation (2003); Blei, Ng, Jordan. pdf\nFiltered Latent Dirichlet Allocation: Variational Bayes Algorithm (2016); Proffitt. pdf\nCorrelated Topic Models (2006); Blei, Lafferty. pdf\nDynamic Topic Models (2006); Blei, Lafferty. pdf\nContent-based Recommendations with Poisson Factorization (2014); Gopalan, Charlin, Blei. pdf\nNumerical Optimization (2006); Nocedal, Wright. Amazon\nMachine Learning: A Probabilistic Perspective (2012); Murphy. Amazon\nOpenCL in Action: How to Accelerate Graphics and Computation (2011); Scarpino. Amazon"
},

]}
