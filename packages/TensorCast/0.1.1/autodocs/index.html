<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · TensorCast.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>TensorCast.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@assertsize" href="#TensorCast.@assertsize"><code>TensorCast.@assertsize</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@assertsize cond str</code></pre><p>Like <code>@assert</code>, but prints both the given string and the condition. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@cast" href="#TensorCast.@cast"><code>TensorCast.@cast</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@cast Z[i,j,...] := f(A[j,k,...])  options</code></pre><p>Macro for broadcasting, reshaping, and slicing of arrays in index notation. Understands the following things: </p><ul><li><code>A[i,j,k]</code> is a three-tensor with these indices.</li><li><code>B[(i,j),k]</code> is the same thing, reshaped to a matrix. Its first axis (the bracket) is indexed  by <code>n = i + (j-1) * N</code> where <code>i ∈ 1:N</code>. This may also be written <code>B[i\j,k]</code>.</li><li><code>C[k][i,j]</code> is a vector of matrices.</li><li><code>D[j,k]{i}</code> is an ordinary matrix of <code>SVector</code>s, which may be reinterpreted from <code>A[i,j,k]</code>.</li><li><code>E[i,_,k]</code> has two nontrivial dimensions, and <code>size(E,2)==1</code>. On the right hand side  (or when writing to an existing array) you may also write <code>E[i,3,k]</code> meaning <code>view(E, :,3,:)</code>, or <code>E[i,$c,j]</code> to use a variable <code>c</code>. Fixing inner indices, like <code>C[k][i,_]</code>, is not allowed.</li><li><code>F[i,-j,k]</code> means <code>reverse(F, dims=2)</code>. </li></ul><p>The left and right hand sides must have all the same indices.  See <code>@reduce</code> for a related macro which can sum over things. </p><p>If several tensors appear on the right hand side, then this represents a broadcasting operation,  and the necessary re-orientations of axes are automatically inserted. </p><p>The following actions are possible:</p><ul><li><code>=</code> writes into an existing array.</li><li><code>:=</code> creates a new object... which may or may not be a view of the input:</li><li><code>==</code> insists on a view of the old object (error if impossible), and <code>|=</code> insists on a copy. </li></ul><p>Options can be specified at the end (if several, separated by <code>,</code> i.e. <code>options::Tuple</code>)</p><ul><li><code>i:3</code> supplies the range of index <code>i</code>. Variables <code>j:rangej</code> and functions <code>k:length(K)</code> are allowed. </li><li><code>assert</code> or <code>!</code> will turn on explicit dimension checks.</li><li><code>cat</code> will glue slices by things like <code>hcat(A...)</code> instead of <code>reduce(hcat, A)</code>, and <code>lazy</code> will instead make a <code>VectorOfArrays</code> container. </li><li><code>strided</code> will place <code>@strided</code> in front of broadcasting operations,  and use <code>@strided permutedims(A, ...)</code> instead of <code>PermutedDimsArray(A, ...)</code>. </li></ul><p>Static slices <code>D[j,k]{i}</code> need <code>using StaticArrays</code>, and to create them you should give all  slice dimensions explicitly. You may write <code>D[k]{i:2,j:2}</code> to specify <code>Size(2,2)</code> slices.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@cast!" href="#TensorCast.@cast!"><code>TensorCast.@cast!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@cast! Z[i...] := A[j...] opt</code></pre><p>Variant of <code>@cast</code> which effectively runs <code>@check!()</code> on each tensor.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@check!" href="#TensorCast.@check!"><code>TensorCast.@check!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@check!(A[i, j, μ, ν])</code></pre><p>Adds <code>A</code> to the store of known tensors, and records that it expects indices <code>i,j,μ,ν</code>. If it is already in the store, then instead this checks whether the present indices differ  from the saved ones. Only the first letter is examined: <code>α</code> and <code>α2</code> are similar, as are nearby  letters <code>β</code>, <code>γ3</code>. More complicated indices like <code>Z[(i,j), -k, _, 3]</code> will be ignored.  This happens while parsing your source code, there is zero run-time penalty. Returns <code>A</code>.</p><p>In addition, with <code>size=true</code> option, it can insert size checks to be performed at run-time, by returning <code>check!(A, stuff)</code>.  At the first occurrance this saves <code>i: =&gt; size(A,1)</code> etc., and on subsequent uses of  the same index (even on different tensors) it gives an error if the sizes do not match.  Here the whole index is used: <code>α</code>, <code>β</code> and <code>β2</code> may have different ranges.  This will need to look up indices in a dictionary, which takes ... 50ns, really? </p><pre><code class="language-none">@check! B[i,j] C[μ,ν] D[i] E[j]</code></pre><p>Checks several tensors, returns nothing. </p><pre><code class="language-none">@check!  alpha=true  tol=3  size=false  throw=false  info  empty</code></pre><p>Controls options for <code>@check!</code> and related macros (<code>@shape!</code>, <code>@reduce!</code>, <code>@einsum!</code> etc).  These are the default settings:</p><ul><li><code>alpha=true</code> turns on the parse-time checking, based on index letters.</li><li><code>tol=3</code> sets how close together letters must be: <code>B[j,k]</code> is not an error but <code>B[a,b]</code> will be. </li><li><code>size=false</code> turns off run-time size checking.</li><li><code>throw=false</code> means that errors are given using <code>@error</code>, without interrupting your program.</li><li><code>empty</code> deletes all saved letters and sizes – there is one global store for each, for now.</li><li><code>info</code> prints what&#39;s currently saved.</li></ul><pre><code class="language-none">@cast! B[i,j] := D[i] * E[j]
@reduce! B[i,j] := sum(μ,ν) A[i,j,μ,ν] / C[μ,ν]

@einsum!  B[i,j] := D[i] * E[j]
@vielsum! B[i,j] := D[i] * E[j]
@tensor!  B[i,j] := D[i] * E[j]</code></pre><p>Versions of the macros from this package, and from Einsum.jl and TensorOperations.jl,  which call <code>@check!</code> on each of their tensors, before proceeding as normal. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@einsum!" href="#TensorCast.@einsum!"><code>TensorCast.@einsum!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@einsum! A[i,j] := B[i,k] * C[k,j]</code></pre><p>Variant of <code>@einsum</code> from package Einsum.jl,  equivalent to wrapping every tensor with <code>@check!()</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@pretty" href="#TensorCast.@pretty"><code>TensorCast.@pretty</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@pretty @shape A[...] := B[...]</code></pre><p>Prints an approximately equivalent expression with the macro expanded. Compared to <code>@macroexpand1</code>, generated symbols are replaced with animal names, comments are deleted, module names are removed from functions, and the final expression is fed to <code>println()</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@reduce" href="#TensorCast.@reduce"><code>TensorCast.@reduce</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@reduce A[i] := sum(j,k) B[i,j,k]             # A = vec(sum(B, dims=(2,3))) 
@reduce A[i] := prod(j) B[i] + ε * C[i,j]     # A = vec(prod(B .+ ε .* C, dims=2))
@reduce A[i] = sum(j) exp( C[i,j] / D[j] )    # sum!(A, exp.(C ./ D&#39;) )</code></pre><p>Tensor reduction macro:</p><ul><li>The reduction function can be anything which works like <code>sum(B, dims=(1,3))</code>,  for instance <code>prod</code> and <code>maximum</code> and <code>Statistics.mean</code>. </li><li>In-place operations <code>Z[j] = sum(...</code> will construct the banged version of the given function&#39;s name,  which must work like <code>sum!(Z, A)</code>.</li><li>The tensors can be anything that <code>@cast</code> understands, including gluing of slices <code>B[i,k][j]</code>  and reshaping <code>B[i\j,k]</code>. </li><li>Index ranges may be given afterwards (as for <code>@cast</code>) or inside the reduction <code>sum(i:3, k:4)</code>. </li><li>All indices appearing on the right must appear either within <code>sum(...)</code> etc, or on the left. </li></ul><pre><code class="language-none">F = @reduce sum(i,j)  B[i] + γ * D[j]         # sum(B .+ γ .* D&#39;)
@reduce G[] := sum(i,j)  B[i] + γ * D[j]      # F == G[]</code></pre><p>Complete reduction to a scalar output <code>F</code>, or a zero-dim array <code>G</code>. </p><pre><code class="language-none">@reduce Z[k] := sum(i,j) A[i] * B[j] * C[k]  lazy, i:N, j:N, k:N</code></pre><p>The option <code>lazy</code> replaces the broadcast expression with a <code>BroadcastArray</code>,  to avoid <code>materialize</code>ing the entire array (here size <code>N^3</code>) before summing. </p><p>The option <code>strided</code> will place <code>@strided</code> in front of the broadcasting operation.  You need <code>using Strided</code> for this to work. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@reduce!" href="#TensorCast.@reduce!"><code>TensorCast.@reduce!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@reduce! Z[j] := sum(i,k) A[i,j,k]</code></pre><p>Variant of <code>@reduce</code> which effectively runs <code>@check!()</code> on each tensor.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@tensor!" href="#TensorCast.@tensor!"><code>TensorCast.@tensor!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@tensor! A[i,j] := B[i,k] * C[k,j]</code></pre><p>Variant of <code>@tensor</code> from package TensorOperations.jl,  equivalent to wrapping every tensor with <code>@check!()</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.@vielsum!" href="#TensorCast.@vielsum!"><code>TensorCast.@vielsum!</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@vielsum! A[i,j] := B[i,k] * C[k,j]</code></pre><p>Variant of <code>@vielsum</code> from package Einsum.jl,  equivalent to wrapping every tensor with <code>@check!()</code>.</p></div></div></section><pre><code class="language-none">TensorCast.CheckOptions</code></pre><pre><code class="language-none">TensorCast.Reverse</code></pre><pre><code class="language-none">TensorCast.Shuffle</code></pre><pre><code class="language-none">TensorCast.SizeDict</code></pre><pre><code class="language-none">TensorCast.StaticArrays</code></pre><pre><code class="language-none">TensorCast.TensorCast</code></pre><pre><code class="language-none">TensorCast.V</code></pre><pre><code class="language-none">TensorCast.__init__</code></pre><pre><code class="language-none">TensorCast._check!</code></pre><pre><code class="language-none">TensorCast._einsum!</code></pre><pre><code class="language-none">TensorCast._macro</code></pre><pre><code class="language-none">TensorCast._tensor!</code></pre><pre><code class="language-none">TensorCast.cat_glue</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.check!" href="#TensorCast.check!"><code>TensorCast.check!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">check!(A, (:i,:j), &quot;A[i,j]&quot;, (mod=..., src=...))</code></pre><p>Performs run-time size checking, on behalf of the <code>@check!</code> macro, returns <code>A</code>.  The string and tuple are just for the error message. </p></div></div></section><pre><code class="language-none">TensorCast.check_err</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.check_one" href="#TensorCast.check_one"><code>TensorCast.check_one</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">check_one(A[i,j,k], (mod=Module, src=...))</code></pre><p>Does the work of <code>@check!</code>, on one index expression,  returning <code>A</code> or <code>check!(A,...)</code> according to global flags. </p></div></div></section><pre><code class="language-none">TensorCast.check_options</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.checkrepeats" href="#TensorCast.checkrepeats"><code>TensorCast.checkrepeats</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">checkrepeats(flat)</code></pre><p>Throws an error if there are repeated indices.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.colonise!" href="#TensorCast.colonise!"><code>TensorCast.colonise!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">colonise!(isz, slist)</code></pre><p>This aims to simplify &quot;isz&quot; which is going to be used <code>reshape(A, isz)</code>.  Partly for cosmetic reasons... some of which are now caught elsewhere. </p><p>But partly because <code>slist</code> may contain one <code>:</code>, and <code>isz</code> may try to multiply that by <code>sz[d]</code>,  correct answer is <code>:</code> again. </p></div></div></section><pre><code class="language-none">TensorCast.copy_glue</code></pre><pre><code class="language-none">TensorCast.countcolons</code></pre><pre><code class="language-none">TensorCast.decolonise</code></pre><pre><code class="language-none">TensorCast.eval</code></pre><pre><code class="language-none">TensorCast.findcheck</code></pre><pre><code class="language-none">TensorCast.flag_list</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.glue" href="#TensorCast.glue"><code>TensorCast.glue</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">glue!(B, A, code)
glue(A, code) = glue!(Array{T}(...), A, code)</code></pre><p>Copy the contents of an array of arrays into one larger array,  un-doing <code>sliceview</code> / <code>slicecopy</code> with the same <code>code</code>. Also called <code>stack</code> or <code>align</code> elsewhere. </p><pre><code class="language-none">cat_glue(A, code)
red_glue(A, code)</code></pre><p>The same result, but calling either things like <code>hcat(A...)</code> or things like <code>reduce(hcat, A)</code>.  The code must be sorted like <code>(:,:,:,*,*)</code>, except that <code>(*,:)</code> is allowed. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.glue!" href="#TensorCast.glue!"><code>TensorCast.glue!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><div><pre><code class="language-none">glue!(B, A, code)
glue(A, code) = glue!(Array{T}(...), A, code)</code></pre><p>Copy the contents of an array of arrays into one larger array,  un-doing <code>sliceview</code> / <code>slicecopy</code> with the same <code>code</code>. Also called <code>stack</code> or <code>align</code> elsewhere. </p><pre><code class="language-none">cat_glue(A, code)
red_glue(A, code)</code></pre><p>The same result, but calling either things like <code>hcat(A...)</code> or things like <code>reduce(hcat, A)</code>.  The code must be sorted like <code>(:,:,:,*,*)</code>, except that <code>(*,:)</code> is allowed. </p></div></div></div></section><pre><code class="language-none">TensorCast.gluecodecheck</code></pre><pre><code class="language-none">TensorCast.gluedsize</code></pre><pre><code class="language-none">TensorCast.include</code></pre><pre><code class="language-none">TensorCast.index_store</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.inputex" href="#TensorCast.inputex"><code>TensorCast.inputex</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">inputex(:( A[i,j][k] ), canon, flags, store, icheck, where)</code></pre><p>Figures out all the steps needed to transform the given tensor to a boring one,  aligned with canonical, and returns the necessary expression.  Write sizes which can be read from <code>A</code> into <code>store</code>, and necessary reshapes in terms of <code>sz[d]</code>.</p></div></div></section><pre><code class="language-none">TensorCast.iscodesorted</code></pre><pre><code class="language-none">TensorCast.isconstant</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.makelazy" href="#TensorCast.makelazy"><code>TensorCast.makelazy</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">makelazy(bc)</code></pre><p>Takes the result of <code>Broadcast.__dot__()</code> and converts it to have a <code>LazyArrays.BroadcastArray</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.oddunique" href="#TensorCast.oddunique"><code>TensorCast.oddunique</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">oddunique(negated)</code></pre><p>Returns a list in which anything repeated evenly many times has been removed, then <code>unique</code>. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.orient" href="#TensorCast.orient"><code>TensorCast.orient</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">B = orient(A, code)</code></pre><p>Reshapes <code>A</code> such that its nontrivial axes lie in the directions where <code>code</code> contains a <code>:</code>, by inserting axes on which <code>size(B, d) == 1</code> as needed. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.outputinplace" href="#TensorCast.outputinplace"><code>TensorCast.outputinplace</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">outputinplace(newright, outUZ, redfun, canonsize, canon, flags, store, nameZ)</code></pre><p>For the case of <code>=</code> this figures out how to write RHS into LHS, in one of three ways:</p><ul><li>reduction <code>sum!(Z, newright)</code></li><li>broadcasting <code>@. Z[...] = newright</code></li><li>neither, <code>copyto!(Z, newright)</code> </li></ul><p>No longer attempts to write <code>permutedims!(Z, A, ...)</code>, now just <code>copyto!(Z, PermutedDimsArray(A, ...))</code>. Doesn&#39;t really need so many arguments...</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.outputnew" href="#TensorCast.outputnew"><code>TensorCast.outputnew</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none"> outputnew(newright, outUZ, redfun, canonsize, canon, flags, store)</code></pre><p>For the case of <code>:=</code>, this constructs the expression to do reduction if needed,  and slicing/reshaping/reversing for LHS. </p><p>outUZ = (redUind, negV, codeW, indW, sizeX, getY, numY, sizeZ)</p></div></div></section><pre><code class="language-none">TensorCast.packagecheck</code></pre><pre><code class="language-none">TensorCast.packageerror</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.parse!" href="#TensorCast.parse!"><code>TensorCast.parse!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">flat, getafix, putsz, negated = parse!(sdict, A, outer, inner)</code></pre><p>Use this for <code>A[outer...][inner...]</code>. </p><ul><li><p><code>flat</code> is a list of symbols, naming the indices, in order. </p></li><li><p><code>negated</code> is the subset which had - in front. </p></li><li><p><code>getafix</code> is the thing for making views of given input or target array, : in non-fixed directions. </p></li><li><p><code>putsz</code> is a tuple of <code>sz[n]</code> and products, which you want for reshaping to the very final array;  the numbers <code>n</code> refer to position in <code>flat</code>. </p></li><li><p><code>sdict::SizeDict</code> ends up with <code>sdict.dict[:i] =</code> an expr for length of this index. To avoid saving these (if you don&#39;t have A yet) set <code>A = nothing</code>.</p><p>parse!(sdict, A, outer, reduce_inner, true)</p></li></ul><p>This is for <code>A[outer...] = sum(inner...) ...</code> LHS, which are allowed to have sum(a:2, ...) ranges.  The allowranges flag <em>also</em> disables the use of <code>size(first(A), 2)</code> etc, to disable <code>size(A,2)</code> set <code>A = nothing</code> again. </p><pre><code class="language-none">parse!(sdict, nothing, [], rex, true, flag_vector)</code></pre><p>For dimensions &amp; annotations.  Will look for flags because it was given something to push them into.  You should now do this first, so that <code>sdict</code> is most likely to have these (neater) entries.  They are added using savesize!(sdict,...) which now puts later sizes into list of checks. </p></div></div></section><pre><code class="language-none">TensorCast.pretty</code></pre><pre><code class="language-none">TensorCast.push_or_append!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.readleft" href="#TensorCast.readleft"><code>TensorCast.readleft</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">canon, outUZ, nameZ, checkZ = readleft(left, redind, flags, store, icheck, where)</code></pre><p>outUZ = (redUind, negV, codeW, sizeX, getY, numY, sizeZ)  are things passed to output construction. </p></div></div></section><pre><code class="language-none">TensorCast.recursive_glue</code></pre><pre><code class="language-none">TensorCast.red_glue</code></pre><pre><code class="language-none">TensorCast.savesize!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.simplicode" href="#TensorCast.simplicode"><code>TensorCast.simplicode</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">p, (bef, aft) = simplicode(p, (bef, aft))</code></pre><p>The idea here is to absorb permutedims into alterations of the slicing / gluing codes.  Done as an optimisation pass, after figuring all of those out, before building expression. </p><ul><li><p>If we also have reversal or shift, those dimensions will need to be updated. Maybe then don&#39;t bother. </p></li><li><p>If there is a reduction, then it&#39;s totally worth bothering! You have more freedom than with slicing.</p></li><li><p>It will also tend to break agreement between canonical sizes &amp; size of actual array at any stage.  This may matter for in-place case. </p></li></ul><p>For now, only apply this when those are not in use. </p></div></div></section><pre><code class="language-none">TensorCast.size_store</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.sizeinfer" href="#TensorCast.sizeinfer"><code>TensorCast.sizeinfer</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">sizeinfer(store, icannon, leaveone=true)</code></pre><p>This is the point of SizeDict.  The goal is to produce a canonical vector of sizes, corresponding to vector of symbols icannon.  If these sizes are known, easy!</p><p>But for unknown ones, we do a second pass, looking for entries in sizedict like [:i, :j] which come from tuples of indices, for which we know the product of their dimensions. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.slicecopy" href="#TensorCast.slicecopy"><code>TensorCast.slicecopy</code></a> — <span class="docstring-category">Function</span>.</div><div><div><div><pre><code class="language-none">sliceview(A, code)
slicecopy(A, code)</code></pre><p>Slice array <code>A</code> according to <code>code</code>, a tuple of length <code>ndims(A)</code>,  in which <code>:</code> indicates a dimension of the slices, and <code>*</code> a dimension separating them.  For example if <code>code = (:,*,:)</code> then slices are either <code>view(A, :,i,:)</code>  or <code>A[:,i,:]</code> with <code>i=1:size(A,2)</code>. </p></div></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.sliceview" href="#TensorCast.sliceview"><code>TensorCast.sliceview</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">sliceview(A, code)
slicecopy(A, code)</code></pre><p>Slice array <code>A</code> according to <code>code</code>, a tuple of length <code>ndims(A)</code>,  in which <code>:</code> indicates a dimension of the slices, and <code>*</code> a dimension separating them.  For example if <code>code = (:,*,:)</code> then slices are either <code>view(A, :,i,:)</code>  or <code>A[:,i,:]</code> with <code>i=1:size(A,2)</code>. </p></div></div></section><pre><code class="language-none">TensorCast.sorted_starcode</code></pre><pre><code class="language-none">TensorCast.static_glue</code></pre><pre><code class="language-none">TensorCast.static_slice</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.stripminus!" href="#TensorCast.stripminus!"><code>TensorCast.stripminus!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">stripminus(negated, i)</code></pre><p>For any index, or vector of indices, or vector of indices containing tuples / backslash combos,  this pushes every letter with a minus in front into negated list.</p><p>It returns an index, or a tidied-up vector of indices, in which tuples etc have become vectors.   </p></div></div></section><pre><code class="language-none">TensorCast.szwrap</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TensorCast.walker" href="#TensorCast.walker"><code>TensorCast.walker</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">walker(outex, x, canon, flags, store, icheck, where)</code></pre><p>Called by <code>MacroTools.prewalk</code> on RHS, finds tensors &amp; pushes <code>:( sym = inputex(this) )</code> into <code>outex.args</code>, and then replaces them with <code>sym</code>.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
