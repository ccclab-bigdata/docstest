<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Build TensorFlow from source · TensorFlow.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-1123761-11', 'auto');
ga('send', 'pageview');
</script><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="TensorFlow.jl logo"/></a><h1>TensorFlow.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="../tutorial/">MNIST tutorial</a></li><li><a class="toctext" href="../visualization/">Visualizing with Tensorboard</a></li><li><a class="toctext" href="../io/">Using queues for loading your data</a></li><li><a class="toctext" href="../shape_inference/">Shape inference</a></li><li><a class="toctext" href="../saving/">Saving and restoring</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../core/">Core functions</a></li><li><a class="toctext" href="../ops/">Operations</a></li><li><a class="toctext" href="../io_ref/">IO pipelines with queues</a></li><li><a class="toctext" href="../summary_ref/">Summaries</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../basic_usage/">Basic usage</a></li><li><a class="toctext" href="../logistic/">Logistic regression</a></li></ul></li><li><span class="toctext">Advanced</span><ul><li class="current"><a class="toctext" href>Build TensorFlow from source</a><ul class="internal"><li><a class="toctext" href="#Step-1:-Build-libtensorflow-1">Step 1: Build libtensorflow</a></li><li><a class="toctext" href="#Step-2:-Install-the-TensorFlow-binary-1">Step 2: Install the TensorFlow binary</a></li><li><a class="toctext" href="#Step-3:-Check-that-the-custom-binary-is-loaded-1">Step 3: Check that the custom binary is loaded</a></li><li><a class="toctext" href="#Tips-and-known-issues-1">Tips &amp; known issues</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Advanced</li><li><a href>Build TensorFlow from source</a></li></ul></nav><hr/><div id="topbar"><span>Build TensorFlow from source</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Building-TensorFlow-from-source-1" href="#Building-TensorFlow-from-source-1">Building TensorFlow from source</a></h1><p>Building TensorFlow from source is recommended by Google for maximum performance, especially when running in CPU mode - on some systems, the difference can be substantial. It will also be required for Mac OS X GPU support for TensorFlow versions later than 1.1. This document describes how to do so.</p><h2><a class="nav-anchor" id="Step-1:-Build-libtensorflow-1" href="#Step-1:-Build-libtensorflow-1">Step 1: Build libtensorflow</a></h2><p>To build libtensorflow for TensorFlow.jl, follow the <a href="https://www.tensorflow.org/install/install_sources">official instructions for building tensorFLow from source</a>, except for a few minor modifications so as to build the library rather than the client.</p><ul><li>In the step &quot;Prepare environment&quot;, ignore &quot;Install python dependencies&quot; – these are not necessary as we are not building for Python. Be sure to follow all other steps as needed for your OS.</li><li>In the step &quot;Build the pip package&quot;, since we are building the binary file and not the pip package, instead run <code>bazel build --config=opt //tensorflow:libtensorflow.so</code>, adding <code>--config=cuda</code> if GPU support is desired.</li></ul><p>Running <code>bazel build</code> will produce the <code>libtensorflow.so</code> binary needed by TensorFlow.jl - there is no need to build the Python package or run anything else. You may place the binary wherever is convenient. If on Mac OS X, you may need to rename the <code>libtensorflow.so</code> to <code>libtensorflow.dylib</code>.</p><h2><a class="nav-anchor" id="Step-2:-Install-the-TensorFlow-binary-1" href="#Step-2:-Install-the-TensorFlow-binary-1">Step 2: Install the TensorFlow binary</a></h2><p>We must now tell TensorFlow.jl to where to load the custom binary from. There are a number of ways to do so.</p><ul><li><p>We can set the environment variable <code>LIBTENSORFLOW</code> to <code>/path/to/tensorflow.so</code>. This may be done system-wide by editing <code>.profile</code> or any other method supported by your OS.</p></li><li><p>For users of the Atom/Juno IDE who do not wish to modify their system-wide environment, environment variables may be set by adding the line <code>process.env.LIBTENSORFLOW = &quot;/path/to/libtensorflow.so&quot;</code> to the <code>init.coffee</code> script (easily accessible by clicking <code>File -&gt; Init Script</code>). Note that Atom may not always inherit environment variables set by the OS.</p></li><li><p>Or you can copy <code>libtensorflow.so</code> to <code>&lt;TensorFlowDIR&gt;/deps/usr/bin/</code>, overwriting the included binary. Where <code>&lt;TensorFlowDIR&gt;</code> is the path to the directory TensorFlow.jl is being loaded from.</p></li></ul><h2><a class="nav-anchor" id="Step-3:-Check-that-the-custom-binary-is-loaded-1" href="#Step-3:-Check-that-the-custom-binary-is-loaded-1">Step 3: Check that the custom binary is loaded</a></h2><p>After running <code>using TensorFlow</code>, it should no longer complain that TensorFlow wasn&#39;t compiled with the necessary instructions. Try generating two random matrices and multiplying them together. You can time the computation with <code>@time run(sess, x)</code>, which should be much faster.</p><h2><a class="nav-anchor" id="Tips-and-known-issues-1" href="#Tips-and-known-issues-1">Tips &amp; known issues</a></h2><ul><li><p>For maximum performance, you should always compile on the same system that will be running the computation, and with the correct CUDA Compute Capability version supported by your GPU.</p></li><li><p>If TensorFlow.jl fails to load with the error <code>Library not loaded: @rpath/libcublas.8.0.dylib</code> or any similar error, it means that the CUDA libraries are not in <code>LD_LIBRARY_PATH</code> as required by Nvidia. Be sure to add <code>/usr/local/cuda/lib</code>, or wherever your CUDA instalation is located, to <code>LD_LIBRARY_PATH</code>. This may be done by editing <code>.profile</code>, or for Atom/Juno users editing <code>init.coffee</code>, or any other method supported by your OS, as described in Step 2. Be careful that you append this folder and do not mistakenly overwrite your entire path.</p></li><li><p>If you get <code>CUDA_ERROR_NOT_INITIALIZED</code>, then for some reason TensorFlow cannot find your GPU. Make sure that the appropriate software is installed, and if using an external GPU, make sure it is plugged in correctly.</p></li><li><p>To check whether the GPU is being used, create your session with <code>TensorFlow.Session(config=TensorFlow.tensorflow.ConfigProto(log_device_placement=true))</code>. TensorFlow will then print information about which device is used.</p></li><li><p>You may need to add symlinks from <code>libcudnn5.dylib</code> to <code>libcudnn.5.dylib</code> so that Bazel is able to correctly locate the necessary dependencies.</p></li><li><p>On Mac OS X, <code>nvcc</code>, Nvidia&#39;s CUDA compiler, requires OS X Command Line Tools version 8.2 and does not work with the latest version. You can download this version from Apple&#39;s website, and switch to it by running <code>sudo xcode-select -s /path/to/CommandLineTools</code>.</p></li><li><p>On Mac OS X, make sure to set the environment variable <code>GCC_HOST_COMPILER_PATH</code> to <code>/usr/bin/gcc</code> - do not install GCC yourself, or the build may fail with obscure error messages.</p></li><li><p>On Mac OS X, if you don&#39;t wish to install Homebrew, you can instead use Julia&#39;s internal Homebrew-based dependency manager Homebrew.jl by running <code>Homebrew.brew(`install --build-from-source libtensorflow`)</code>. GPU support can be enabled by modifying the Ruby formula using <code>Homebrew.brew(`edit libtensorflow`)</code> – you should set all necessary environment variables in the Ruby formula, as Homebrew may not display prompts correctly.</p></li></ul><footer><hr/><a class="previous" href="../logistic/"><span class="direction">Previous</span><span class="title">Logistic regression</span></a></footer></article></body></html>
