<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · Distances.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Distances.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Supported-distances-1">Supported distances</a></li><li><a class="toctext" href="#Basic-Use-1">Basic Use</a></li><li><a class="toctext" href="#Distance-type-hierarchy-1">Distance type hierarchy</a></li><li><a class="toctext" href="#Benchmarks-1">Benchmarks</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Distances.jl-1" href="#Distances.jl-1">Distances.jl</a></h1><p><a href="https://travis-ci.org/JuliaStats/Distances.jl"><img src="https://travis-ci.org/JuliaStats/Distances.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://coveralls.io/github/JuliaStats/Distances.jl?branch=master"><img src="https://coveralls.io/repos/JuliaStats/Distances.jl/badge.svg?branch=master&amp;service=github" alt="Coverage Status"/></a></p><p><a href="http://pkg.julialang.org/?pkg=Distances"><img src="http://pkg.julialang.org/badges/Distances_0.6.svg" alt="Distances"/></a></p><p>A Julia package for evaluating distances(metrics) between vectors.</p><p>This package also provides optimized functions to compute column-wise and pairwise distances, which are often substantially faster than a straightforward loop implementation. (See the benchmark section below for details).</p><h2><a class="nav-anchor" id="Supported-distances-1" href="#Supported-distances-1">Supported distances</a></h2><ul><li>Euclidean distance</li><li>Squared Euclidean distance</li><li>Cityblock distance</li><li>Jaccard distance</li><li>Rogers-Tanimoto distance</li><li>Chebyshev distance</li><li>Minkowski distance</li><li>Hamming distance</li><li>Cosine distance</li><li>Correlation distance</li><li>Chi-square distance</li><li>Kullback-Leibler divergence</li><li>Generalized Kullback-Leibler divergence</li><li>Rényi divergence</li><li>Jensen-Shannon divergence</li><li>Mahalanobis distance</li><li>Squared Mahalanobis distance</li><li>Bhattacharyya distance</li><li>Hellinger distance</li><li>Haversine distance</li><li>Mean absolute deviation</li><li>Mean squared deviation</li><li>Root mean squared deviation</li><li>Normalized root mean squared deviation</li><li>Bray-Curtis dissimilarity</li><li>Bregman divergence </li></ul><p>For <span>$Euclidean distance$</span>, <span>$Squared Euclidean distance$</span>, <span>$Cityblock distance$</span>, <span>$Minkowski distance$</span>, and <span>$Hamming distance$</span>, a weighted version is also provided.</p><h2><a class="nav-anchor" id="Basic-Use-1" href="#Basic-Use-1">Basic Use</a></h2><p>The library supports three ways of computation: <em>computing the distance between two vectors</em>, <em>column-wise computation</em>, and <em>pairwise computation</em>.</p><h4><a class="nav-anchor" id="Computing-the-distance-between-two-vectors-1" href="#Computing-the-distance-between-two-vectors-1">Computing the distance between two vectors</a></h4><p>Each distance corresponds to a <em>distance type</em>. You can always compute a certain distance between two vectors using the following syntax</p><pre><code class="language-julia">r = evaluate(dist, x, y)</code></pre><p>Here, dist is an instance of a distance type. For example, the type for Euclidean distance is <span>$Euclidean$</span> (more distance types will be introduced in the next section), then you can compute the Euclidean distance between <span>$x$</span> and <span>$y$</span> as</p><pre><code class="language-julia">r = evaluate(Euclidean(), x, y)</code></pre><p>Common distances also come with convenient functions for distance evaluation. For example, you may also compute Euclidean distance between two vectors as below</p><pre><code class="language-julia">r = euclidean(x, y)</code></pre><h4><a class="nav-anchor" id="Computing-distances-between-corresponding-columns-1" href="#Computing-distances-between-corresponding-columns-1">Computing distances between corresponding columns</a></h4><p>Suppose you have two <span>$m-by-n$</span> matrix <span>$X$</span> and <span>$Y$</span>, then you can compute all distances between corresponding columns of X and Y in one batch, using the <span>$colwise$</span> function, as</p><pre><code class="language-julia">r = colwise(dist, X, Y)</code></pre><p>The output <span>$r$</span> is a vector of length <span>$n$</span>. In particular, <span>$r[i]$</span> is the distance between <span>$X[:,i]$</span> and <span>$Y[:,i]$</span>. The batch computation typically runs considerably faster than calling <span>$evaluate$</span> column-by-column.</p><p>Note that either of <span>$X$</span> and <span>$Y$</span> can be just a single vector – then the <span>$colwise$</span> function will compute the distance between this vector and each column of the other parameter.</p><h4><a class="nav-anchor" id="Computing-pairwise-distances-1" href="#Computing-pairwise-distances-1">Computing pairwise distances</a></h4><p>Let <span>$X$</span> and <span>$Y$</span> respectively have <span>$m$</span> and <span>$n$</span> columns. Then the <span>$pairwise$</span> function computes distances between each pair of columns in <span>$X$</span> and <span>$Y$</span>:</p><pre><code class="language-julia">R = pairwise(dist, X, Y)</code></pre><p>In the output, <span>$R$</span> is a matrix of size <span>$(m, n)$</span>, such that <span>$R[i,j]$</span> is the distance between <span>$X[:,i]$</span> and <span>$Y[:,j]$</span>. Computing distances for all pairs using <span>$pairwise$</span> function is often remarkably faster than evaluting for each pair individually.</p><p>If you just want to just compute distances between columns of a matrix <span>$X$</span>, you can write</p><pre><code class="language-julia">R = pairwise(dist, X)</code></pre><p>This statement will result in an <span>$m-by-m$</span> matrix, where <span>$R[i,j]$</span> is the distance between <span>$X[:,i]$</span> and <span>$X[:,j]$</span>. <span>$pairwise(dist, X)$</span> is typically more efficient than <span>$pairwise(dist, X, X)$</span>, as the former will take advantage of the symmetry when <span>$dist$</span> is a semi-metric (including metric).</p><h4><a class="nav-anchor" id="Computing-column-wise-and-pairwise-distances-inplace-1" href="#Computing-column-wise-and-pairwise-distances-inplace-1">Computing column-wise and pairwise distances inplace</a></h4><p>If the vector/matrix to store the results are pre-allocated, you may use the storage (without creating a new array) using the following syntax:</p><pre><code class="language-julia">colwise!(r, dist, X, Y)
pairwise!(R, dist, X, Y)
pairwise!(R, dist, X)</code></pre><p>Please pay attention to the difference, the functions for inplace computation are <span>$colwise!$</span> and <span>$pairwise!$</span> (instead of <span>$colwise$</span> and <span>$pairwise$</span>).</p><h2><a class="nav-anchor" id="Distance-type-hierarchy-1" href="#Distance-type-hierarchy-1">Distance type hierarchy</a></h2><p>The distances are organized into a type hierarchy.</p><p>At the top of this hierarchy is an abstract class <strong>PreMetric</strong>, which is defined to be a function <span>$d$</span> that satisfies</p><pre><code class="language-none">d(x, x) == 0  for all x
d(x, y) &gt;= 0  for all x, y</code></pre><p><strong>SemiMetric</strong> is a abstract type that refines <strong>PreMetric</strong>. Formally, a <em>semi-metric</em> is a <em>pre-metric</em> that is also symmetric, as</p><pre><code class="language-none">d(x, y) == d(y, x)  for all x, y</code></pre><p><strong>Metric</strong> is a abstract type that further refines <strong>SemiMetric</strong>. Formally, a <em>metric</em> is a <em>semi-metric</em> that also satisfies triangle inequality, as</p><pre><code class="language-none">d(x, z) &lt;= d(x, y) + d(y, z)  for all x, y, z</code></pre><p>This type system has practical significance. For example, when computing pairwise distances between a set of vectors, you may only perform computation for half of the pairs, and derive the values immediately for the remaining halve by leveraging the symmetry of <em>semi-metrics</em>.</p><p>Each distance corresponds to a distance type. The type name and the corresponding mathematical definitions of the distances are listed in the following table.</p><table><tr><th>type name</th><th>convenient syntax</th><th>math definition</th></tr><tr><td>Euclidean</td><td><code>euclidean(x, y)</code></td><td><code>sqrt(sum((x - y) .^ 2))</code></td></tr><tr><td>SqEuclidean</td><td><code>sqeuclidean(x, y)</code></td><td><code>sum((x - y).^2)</code></td></tr><tr><td>Cityblock</td><td><code>cityblock(x, y)</code></td><td><code>sum(abs(x - y))</code></td></tr><tr><td>Chebyshev</td><td><code>chebyshev(x, y)</code></td><td><code>max(abs(x - y))</code></td></tr><tr><td>Minkowski</td><td><code>minkowski(x, y, p)</code></td><td><code>sum(abs(x - y).^p) ^ (1/p)</code></td></tr><tr><td>Hamming</td><td><code>hamming(k, l)</code></td><td><code>sum(k .!= l)</code></td></tr><tr><td>RogersTanimoto</td><td><code>rogerstanimoto(a, b)</code></td><td><code>2(sum(a&amp;!b) + sum(!a&amp;b)) / (2(sum(a&amp;!b) + sum(!a&amp;b)) + sum(a&amp;b) + sum(!a&amp;!b))</code></td></tr><tr><td>Jaccard</td><td><code>jaccard(x, y)</code></td><td><code>1 - sum(min(x, y)) / sum(max(x, y))</code></td></tr><tr><td>BrayCurtis</td><td><code>braycurtis(x, y)</code></td><td><code>sum(abs(x - y)) / sum(abs(x + y))</code></td></tr><tr><td>CosineDist</td><td><code>cosine_dist(x, y)</code></td><td><code>1 - dot(x, y) / (norm(x) * norm(y))</code></td></tr><tr><td>CorrDist</td><td><code>corr_dist(x, y)</code></td><td><code>cosine_dist(x - mean(x), y - mean(y))</code></td></tr><tr><td>ChiSqDist</td><td><code>chisq_dist(x, y)</code></td><td><code>sum((x - y).^2 / (x + y))</code></td></tr><tr><td>KLDivergence</td><td><code>kl_divergence(p, q)</code></td><td><code>sum(p .* log(p ./ q))</code></td></tr><tr><td>GenKLDivergence</td><td><code>gkl_divergence(x, y)</code></td><td><code>sum(p .* log(p ./ q) - p + q)</code></td></tr><tr><td>RenyiDivergence</td><td><code>renyi_divergence(p, q, k)</code></td><td><code>log(sum( p .* (p ./ q) .^ (k - 1))) / (k - 1)</code></td></tr><tr><td>JSDivergence</td><td><code>js_divergence(p, q)</code></td><td><code>KL(p, m) / 2 + KL(p, m) / 2 with m = (p + q) / 2</code></td></tr><tr><td>SpanNormDist</td><td><code>spannorm_dist(x, y)</code></td><td><code>max(x - y) - min(x - y)</code></td></tr><tr><td>BhattacharyyaDist</td><td><code>bhattacharyya(x, y)</code></td><td><code>-log(sum(sqrt(x .* y) / sqrt(sum(x) * sum(y)))</code></td></tr><tr><td>HellingerDist</td><td><code>hellinger(x, y)</code></td><td><code>sqrt(1 - sum(sqrt(x .* y) / sqrt(sum(x) * sum(y))))</code></td></tr><tr><td>Haversine</td><td><code>haversine(x, y, r)</code></td><td><a href="https://en.wikipedia.org/wiki/Haversine_formula">Haversine formula</a></td></tr><tr><td>Mahalanobis</td><td><code>mahalanobis(x, y, Q)</code></td><td><code>sqrt((x - y)&#39; * Q * (x - y))</code></td></tr><tr><td>SqMahalanobis</td><td><code>sqmahalanobis(x, y, Q)</code></td><td><code>(x - y)&#39; * Q * (x - y)</code></td></tr><tr><td>MeanAbsDeviation</td><td><code>meanad(x, y)</code></td><td><code>mean(abs.(x - y))</code></td></tr><tr><td>MeanSqDeviation</td><td><code>msd(x, y)</code></td><td><code>mean(abs2.(x - y))</code></td></tr><tr><td>RMSDeviation</td><td><code>rmsd(x, y)</code></td><td><code>sqrt(msd(x, y))</code></td></tr><tr><td>NormRMSDeviation</td><td><code>nrmsd(x, y)</code></td><td><code>rmsd(x, y) / (maximum(x) - minimum(x))</code></td></tr><tr><td>WeightedEuclidean</td><td><code>weuclidean(x, y, w)</code></td><td><code>sqrt(sum((x - y).^2 .* w))</code></td></tr><tr><td>WeightedSqEuclidean</td><td><code>wsqeuclidean(x, y, w)</code></td><td><code>sum((x - y).^2 .* w)</code></td></tr><tr><td>WeightedCityblock</td><td><code>wcityblock(x, y, w)</code></td><td><code>sum(abs(x - y) .* w)</code></td></tr><tr><td>WeightedMinkowski</td><td><code>wminkowski(x, y, w, p)</code></td><td><code>sum(abs(x - y).^p .* w) ^ (1/p)</code></td></tr><tr><td>WeightedHamming</td><td><code>whamming(x, y, w)</code></td><td><code>sum((x .!= y) .* w)</code></td></tr><tr><td>Bregman</td><td><code>bregman(F, ∇, x, y; inner = LinearAlgebra.dot)</code></td><td><code>F(x) - F(y) - inner(∇(y), x - y)</code></td></tr></table><p><strong>Note:</strong> The formulas above are using <em>Julia</em>&#39;s functions. These formulas are mainly for conveying the math concepts in a concise way. The actual implementation may use a faster way. The arguments <code>x</code> and <code>y</code> are arrays of real numbers; <code>k</code> and <code>l</code> are arrays of distinct elements of any kind; a and b are arrays of Bools; and finally, <code>p</code> and <code>q</code> are arrays forming a discrete probability distribution and are therefore both expected to sum to one.</p><h3><a class="nav-anchor" id="Precision-for-Euclidean-and-SqEuclidean-1" href="#Precision-for-Euclidean-and-SqEuclidean-1">Precision for Euclidean and SqEuclidean</a></h3><p>For efficiency (see the benchmarks below), <code>Euclidean</code> and <code>SqEuclidean</code> make use of BLAS3 matrix-matrix multiplication to calculate distances.  This corresponds to the following expansion:</p><pre><code class="language-julia">(x-y)^2 == x^2 - 2xy + y^2</code></pre><p>However, equality is not precise in the presence of roundoff error, and particularly when <code>x</code> and <code>y</code> are nearby points this may not be accurate.  Consequently, <code>Euclidean</code> and <code>SqEuclidean</code> allow you to supply a relative tolerance to force recalculation:</p><pre><code class="language-julia">julia&gt; x = reshape([0.1, 0.3, -0.1], 3, 1);

julia&gt; pairwise(Euclidean(), x, x)
1×1 Array{Float64,2}:
 7.45058e-9

julia&gt; pairwise(Euclidean(1e-12), x, x)
1×1 Array{Float64,2}:
 0.0</code></pre><h2><a class="nav-anchor" id="Benchmarks-1" href="#Benchmarks-1">Benchmarks</a></h2><p>The implementation has been carefully optimized based on benchmarks. The script in <code>benchmark/benchmarks.jl</code> defines a benchmark suite for a variety of distances, under column-wise and pairwise settings.</p><p>Here are benchmarks obtained running Julia 0.6 on a computer with a quad-core Intel Core i5-2500K processor @ 3.3 GHz. The tables below can be replicated using the script in <code>benchmark/print_table.jl</code>.</p><h4><a class="nav-anchor" id="Column-wise-benchmark-1" href="#Column-wise-benchmark-1">Column-wise benchmark</a></h4><p>The table below compares the performance (measured in terms of average elapsed time of each iteration) of a straightforward loop implementation and an optimized implementation provided in <em>Distances.jl</em>. The task in each iteration is to compute a specific distance between corresponding columns in two <span>$200-by-10000$</span> matrices.</p><table><tr><th>distance</th><th>loop</th><th>colwise</th><th>gain</th></tr><tr><td>SqEuclidean</td><td>0.005460s</td><td>0.001676s</td><td>3.2582</td></tr><tr><td>Euclidean</td><td>0.005513s</td><td>0.001681s</td><td>3.2792</td></tr><tr><td>Cityblock</td><td>0.005409s</td><td>0.001675s</td><td>3.2292</td></tr><tr><td>Chebyshev</td><td>0.008592s</td><td>0.004575s</td><td>1.8779</td></tr><tr><td>Minkowski</td><td>0.056741s</td><td>0.048808s</td><td>1.1625</td></tr><tr><td>Hamming</td><td>0.005320s</td><td>0.001670s</td><td>3.1847</td></tr><tr><td>CosineDist</td><td>0.005663s</td><td>0.001697s</td><td>3.3378</td></tr><tr><td>CorrDist</td><td>0.010000s</td><td>0.013904s</td><td>0.7192</td></tr><tr><td>ChiSqDist</td><td>0.009626s</td><td>0.004734s</td><td>2.0333</td></tr><tr><td>KLDivergence</td><td>0.046696s</td><td>0.035091s</td><td>1.3307</td></tr><tr><td>RenyiDivergence</td><td>0.021123s</td><td>0.012006s</td><td>1.7594</td></tr><tr><td>RenyiDivergence</td><td>0.080503s</td><td>0.066987s</td><td>1.2018</td></tr><tr><td>JSDivergence</td><td>0.066404s</td><td>0.059564s</td><td>1.1148</td></tr><tr><td>BhattacharyyaDist</td><td>0.013065s</td><td>0.008807s</td><td>1.4836</td></tr><tr><td>HellingerDist</td><td>0.013013s</td><td>0.008679s</td><td>1.4993</td></tr><tr><td>WeightedSqEuclidean</td><td>0.005534s</td><td>0.001676s</td><td>3.3028</td></tr><tr><td>WeightedEuclidean</td><td>0.005601s</td><td>0.001723s</td><td>3.2513</td></tr><tr><td>WeightedCityblock</td><td>0.005496s</td><td>0.001675s</td><td>3.2815</td></tr><tr><td>WeightedMinkowski</td><td>0.057847s</td><td>0.051389s</td><td>1.1257</td></tr><tr><td>WeightedHamming</td><td>0.005439s</td><td>0.001673s</td><td>3.2513</td></tr><tr><td>SqMahalanobis</td><td>0.134717s</td><td>0.019530s</td><td>6.8980</td></tr><tr><td>Mahalanobis</td><td>0.129455s</td><td>0.020114s</td><td>6.4361</td></tr><tr><td>BrayCurtis</td><td>0.005666s</td><td>0.001680s</td><td>3.3736</td></tr></table><p>We can see that using <span>$colwise$</span> instead of a simple loop yields considerable gain (2x - 4x), especially when the internal computation of each distance is simple. Nonetheless, when the computation of a single distance is heavy enough (e.g. <em>KLDivergence</em>,  <em>RenyiDivergence</em>), the gain is not as significant.</p><h4><a class="nav-anchor" id="Pairwise-benchmark-1" href="#Pairwise-benchmark-1">Pairwise benchmark</a></h4><p>The table below compares the performance (measured in terms of average elapsed time of each iteration) of a straightforward loop implementation and an optimized implementation provided in <em>Distances.jl</em>. The task in each iteration is to compute a specific distance in a pairwise manner between columns in a <span>$100-by-200$</span> and <span>$100-by-250$</span> matrices, which will result in a <span>$200-by-250$</span> distance matrix.</p><table><tr><th>distance</th><th>loop</th><th>pairwise</th><th>gain</th></tr><tr><td>SqEuclidean</td><td>0.015116s</td><td>0.000192s</td><td><strong>78.7747</strong></td></tr><tr><td>Euclidean</td><td>0.015565s</td><td>0.000390s</td><td>39.8829</td></tr><tr><td>Cityblock</td><td>0.015048s</td><td>0.001400s</td><td>10.7469</td></tr><tr><td>Chebyshev</td><td>0.023325s</td><td>0.010921s</td><td>2.1358</td></tr><tr><td>Minkowski</td><td>0.143427s</td><td>0.121050s</td><td>1.1849</td></tr><tr><td>Hamming</td><td>0.015191s</td><td>0.001334s</td><td>11.3856</td></tr><tr><td>CosineDist</td><td>0.016688s</td><td>0.000393s</td><td><strong>42.5158</strong></td></tr><tr><td>CorrDist</td><td>0.029024s</td><td>0.000435s</td><td><strong>66.7043</strong></td></tr><tr><td>ChiSqDist</td><td>0.026035s</td><td>0.012194s</td><td>2.1351</td></tr><tr><td>KLDivergence</td><td>0.115800s</td><td>0.086968s</td><td>1.3315</td></tr><tr><td>RenyiDivergence</td><td>0.055551s</td><td>0.029628s</td><td>1.8749</td></tr><tr><td>RenyiDivergence</td><td>0.205270s</td><td>0.163031s</td><td>1.2591</td></tr><tr><td>JSDivergence</td><td>0.165078s</td><td>0.148902s</td><td>1.1086</td></tr><tr><td>BhattacharyyaDist</td><td>0.035493s</td><td>0.022429s</td><td>1.5824</td></tr><tr><td>HellingerDist</td><td>0.035028s</td><td>0.021867s</td><td>1.6019</td></tr><tr><td>WeightedSqEuclidean</td><td>0.016330s</td><td>0.000276s</td><td><strong>59.2117</strong></td></tr><tr><td>WeightedEuclidean</td><td>0.016600s</td><td>0.000508s</td><td><strong>32.6478</strong></td></tr><tr><td>WeightedCityblock</td><td>0.015604s</td><td>0.001816s</td><td>8.5913</td></tr><tr><td>WeightedMinkowski</td><td>0.159052s</td><td>0.128427s</td><td>1.2385</td></tr><tr><td>WeightedHamming</td><td>0.015212s</td><td>0.001634s</td><td>9.3110</td></tr><tr><td>SqMahalanobis</td><td>0.607881s</td><td>0.000365s</td><td><strong>1665.3228</strong></td></tr><tr><td>Mahalanobis</td><td>0.623032s</td><td>0.000604s</td><td><strong>1031.9581</strong></td></tr><tr><td>BrayCurtis</td><td>0.015843s</td><td>0.002273s</td><td>6.9695</td></tr></table><p>For distances of which a major part of the computation is a quadratic form (e.g. <em>Euclidean</em>, <em>CosineDist</em>, <em>Mahalanobis</em>), the performance can be drastically improved by restructuring the computation and delegating the core part to <span>$GEMM$</span> in <em>BLAS</em>. The use of this strategy can easily lead to 100x performance gain over simple loops (see the highlighted part of the table above).</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
