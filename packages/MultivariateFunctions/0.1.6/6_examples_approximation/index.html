<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples - Approximation Â· MultivariateFunctions</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MultivariateFunctions</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">MultivariateFunctions</a></li><li><a class="toctext" href="../1_structs_and_limitations/">Structs</a></li><li><a class="toctext" href="../2_interpolation_methods/">Univariate Interpolation Methods</a></li><li><a class="toctext" href="../3_approximation_methods/">Supported Approximation Methods</a></li><li><a class="toctext" href="../4_examples_algebra/">Examples - Algebra</a></li><li><a class="toctext" href="../5_examples_interpolation/">Examples - Data interpolation</a></li><li class="current"><a class="toctext" href>Examples - Approximation</a><ul class="internal"><li><a class="toctext" href="#OLS-approximation-1">OLS approximation</a></li><li><a class="toctext" href="#Numerical-Integration-with-Chebyshev-polynomials-1">Numerical Integration with Chebyshev polynomials</a></li><li><a class="toctext" href="#Multivariate:-MARS-Spline-for-approximation-1">Multivariate: MARS Spline for approximation</a></li></ul></li><li><a class="toctext" href="../99_refs/">References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Examples - Approximation</a></li></ul></nav><hr/><div id="topbar"><span>Examples - Approximation</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Examples-Approximation-1" href="#Examples-Approximation-1">Examples - Approximation</a></h1><h2><a class="nav-anchor" id="OLS-approximation-1" href="#OLS-approximation-1">OLS approximation</a></h2><p>If we have lots of data that we want to summarise with OLS</p><pre><code class="language-none"># Generating example data
using Random
using Distributions
using DataStructures
Random.seed!(1)
obs = 1000
X = rand(obs)
y = X .+ rand(Normal(),obs) .+ 7
# And now making an approximation function
approxFunction = create_ols_approximation(y, X, 2)</code></pre><h2><a class="nav-anchor" id="Numerical-Integration-with-Chebyshev-polynomials-1" href="#Numerical-Integration-with-Chebyshev-polynomials-1">Numerical Integration with Chebyshev polynomials</a></h2><p>And if we want to approximate the sin function in the [2.3, 5.6] bound with 7 polynomial terms and 20 approximation nodes:</p><pre><code class="language-none">chebyshevs = create_chebyshev_approximation(sin, 20, 7, OrderedDict{Symbol,Tuple{Float64,Float64}}(:default =&gt; (2.3, 5.6)))</code></pre><p>We can integrate the above term in the normal way to achieve Gauss-Chebyshev quadrature:</p><pre><code class="language-none">integral(chebyshevs, 2.3, 5.6)</code></pre><h2><a class="nav-anchor" id="Multivariate:-MARS-Spline-for-approximation-1" href="#Multivariate:-MARS-Spline-for-approximation-1">Multivariate: MARS Spline for approximation</a></h2><p>First we will generate some example data.</p><pre><code class="language-none">using MultivariateFunctions
using Random
using DataFrames
using Distributions
using DataStructures

Random.seed!(1992)
nObs = 1000
dd = DataFrame()
dd[:x] = rand( Normal(),nObs) + 0.1 .* rand( Normal(),nObs)
dd[:z] = rand( Normal(),nObs) + 0.1 .* rand( Normal(),nObs)
dd[:w] = (0.5 .* rand( Normal(),nObs)) .+ 0.7.*(dd[:z] .- dd[:x]) + 0.1 .* rand( Normal(),nObs)
dd[:y] = (dd[:x] .*dd[:w] ) .* (dd[:z] .- dd[:w]) .+ dd[:x] + rand( Normal(),nObs)
dd[7,:y] = 1.0
y = :y
x_variables = Set{Symbol}([:w, :x, :z])</code></pre><p>It is important to note here that we have a set of symbols for x_variables. This is the set of columns in the dataframe that we will use to predict y - the dependent variable.</p><p>We can then create an approximation with recursive partitioning:</p><pre><code class="language-none">number_of_divisions = 7
rp_4, rp_reg_4 = create_recursive_partitioning(dd, y, x_variables, number_of_divisions; rel_tol = 1e-3)</code></pre><p>We can also create a MARS approximation spline:</p><pre><code class="language-none">rp_1, rp_reg_1 = create_mars_spline(dd, y, x_variables, number_of_divisions; rel_tol = 1e-3)</code></pre><p>Note that the rel_tol here is the tolerance in the optimisation step for hinges (or divisions in the recursive partitioning case). In most applied cases it generally doesn&#39;t matter much if there is a hinge at 1.0006 or at 1.0007 so in most settings this can be set higher than you would generally set the tolerance for a numerical optimiser. For this reason the default value is 1e-02.</p><footer><hr/><a class="previous" href="../5_examples_interpolation/"><span class="direction">Previous</span><span class="title">Examples - Data interpolation</span></a><a class="next" href="../99_refs/"><span class="direction">Next</span><span class="title">References</span></a></footer></article></body></html>
