<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · ReinforcementLearningEnvironmentDiscrete.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearningEnvironmentDiscrete.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.AbsorbingDetMDP" href="#ReinforcementLearningEnvironmentDiscrete.AbsorbingDetMDP"><code>ReinforcementLearningEnvironmentDiscrete.AbsorbingDetMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">AbsorbingDetMDP(;ns = 10^3, na = 10)</code></pre><p>Returns a random deterministic absorbing MDP</p></div></div></section><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.CliffWalking</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.DetMDP" href="#ReinforcementLearningEnvironmentDiscrete.DetMDP"><code>ReinforcementLearningEnvironmentDiscrete.DetMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">DetMDP(; ns = 10^4, na = 10)</code></pre><p>Returns a random deterministic MDP.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.DetTreeMDP" href="#ReinforcementLearningEnvironmentDiscrete.DetTreeMDP"><code>ReinforcementLearningEnvironmentDiscrete.DetTreeMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">DetTreeMDP(; na = 4, depth = 5)</code></pre><p>Returns a treeMDP with random rewards at the leaf nodes.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.DetTreeMDPwithinrew" href="#ReinforcementLearningEnvironmentDiscrete.DetTreeMDPwithinrew"><code>ReinforcementLearningEnvironmentDiscrete.DetTreeMDPwithinrew</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">DetTreeMDPwithinrew(; na = 4, depth = 5)</code></pre><p>Returns a treeMDP with random rewards.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.DiscreteMaze" href="#ReinforcementLearningEnvironmentDiscrete.DiscreteMaze"><code>ReinforcementLearningEnvironmentDiscrete.DiscreteMaze</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct DiscreteMaze
    mdp::MDP
    maze::Array{Int64, 2}
    goals::Array{Int64, 1}
    nzpos::Array{Int64, 1}</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.MDP" href="#ReinforcementLearningEnvironmentDiscrete.MDP"><code>ReinforcementLearningEnvironmentDiscrete.MDP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct MDP 
    ns::Int64
    na::Int64
    state::Int64
    trans_probs::Array{AbstractArray, 2}
    reward::Array{Float64, 2}
    initialstates::Array{Int64, 1}
    isterminal::Array{Int64, 1}</code></pre><p>A Markov Decision Process with <code>ns</code> states, <code>na</code> actions, current <code>state</code>, <code>na</code>x<code>ns</code> - array of transition probabilites <code>trans_props</code> which consists for every (action, state) pair of a (potentially sparse) array that sums to 1 (see <a href="#ReinforcementLearningEnvironmentDiscrete.getprobvecrandom"><code>getprobvecrandom</code></a>, <a href="#ReinforcementLearningEnvironmentDiscrete.getprobvecuniform"><code>getprobvecuniform</code></a>, <a href="#ReinforcementLearningEnvironmentDiscrete.getprobvecdeterministic"><code>getprobvecdeterministic</code></a> for helpers to constract the transition probabilities) <code>na</code>x<code>ns</code> - array of <code>reward</code>, array of initial states <code>initialstates</code>, and <code>ns</code> - array of 0/1 indicating if a state is terminal.</p></div></div></section><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.MDPEnv</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.POMDPEnv</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.ReinforcementLearningEnvironmentDiscrete</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.StochMDP" href="#ReinforcementLearningEnvironmentDiscrete.StochMDP"><code>ReinforcementLearningEnvironmentDiscrete.StochMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">StochMDP(; na = 10, ns = 50) = MDP(ns, na)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.StochTreeMDP" href="#ReinforcementLearningEnvironmentDiscrete.StochTreeMDP"><code>ReinforcementLearningEnvironmentDiscrete.StochTreeMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">StochTreeMDP(; na = 4, depth = 4, bf = 2)</code></pre><p>Returns a random stochastic treeMDP with branching factor <code>bf</code>.</p></div></div></section><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.addrandomwall!</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.breaksomewalls</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.checkpos</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.emptytransprob!</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.eval</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.getemptymaze</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.getprobvecdeterministic" href="#ReinforcementLearningEnvironmentDiscrete.getprobvecdeterministic"><code>ReinforcementLearningEnvironmentDiscrete.getprobvecdeterministic</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">getprobvecdeterministic(n, min = 1, max = n)</code></pre><p>Returns a <code>SparseVector</code> of length <code>n</code> where one element in <code>min</code>:<code>max</code> has  value 1.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.getprobvecrandom" href="#ReinforcementLearningEnvironmentDiscrete.getprobvecrandom"><code>ReinforcementLearningEnvironmentDiscrete.getprobvecrandom</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">getprobvecrandom(n)</code></pre><p>Returns an array of length <code>n</code> that sums to 1. More precisely, the array is a sample of a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> with <code>n</code> categories and <span>$α_1 = ⋯  = α_n = 1$</span>.</p></div></div><div><div><pre><code class="language-none">getprobvecrandom(n, min, max)</code></pre><p>Returns an array of length <code>n</code> that sums to 1 where all elements outside of <code>min</code>:<code>max</code> are zero.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.getprobvecuniform" href="#ReinforcementLearningEnvironmentDiscrete.getprobvecuniform"><code>ReinforcementLearningEnvironmentDiscrete.getprobvecuniform</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">getprobvecuniform(n)  = fill(1/n, n)</code></pre></div></div></section><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.include</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.indto2d</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.mazetomdp</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.observationindex</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.posto1d</code></pre><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.rng</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.run!" href="#ReinforcementLearningEnvironmentDiscrete.run!"><code>ReinforcementLearningEnvironmentDiscrete.run!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">run!(mdp::MDP, action::Int64)</code></pre><p>Transition to a new state given <code>action</code>. Returns the new state.</p></div></div><div><div><pre><code class="language-none">run!(mdp::MDP, policy::Array{Int64, 1}) = run!(mdp, policy[mdp.state])</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.setterminalstates!" href="#ReinforcementLearningEnvironmentDiscrete.setterminalstates!"><code>ReinforcementLearningEnvironmentDiscrete.setterminalstates!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">setterminalstates!(mdp, range)</code></pre><p>Sets <code>mdp.isterminal[range] .= 1</code>, empties the table of transition probabilities for terminal states and sets the reward for all actions in the terminal state to the same value.</p></div></div></section><pre><code class="language-none">ReinforcementLearningEnvironmentDiscrete.setwall!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironmentDiscrete.treeMDP" href="#ReinforcementLearningEnvironmentDiscrete.treeMDP"><code>ReinforcementLearningEnvironmentDiscrete.treeMDP</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">treeMDP(na, depth; init = &quot;random&quot;, branchingfactor = 3)</code></pre><p>Returns a tree structured MDP with na actions and <code>depth</code> of the tree. If <code>init</code> is random, the <code>branchingfactor</code> determines how many possible states a (action, state) pair has. If <code>init = &quot;deterministic&quot;</code> the <code>branchingfactor = na</code>.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
