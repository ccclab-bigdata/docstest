<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · BoltzmannMachines.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>BoltzmannMachines.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Types-of-Boltzmann-Machines-1">Types of Boltzmann Machines</a></li><li><a class="toctext" href="#Overview-of-functions-1">Overview of functions</a></li><li><a class="toctext" href="#Examples-1">Examples</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="BoltzmannMachines.jl-1" href="#BoltzmannMachines.jl-1">BoltzmannMachines.jl</a></h1><p><a href="https://travis-ci.org/stefan-m-lenz/BoltzmannMachines.jl"><img src="https://travis-ci.org/stefan-m-lenz/BoltzmannMachines.jl.svg?branch=master" alt="Build Status"/></a> &lt;!–<a href="https://coveralls.io/github/stefan-m-lenz/BoltzmannMachines.jl?branch=master"><img src="https://coveralls.io/repos/github/stefan-m-lenz/BoltzmannMachines.jl/badge.svg?branch=master" alt="Coverage Status"/></a>–&gt;</p><p>This Julia package implements algorithms for training and evaluating several types of Boltzmann Machines (BMs):</p><ul><li>Learning of Restricted Boltzmann Machines (RBMs) using Contrastive Divergence (CD)</li><li>Greedy layerwise pre-training of Deep Boltzmann Machines (DBMs)</li><li>Learning procedure for general Boltzmann Machines using mean-field inference and stochastic approximation. Applicable to DBMs and used for fine-tuning the weights after the pre-training</li><li>Exact calculation of the likelihood of BMs (only suitable for small models)</li><li>Annealed Importance Sampling (AIS) for estimating the likelihood of larger BMs</li></ul><h2><a class="nav-anchor" id="Types-of-Boltzmann-Machines-1" href="#Types-of-Boltzmann-Machines-1">Types of Boltzmann Machines</a></h2><h3><a class="nav-anchor" id="Restricted-Boltzmann-Machines-1" href="#Restricted-Boltzmann-Machines-1">Restricted Boltzmann Machines</a></h3><p>The package contains the following types of RBMs (subtypes of <code>AbstractRBM</code>):</p><table><tr><th>Type</th><th>Distribution of visible units</th><th>Distribution of hidden units</th></tr><tr><td><code>BernoulliRBM</code></td><td>Bernoulli</td><td>Bernoulli</td></tr><tr><td><code>Softmax0BernoulliRBM</code></td><td>Categorical (binary encoded)</td><td>Bernoulli</td></tr><tr><td><code>GaussianBernoulliRBM</code>, <code>GaussianBernoulliRBM2</code> ([6])</td><td>Gaussian</td><td>Bernoulli</td></tr><tr><td><code>Binomial2BernoulliRBM</code></td><td>Binomial distribution with n = 2</td><td>Bernoulli</td></tr><tr><td><code>BernoulliGaussianRBM</code></td><td>Bernoulli</td><td>Gaussian</td></tr></table><h3><a class="nav-anchor" id="(Multimodal)-Deep-Boltzmann-Machines-1" href="#(Multimodal)-Deep-Boltzmann-Machines-1">(Multimodal) Deep Boltzmann Machines</a></h3><p>DBMs are implemented as vectors of RBMs. <code>BasicDBM</code>s have only Bernoulli distributed nodes and therefore consist of a vector of <code>BernoulliRBM</code>s. DBMs with different types of visible units can be constructed by using the corresponding RBM type in the first layer. Actual <code>MultimodalDBM</code>s can be formed by using <code>PartitionedRBM</code>s, which is a type of <code>AbstractRBM</code> that is able to encapsulate non-connected RBMs of different types into an RBM-like layer.</p><p>All these types of DBMs can be trained using layerwise pre-training and fine-tuning employing the mean-field approximation. It is also possible to estimate or calculate the likelihood for these DBM types.</p><h2><a class="nav-anchor" id="Overview-of-functions-1" href="#Overview-of-functions-1">Overview of functions</a></h2><p>The following tables provide an overview of the functions of the package, together with a short description. You can find more detailed descriptions for each function using the Julia help mode (entered by typing <code>?</code> at the beginning of the Julia command prompt).</p><h3><a class="nav-anchor" id="Data-preprocessing-1" href="#Data-preprocessing-1">Data preprocessing</a></h3><p>Continuously valued data or ordinal data can be transformed into probabilities via <code>intensities</code> and then fed to <code>BernoulliRBM</code>s, like it is usually done when handling grayscale or color intensities in images.</p><p>Categorical data can be binary encoded as input for a <code>Softmax0BernoulliRBM</code> via <code>oneornone_encode</code>.</p><h3><a class="nav-anchor" id="Functions-for-Training-1" href="#Functions-for-Training-1">Functions for Training</a></h3><h4><a class="nav-anchor" id="Training-of-RBMs-1" href="#Training-of-RBMs-1">Training of RBMs</a></h4><table><tr><th>Function name</th><th>Short description</th></tr><tr><td><code>initrbm</code></td><td>Initializes an RBM model.</td></tr><tr><td><code>trainrbm!</code></td><td>Performs CD-learning on an RBM model.</td></tr><tr><td><code>fitrbm</code></td><td>Fits a RBM model to a dataset using CD. (Wraps <code>initrbm</code> and <code>trainrbm!</code>)</td></tr><tr><td><code>samplevisible</code>, <code>samplevisible!</code> (<code>samplehidden</code>, <code>samplehidden!</code>)</td><td>Gibbs sampling of visible (hidden) nodes&#39; states given the hidden (visible) nodes&#39; states in an RBM.</td></tr><tr><td><code>visiblepotential</code>, <code>visiblepotential!</code> (<code>hiddenpotential</code>, <code>hiddenpotential!</code>)</td><td>Computes the deterministic potential for the activation of the visible (hidden) nodes of an RBM.</td></tr><tr><td><code>visibleinput</code>, <code>visibleinput!</code> (<code>hiddeninput</code>, <code>hiddeninput!</code>)</td><td>Computes the total input received by the visible (hidden) layer of an RBM.</td></tr></table><h4><a class="nav-anchor" id="Training-of-DBMs-1" href="#Training-of-DBMs-1">Training of DBMs</a></h4><table><tr><th>Function name</th><th>Short description</th></tr><tr><td><code>fitdbm</code></td><td>Fits a DBM model to a dataset. This includes pre-training, followed by the general Boltzmann Machine learning procedure for fine-tuning.</td></tr><tr><td><code>gibbssample!</code></td><td>Performs Gibbs sampling in a DBM.</td></tr><tr><td><code>meanfield</code></td><td>Computes the mean-field inference of the hidden nodes&#39; activations in a DBM.</td></tr><tr><td><code>stackrbms</code></td><td>Greedy layerwise pre-training of a DBM model or a Deep Belief Network.</td></tr><tr><td><code>traindbm!</code></td><td>Trains a DBM using the learning procedure for a general Boltzmann Machine.</td></tr></table><h3><a class="nav-anchor" id="Partitioned-training-and-joining-of-models-1" href="#Partitioned-training-and-joining-of-models-1">Partitioned training and joining of models</a></h3><p>The following functions may be used to join models fitted on partitioned data sets. The weights cross-linking the models are initialized with zeros.</p><table><tr><th>Function name</th><th>Short description</th></tr><tr><td><code>joindbms</code></td><td>Joins two or more DBM models together.</td></tr><tr><td><code>joinrbms</code></td><td>Joins two or more RBM models to form a joint RBM model of the same type.</td></tr></table><h3><a class="nav-anchor" id="Functions-for-evaluating-a-trained-model-1" href="#Functions-for-evaluating-a-trained-model-1">Functions for evaluating a trained model</a></h3><table><tr><th>Function name</th><th>Short description</th></tr><tr><td><code>aislogimpweights</code></td><td>Performs AIS on a BM and calculates the logarithmised importance weights for estimating the BM&#39;s partition function.</td></tr><tr><td><code>freeenergy</code></td><td>Computes the mean free energy of a data set in an RBM model.</td></tr><tr><td><code>loglikelihood</code></td><td>Estimates the mean loglikelihood of a dataset in a BM model using AIS.</td></tr><tr><td><code>logpartitionfunction</code></td><td>Estimates the log of the partition function of a BM.</td></tr><tr><td><code>logproblowerbound</code></td><td>Estimates the mean lower bound of the log probability of a dataset in a DBM model.</td></tr><tr><td><code>reconstructionerror</code></td><td>Computes the mean reconstruction error of a dataset in an RBM model.</td></tr><tr><td><code>samples</code></td><td>Generates samples from the distribution defined by a BM model. (See also <code>gibbssample!</code> and <code>gibbsamplecond!</code> for (conditional) Gibbs sampling.)</td></tr></table><h3><a class="nav-anchor" id="Monitoring-the-learning-process-1" href="#Monitoring-the-learning-process-1">Monitoring the learning process</a></h3><p>The functions of the form <code>monitor*!</code> can be used for monitoring a property of the model during the learning process. The following words, corresponding to the denominated properties, may stand in place of <code>*</code>:</p><ul><li><code>freeenergy</code></li><li><code>exactloglikelihood</code></li><li><code>loglikelihood</code></li><li><code>logproblowerbound</code></li><li><code>reconstructionerror</code></li><li><code>weightsnorm</code></li></ul><p>The results of evaluations are stored in <code>Monitor</code> objects. The evaluations can be plotted by calling the function <code>plotevaluation</code> in the submodule <code>BMPlots</code> as <code>BMPlots.plotevaluation(monitor, key)</code>, with the key being one of the constants <code>monitor*</code> defined in the module.</p><p>For intended usage of these functions, best see the <a href="test/examples.jl">examples</a>.</p><h2><a class="nav-anchor" id="Examples-1" href="#Examples-1">Examples</a></h2><p>You can find <a href="test/examples.jl">example code here</a>.</p><p>If you want to use the plotting functionality, you need to install the package <a href="https://github.com/stefan-m-lenz/BoltzmannMachinesPlots.jl"><code>BoltzmannMachinesPlots</code></a> in addition.</p><h3><a class="nav-anchor" id="Applications-1" href="#Applications-1">Applications</a></h3><p>The package has been used for an approach to uncover patterns in high-dimensional genetic data, described in the article</p><blockquote><p>Hess M., Lenz S., Blätte T. J., Bullinger L., Binder H. <em>Partitioned learning of deep Boltzmann machines for SNP data</em>. Bioinformatics 2017 btx408. doi: https://doi.org/10.1093/bioinformatics/btx408.</p></blockquote><p>The code for the analyses presented there is available in the article supplement.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><p>[1] Salakhutdinov, R. (2015). <em>Learning Deep Generative Models</em>. Annual Review of Statistics and Its Application, 2, 361-385.</p><p>[2] Salakhutdinov, R. Hinton, G. (2012). <em>An Efficient Learning Procedure for Deep Boltzmann Machines</em>. Neural computation, 24(8), 1967-2006.</p><p>[3] Salakhutdinov. R. (2008). <em>Learning and Evaluating Boltzmann Machines</em>. Technical Report UTML TR 2008-002, Department of Computer Science, University of Toronto.</p><p>[4] Krizhevsky, A., Hinton, G. (2009). <em>Learning Multiple Layers of Features from Tiny Images</em>.</p><p>[5] Srivastava, N., Salakhutdinov R. (2014). <em>Multimodal Learning with Deep Boltzmann Machines</em>. Journal of Machine Learning Research, 15, 2949-2980.</p><p>[6] Cho, K., Ilin A., Raiko, T. (2011) <em>Improved learning of Gaussian-Bernoulli restricted Boltzmann machines</em>. Artificial Neural Networks and Machine Learning – ICANN 2011.</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
