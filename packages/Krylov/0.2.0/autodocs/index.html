<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · Krylov.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Krylov.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><pre><code class="language-none">Krylov.@kaxpby!</code></pre><pre><code class="language-none">Krylov.@kaxpy!</code></pre><pre><code class="language-none">Krylov.@kdot</code></pre><pre><code class="language-none">Krylov.@knrm2</code></pre><pre><code class="language-none">Krylov.@kscal!</code></pre><pre><code class="language-none">Krylov.Krylov</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.KrylovStats" href="#Krylov.KrylovStats"><code>Krylov.KrylovStats</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Abstract type for statistics returned by a solver</p></div></div></section><pre><code class="language-none">Krylov.LanczosStats</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.SimpleStats" href="#Krylov.SimpleStats"><code>Krylov.SimpleStats</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Type for statistics returned by non-Lanczos solvers</p></div></div></section><pre><code class="language-none">Krylov.SymmlqStats</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cg" href="#Krylov.cg"><code>Krylov.cg</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>The conjugate gradient method to solve the symmetric linear system Ax=b.</p><p>The method does <em>not</em> abort if A is not definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cg_lanczos" href="#Krylov.cg_lanczos"><code>Krylov.cg_lanczos</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>The Lanczos version of the conjugate gradient method to solve the symmetric linear system</p><p>Ax = b</p><p>The method does <em>not</em> abort if A is not definite.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cg_lanczos_shift_seq" href="#Krylov.cg_lanczos_shift_seq"><code>Krylov.cg_lanczos_shift_seq</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>The Lanczos version of the conjugate gradient method to solve a family of shifted systems</p><p>(A + αI) x = b  (α = α₁, α₂, ...)</p><p>The method does <em>not</em> abort if A + αI is not definite.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cgls" href="#Krylov.cgls"><code>Krylov.cgls</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the regularized linear least-squares problem</p><p>minimize ‖b - Ax‖₂² + λ ‖x‖₂²</p><p>using the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations</p><p>(A&#39;A + λI) x = A&#39;b</p><p>but is more stable.</p><p>CGLS produces monotonic residuals ‖r‖₂ but not optimality residuals ‖A&#39;r‖₂. It is formally equivalent to LSQR, though can be slightly less accurate, but simpler to implement.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cgne" href="#Krylov.cgne"><code>Krylov.cgne</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system</p><p>Ax + √λs = b</p><p>using the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations of the second kind</p><p>(AA&#39; + λI) y = b</p><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><p>min ‖x‖₂  s.t. Ax = b.</p><p>When λ &gt; 0, it solves the problem</p><p>min ‖(x,s)‖₂  s.t. Ax + √λs = b.</p><p>CGNE produces monotonic errors ‖x-x*‖₂ but not residuals ‖r‖₂. It is formally equivalent to CRAIG, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.</p><p>A preconditioner M may be provided in the form of a linear operator.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cgs" href="#Krylov.cgs"><code>Krylov.cgs</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system Ax = b using conjugate gradient squared algorithm.</p><p>From &quot;Iterative Methods for Sparse Linear Systems (Y. Saad)&quot; :</p><p>«The method is based on a polynomial variant of the conjugate gradients algorithm. Although related to the so-called bi-conjugate gradients (BCG) algorithm, it does not involve adjoint matrix-vector multiplications, and the expected convergence rate is about twice that of the BCG algorithm.</p><p>The Conjugate Gradient Squared algorithm works quite well in many cases. However, one difficulty is that, since the polynomials are squared, rounding errors tend to be more damaging than in the standard BCG algorithm. In particular, very high variations of the residual vectors often cause the residual norms computed to become inaccurate.</p><p>TFQMR and BICGSTAB were developed to remedy this difficulty.»</p><p>This implementation allows a right preconditioner M.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.cr" href="#Krylov.cr"><code>Krylov.cr</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>A truncated version of Stiefel’s Conjugate Residual method to solve the symmetric linear system Ax=b. The matrix A must be positive semi-definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. In a linesearch context, &#39;linesearch&#39; must be set to &#39;true&#39;.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.craig" href="#Krylov.craig"><code>Krylov.craig</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Find the least-norm solution of the consistent linear system</p><p>Ax + √λs = b</p><p>using the Golub-Kahan implementation of Craig&#39;s method, where λ ≥ 0 is a regularization parameter. This method is equivalent to CGNE but is more stable.</p><p>For a system in the form Ax = b, Craig&#39;s method is equivalent to applying CG to AA&#39;y = b and recovering x = A&#39;y. Note that y are the Lagrange multipliers of the least-norm problem</p><p>minimize ‖x‖  subject to Ax = b.</p><p>In this implementation, both the x and y-parts of the solution are returned.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.craigmr" href="#Krylov.craigmr"><code>Krylov.craigmr</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system</p><p>Ax + √λs = b</p><p>using the CRAIG-MR method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying the Conjugate Residuals method to the normal equations of the second kind</p><p>(AA&#39; + λI) y = b</p><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><p>min ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.</p><p>When λ &gt; 0, this method solves the problem</p><p>min ‖(x,s)‖₂  s.t. Ax + √λs = b.</p><p>CRAIGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRMR, though can be slightly more accurate, and intricate to implement. Both the x- and y-parts of the solution are returned.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.crls" href="#Krylov.crls"><code>Krylov.crls</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the linear least-squares problem</p><p>minimize ‖b - Ax‖₂² + λ ‖x‖₂²</p><p>using the Conjugate Residuals (CR) method. This method is equivalent to applying MINRES to the normal equations</p><p>(A&#39;A + λI) x = A&#39;b.</p><p>This implementation recurs the residual r := b - Ax.</p><p>CRLS produces monotonic residuals ‖r‖₂ and optimality residuals ‖A&#39;r‖₂. It is formally equivalent to LSMR, though can be slightly less accurate, but simpler to implement.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.crmr" href="#Krylov.crmr"><code>Krylov.crmr</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system</p><p>Ax + √λs = b</p><p>using the Conjugate Residual (CR) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CR to the normal equations of the second kind</p><p>(AA&#39; + λI) y = b</p><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><p>min ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.</p><p>When λ &gt; 0, this method solves the problem</p><p>min ‖(x,s)‖₂  s.t. Ax + √λs = b.</p><p>CGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRAIG-MR, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.</p><p>A preconditioner M may be provided in the form of a linear operator.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.diom" href="#Krylov.diom"><code>Krylov.diom</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system Ax = b using direct incomplete orthogonalization method.</p><p>DIOM is similar to CG with partial reorthogonalization.</p><p>An advantage of DIOM is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by this single algorithm.</p><p>This implementation allows a right preconditioner M.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.dqgmres" href="#Krylov.dqgmres"><code>Krylov.dqgmres</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the consistent linear system Ax = b using DQGMRES method.</p><p>DQGMRES algorithm is based on the incomplete Arnoldi orthogonalization process and computes a sequence of approximate solutions with the quasi-minimal residual property.</p><p>This implementation allows a right preconditioner M.</p></div></div></section><pre><code class="language-none">Krylov.eval</code></pre><pre><code class="language-none">Krylov.include</code></pre><pre><code class="language-none">Krylov.krylov_axpby!</code></pre><pre><code class="language-none">Krylov.krylov_axpy!</code></pre><pre><code class="language-none">Krylov.krylov_dot</code></pre><pre><code class="language-none">Krylov.krylov_norm2</code></pre><pre><code class="language-none">Krylov.krylov_scal!</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.lslq" href="#Krylov.lslq"><code>Krylov.lslq</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">lslq(A, b, λ=0.0)</code></pre><p>Solve the regularized linear least-squares problem</p><pre><code class="language-none">minimize ‖b - Ax‖₂² + λ² ‖x‖₂²</code></pre><p>using the LSLQ method, where λ ≥ 0 is a regularization parameter. LSLQ is formally equivalent to applying SYMMLQ to the normal equations</p><pre><code class="language-none">(A&#39;A + λ² I) x = A&#39;b</code></pre><p>but is more stable.</p><p><strong>Main features</strong></p><ul><li>the solution estimate is updated along orthogonal directions</li><li>the norm of the solution estimate ‖xᴸₖ‖₂ is increasing</li><li>the error ‖eₖ‖₂ := ‖xᴸₖ - x*‖₂ is decreasing</li><li>it is possible to transition cheaply from the LSLQ iterate to the LSQR iterate if there is an advantage (there always is in terms of error)</li><li>if <code>A</code> is rank deficient, identify the minimum least-squares solution</li></ul><p><strong>Input arguments</strong></p><ul><li><code>A::AbstractLinearOperator</code></li><li><code>b::Vector{Float64}</code></li></ul><p><strong>Optional arguments</strong></p><ul><li><p><code>M::AbstractLinearOperator=opEye(size(A,1))</code>: a symmetric and positive definite dual preconditioner</p></li><li><p><code>N::AbstractLinearOperator=opEye(size(A,2))</code>: a symmetric and positive definite primal preconditioner</p></li><li><p><code>sqd::Bool=false</code> indicates whether or not we are solving a symmetric and quasi-definite augmented system If <code>sqd = true</code>, we solve the symmetric and quasi-definite system</p><pre><code class="language-none">[ E   A&#39; ] [ r ]   [ b ]
[ A  -F  ] [ x ] = [ 0 ],</code></pre><p>where E = M⁻¹  and F = N⁻¹.</p><p>If <code>sqd = false</code>, we solve the symmetric and indefinite system</p><pre><code class="language-none">[ E   A&#39; ] [ r ]   [ b ]
[ A   0  ] [ x ] = [ 0 ].</code></pre><p>In this case, <code>N</code> can still be specified and indicates the norm in which <code>x</code> and the forward error should be measured.</p></li><li><p><code>λ::Float64=0.0</code> is a regularization parameter (see the problem statement above)</p></li><li><p><code>σ::Float64=0.0</code> is an underestimate of the smallest nonzero singular value of <code>A</code>–-setting <code>σ</code> too large will result in an error in the course of the iterations</p></li><li><p><code>atol::Float64=1.0e-8</code> is a stopping tolerance based on the residual</p></li><li><p><code>btol::Float64=1.0e-8</code> is a stopping tolerance used to detect zero-residual problems</p></li><li><p><code>etol::Float64=1.0e-8</code> is a stopping tolerance based on the lower bound on the error</p></li><li><p><code>window::Int=5</code> is the number of iterations used to accumulate a lower bound on the error</p></li><li><p><code>utol::Float64=1.0e-8</code> is a stopping tolerance based on the upper bound on the error</p></li><li><p><code>itmax::Int=0</code> is the maximum number of iterations (0 means no imposed limit)</p></li><li><p><code>conlim::Float64=1.0e+8</code> is the limit on the estimated condition number of <code>A</code> beyond which the solution will be abandoned</p></li><li><p><code>verbose::Bool=false</code> determines verbosity.</p></li></ul><p><strong>Return values</strong></p><p><code>lslq()</code> returns the tuple <code>(x_lq, x_cg, err_lbnds, err_ubnds_lq, err_ubnds_cg, stats)</code> where</p><ul><li><code>x_lq::Vector{Float64}</code> is the LQ solution estimate</li><li><code>x_cg::Vector{Float64}</code> is the CG solution estimate (i.e., the LSQR point)</li><li><code>err_lbnds::Vector{Float64}</code> is a vector of lower bounds on the LQ error–-the vector is empty if <code>window</code> is set to zero</li><li><code>err_ubnds_lq::Vector{Float64}</code> is a vector of upper bounds on the LQ error–-the vector is empty if <code>σ == 0</code> is left at zero</li><li><code>err_ubnds_cg::Vector{Float64}</code> is a vector of upper bounds on the CG error–-the vector is empty if <code>σ == 0</code> is left at zero</li><li><code>stats::SimpleStats</code> collects other statistics on the run.</li></ul><p><strong>Stopping conditions</strong></p><p>The iterations stop as soon as one of the following conditions holds true:</p><ul><li>the optimality residual is sufficiently small (<code>stats.status = &quot;found approximate minimum least-squares solution&quot;</code>) in the sense that either<ul><li>‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ atol, or</li><li>1 + ‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ 1</li></ul></li><li>an approximate zero-residual solution has been found (<code>stats.status = &quot;found approximate zero-residual solution&quot;</code>) in the sense that either<ul><li>‖r‖ / ‖b‖ ≤ btol + atol ‖A‖ * ‖xᴸ‖ / ‖b‖, or</li><li>1 + ‖r‖ / ‖b‖ ≤ 1</li></ul></li><li>the estimated condition number of <code>A</code> is too large in the sense that either<ul><li>1/cond(A) ≤ 1/conlim (<code>stats.status = &quot;condition number exceeds tolerance&quot;</code>), or</li><li>1 + 1/cond(A) ≤ 1 (<code>stats.status = &quot;condition number seems too large for this machine&quot;</code>)</li></ul></li><li>the lower bound on the LQ forward error is less than etol * ‖xᴸ‖</li><li>the upper bound on the CG forward error is less than utol * ‖xᶜ‖</li></ul><p><strong>References</strong></p><ul><li>R. Estrin, D. Orban and M. A. Saunders, <em>Estimates of the 2-Norm Forward Error for SYMMLQ and CG</em>, Cahier du GERAD G-2016-70, GERAD, Montreal, 2016. DOI http://dx.doi.org/10.13140/RG.2.2.19581.77288.</li><li>R. Estrin, D. Orban and M. A. Saunders, <em>LSLQ: An Iterative Method for Linear Least-Squares with an Error Minimization Property</em>, Cahier du GERAD G-2017-xx, GERAD, Montreal, 2017.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.lsmr" href="#Krylov.lsmr"><code>Krylov.lsmr</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the regularized linear least-squares problem</p><p>minimize ‖b - Ax‖₂² + λ² ‖x‖₂²</p><p>using the LSMR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying MINRES to the normal equations</p><p>(A&#39;A + λ² I) x = A&#39;b</p><p>(and therefore to CRLS) but is more stable.</p><p>LSMR produces monotonic residuals ‖r‖₂ and optimality residuals ‖A&#39;r‖₂. rt is formally equivalent to CRLS, though can be slightly more accurate.</p><p>Preconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If <code>sqd</code> is set to <code>true</code>, we solve the symmetric and quasi-definite system</p><p>[ E   A&#39; ] [ r ]   [ b ]   [ A  -F  ] [ x ] = [ 0 ],</p><p>where E = M⁻¹  and F = N⁻¹.</p><p>If <code>sqd</code> is set to <code>false</code> (the default), we solve the symmetric and indefinite system</p><p>[ E   A&#39; ] [ r ]   [ b ]   [ A   0  ] [ x ] = [ 0 ].</p><p>In this case, <code>N</code> can still be specified and indicates the norm in which <code>x</code> should be measured.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.lsqr" href="#Krylov.lsqr"><code>Krylov.lsqr</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the regularized linear least-squares problem</p><p>minimize ‖b - Ax‖₂² + λ² ‖x‖₂²</p><p>using the LSQR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying CG to the normal equations</p><p>(A&#39;A + λ² I) x = A&#39;b</p><p>(and therefore to CGLS) but is more stable.</p><p>LSQR produces monotonic residuals ‖r‖₂ but not optimality residuals ‖A&#39;r‖₂. It is formally equivalent to CGLS, though can be slightly more accurate.</p><p>Preconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If <code>sqd</code> is set to <code>true</code>, we solve the symmetric and quasi-definite system</p><p>[ E   A&#39; ] [ r ]   [ b ]   [ A  -F  ] [ x ] = [ 0 ],</p><p>where E = M⁻¹  and F = N⁻¹.</p><p>If <code>sqd</code> is set to <code>false</code> (the default), we solve the symmetric and indefinite system</p><p>[ E   A&#39; ] [ r ]   [ b ]   [ A   0  ] [ x ] = [ 0 ].</p><p>In this case, <code>N</code> can still be specified and indicates the norm in which <code>x</code> should be measured.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.minres" href="#Krylov.minres"><code>Krylov.minres</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the shifted linear least-squares problem</p><p>minimize ‖b - (A + λ I)x‖₂²</p><p>or the shifted linear system</p><p>(A + λ I) x = b</p><p>using the MINRES method, where λ ≥ 0 is a shift parameter, where A is square and symmetric.</p><p>MINRES is formally equivalent to applying CR to Ax=b when A is positive definite, but is typically more stable and also applies to the case where A is indefinite.</p><p>MINRES produces monotonic residuals ‖r‖₂ and optimality residuals ‖A&#39;r‖₂.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p></div></div></section><pre><code class="language-none">Krylov.preallocated_LinearOperator</code></pre><pre><code class="language-none">Krylov.preallocated_symmetric_LinearOperator</code></pre><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.roots_quadratic" href="#Krylov.roots_quadratic"><code>Krylov.roots_quadratic</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Find the real roots of the quadratic</p><pre><code class="language-none">q(x) = q₂ x² + q₁ x + q₀,</code></pre><p>where q₂, q₁ and q₀ are real. Care is taken to avoid numerical cancellation. Optionally, <code>nitref</code> steps of iterative refinement may be performed to improve accuracy. By default, <code>nitref=1</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.sym_givens" href="#Krylov.sym_givens"><code>Krylov.sym_givens</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Numerically stable symmetric Givens reflection. Given <code>a</code> and <code>b</code>, return <code>(c, s, ρ)</code> such that</p><pre><code class="language-none">[ c  s ] [ a ] = [ ρ ]
[ s -c ] [ b ] = [ 0 ].</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.symmlq" href="#Krylov.symmlq"><code>Krylov.symmlq</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Solve the shifted linear system</p><p>(A + λ I) x = b</p><p>using the SYMMLQ method, where λ is a shift parameter, and A is square and symmetric.</p><p>SYMMLQ produces monotonic errors ‖x*-x‖₂.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Krylov.to_boundary" href="#Krylov.to_boundary"><code>Krylov.to_boundary</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Given a trust-region radius <code>radius</code>, a vector <code>x</code> lying inside the trust-region and a direction <code>d</code>, return <code>σ1</code> and <code>σ2</code> such that</p><pre><code class="language-none">‖x + σi d‖ = radius, i = 1, 2</code></pre><p>in the Euclidean norm. If known, ‖x‖² may be supplied in <code>xNorm2</code>.</p><p>If <code>flip</code> is set to <code>true</code>, <code>σ1</code> and <code>σ2</code> are computed such that</p><pre><code class="language-none">‖x - σi d‖ = radius, i = 1, 2.</code></pre></div></div></section><pre><code class="language-none">Krylov.vec2str</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
