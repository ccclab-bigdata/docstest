<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme Â· ClusterManagers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ClusterManagers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Currently-supported-job-queue-systems-1">Currently supported job queue systems</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="ClusterManagers-1" href="#ClusterManagers-1">ClusterManagers</a></h1><p>Support for different job queue systems commonly used on compute clusters.</p><h2><a class="nav-anchor" id="Currently-supported-job-queue-systems-1" href="#Currently-supported-job-queue-systems-1">Currently supported job queue systems</a></h2><table><tr><th>Job queue system</th><th>Command to add processors</th></tr><tr><td>Sun Grid Engine</td><td><code>addprocs_sge(np::Integer, queue=&quot;&quot;)</code> or <code>addprocs(SGEManager(np, queue))</code></td></tr><tr><td>PBS</td><td><code>addprocs_pbs(np::Integer, queue=&quot;&quot;)</code> or <code>addprocs(PBSManager(np, queue))</code></td></tr><tr><td>Scyld</td><td><code>addprocs_scyld(np::Integer)</code> or <code>addprocs(ScyldManager(np))</code></td></tr><tr><td>HTCondor</td><td><code>addprocs_htc(np::Integer)</code> or <code>addprocs(HTCManager(np))</code></td></tr><tr><td>Slurm</td><td><code>addprocs_slurm(np::Integer; kwargs...)</code> or <code>addprocs(SlurmManager(np); kwargs...)</code></td></tr><tr><td>Local manager with CPU affinity setting</td><td><code>addprocs(LocalAffinityManager(;np=CPU_CORES, mode::AffinityMode=BALANCED, affinities=[]); kwargs...)</code></td></tr></table><p>You can also write your own custom cluster manager; see the instructions in the <a href="https://docs.julialang.org/en/latest/manual/parallel-computing/#ClusterManagers-1">Julia manual</a></p><h3><a class="nav-anchor" id="Slurm:-a-simple-example-1" href="#Slurm:-a-simple-example-1">Slurm: a simple example</a></h3><pre><code class="language-jl">using ClusterManagers

# Arguments to the Slurm srun(1) command can be given as keyword
# arguments to addprocs.  The argument name and value is translated to
# a srun(1) command line argument as follows:
# 1) If the length of the argument is 1 =&gt; &quot;-arg value&quot;,
#    e.g. t=&quot;0:1:0&quot; =&gt; &quot;-t 0:1:0&quot;
# 2) If the length of the argument is &gt; 1 =&gt; &quot;--arg=value&quot;
#    e.g. time=&quot;0:1:0&quot; =&gt; &quot;--time=0:1:0&quot;
# 3) If the value is the empty string, it becomes a flag value,
#    e.g. exclusive=&quot;&quot; =&gt; &quot;--exclusive&quot;
# 4) If the argument contains &quot;_&quot;, they are replaced with &quot;-&quot;,
#    e.g. mem_per_cpu=100 =&gt; &quot;--mem-per-cpu=100&quot;
addprocs(SlurmManager(2), partition=&quot;debug&quot;, t=&quot;00:5:00&quot;)

hosts = []
pids = []
for i in workers()
	host, pid = fetch(@spawnat i (gethostname(), getpid()))
	push!(hosts, host)
	push!(pids, pid)
end

# The Slurm resource allocation is released when all the workers have
# exited
for i in workers()
	rmprocs(i)
end</code></pre><h3><a class="nav-anchor" id="SGE-a-simple-interactive-example-1" href="#SGE-a-simple-interactive-example-1">SGE - a simple interactive example</a></h3><pre><code class="language-jl">julia&gt; using ClusterManagers

julia&gt; ClusterManagers.addprocs_sge(5)
job id is 961, waiting for job to start .
5-element Array{Any,1}:
2
3
4
5
6

julia&gt; @parallel for i=1:5
       run(`hostname`)
       end

julia&gt;  From worker 2:  compute-6
        From worker 4:  compute-6
        From worker 5:  compute-6
        From worker 6:  compute-6
        From worker 3:  compute-6</code></pre><h3><a class="nav-anchor" id="SGE-an-example-with-resource-list-1" href="#SGE-an-example-with-resource-list-1">SGE - an example with resource list</a></h3><p>Some clusters require the user to specify a list of required resources. For example, it may be necessary to specify how much memory will be needed by the job - see this <a href="https://github.com/JuliaLang/julia/issues/10390">issue</a>.</p><pre><code class="language-jl">julia&gt; using ClusterManagers

julia&gt; addprocs_sge(5,res_list=&quot;h_vmem=4G,tmem=4G&quot;)
job id is 9827051, waiting for job to start ........
5-element Array{Int64,1}:
 22
 23
 24
 25
 26

julia&gt; pmap(x-&gt;run(`hostname`),workers());

julia&gt;  From worker 26: lum-7-2.local
        From worker 23: pace-6-10.local
        From worker 22: chong-207-10.local
        From worker 24: pace-6-11.local
        From worker 25: cheech-207-16.local</code></pre><h3><a class="nav-anchor" id="Using-LocalAffinityManager-(for-pinning-local-workers-to-specific-cores)-1" href="#Using-LocalAffinityManager-(for-pinning-local-workers-to-specific-cores)-1">Using <code>LocalAffinityManager</code> (for pinning local workers to specific cores)</a></h3><ul><li>Linux only feature.</li><li>Requires the Linux <code>taskset</code> command to be installed.</li><li>Usage : <code>addprocs(LocalAffinityManager(;np=CPU_CORES, mode::AffinityMode=BALANCED, affinities=[]); kwargs...)</code>.</li></ul><p>where</p><ul><li><code>np</code> is the number of workers to be started.</li><li><code>affinities</code>, if specified, is a list of CPU IDs. As many workers as entries in <code>affinities</code> are launched. Each worker is pinned</li></ul><p>to the specified CPU ID.</p><ul><li><code>mode</code> (used only when <code>affinities</code> is not specified, can be either <code>COMPACT</code> or <code>BALANCED</code>) - <code>COMPACT</code> results in the requested number</li></ul><p>of workers pinned to cores in increasing order, For example, worker1 =&gt; CPU0, worker2 =&gt; CPU1 and so on. <code>BALANCED</code> tries to spread the workers. Useful when we have multiple CPU sockets, with each socket having multiple cores. A <code>BALANCED</code> mode results in workers spread across CPU sockets. Default is <code>BALANCED</code>.</p><h3><a class="nav-anchor" id="Using-ElasticManager-(dynamically-adding-workers-to-a-cluster)-1" href="#Using-ElasticManager-(dynamically-adding-workers-to-a-cluster)-1">Using <code>ElasticManager</code> (dynamically adding workers to a cluster)</a></h3><p>The <code>ElasticManager</code> is useful in scenarios where we want to dynamically add workers to a cluster. It achieves this by listening on a known port on the master. The launched workers connect to this port and publish their own host/port information for other workers to connect to.</p><h5><a class="nav-anchor" id="Usage-1" href="#Usage-1">Usage</a></h5><p>On the master, you need to instantiate an instance of <code>ElasticManager</code>. The constructors defined are:</p><pre><code class="language-jl">ElasticManager(;addr=IPv4(&quot;127.0.0.1&quot;), port=9009, cookie=nothing, topology=:all_to_all)
ElasticManager(port) = ElasticManager(;port=port)
ElasticManager(addr, port) = ElasticManager(;addr=addr, port=port)
ElasticManager(addr, port, cookie) = ElasticManager(;addr=addr, port=port, cookie=cookie)</code></pre><p>On the worker, you need to call <code>ClusterManagers.elastic_worker</code> with the addr/port that the master is listening on and the same cookie. <code>elastic_worker</code> is defined as:</p><pre><code class="language-none">ClusterManagers.elastic_worker(cookie, addr=&quot;127.0.0.1&quot;, port=9009; stdout_to_master=true)</code></pre><p>For example, on the master:</p><pre><code class="language-jl">using ClusterManagers
em=ElasticManager(cookie=&quot;foobar&quot;)</code></pre><p>and launch each worker locally as <code>echo &quot;using ClusterManagers; ClusterManagers.elastic_worker(\&quot;foobar\&quot;)&quot; | julia  &amp;</code></p><p>or if you want a REPL on the worker, you can start a julia process normally and manually enter</p><pre><code class="language-jl">using ClusterManagers
@schedule ClusterManagers.elastic_worker(&quot;foobar&quot;, &quot;addr_of_master&quot;, port_of_master; stdout_to_master=false)</code></pre><p>The above will yield back the REPL prompt and also display any printed output locally.</p><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
