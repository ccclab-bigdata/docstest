var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Readme",
    "title": "Readme",
    "category": "page",
    "text": "<p align=\"center\">     <img src=\"https://github.com/phrb/NODAL.jl/blob/master/img/logo.svg\"          height=\"280\"> </p> <p align=\"center\">     <a href=\"https://badge.fury.io/gh/phrb%2FNODAL.jl\">         <img src=\"https://badge.fury.io/gh/phrb%2FNODAL.jl.svg\"              alt=\"Git Version\">     </a>     <a href=\"http://pkg.julialang.org/?pkg=StochasticSearch\">         <img src=\"http://pkg.julialang.org/badges/NODAL_0.7.svg\"              alt=\"Julia Package Version\">     </a>     <a href=\"https://travis-ci.org/phrb/NODAL.jl\">         <img src=\"https://travis-ci.org/phrb/NODAL.jl.svg\"              alt=\"Build Status\">     </a>     <a href=\"https://coveralls.io/r/phrb/NODAL.jl\">         <img src=\"https://coveralls.io/repos/phrb/NODAL.jl/badge.svg\"              alt=\"Coverage\">     </a>     <a href=\"http://nodal.readthedocs.org/en/latest/?badge=latest\">         <img src=\"https://readthedocs.org/projects/nodal/badge/?version=latest\"              alt=\"Documentation Status\">     </a> </p>NODAL provides tools for implementing parallel and distributed program autotuners.  This Julia package provides tools and optimization algorithms for implementing different Stochastic Local Search methods, such as Simulated Annealing and Tabu Search. NODAL is an ongoing project, and will implement more optimization and local search algorithms.You can use NODAL to optimize user-defined functions with a few Stochastic Local Search basic methods, that are composed by building blocks also provided in the package. The package distributes evaluations of functions and technique executions between Julia workers. It is possible to have multiple instances of search techniques running on the same problem."
},

{
    "location": "#Installing-1",
    "page": "Readme",
    "title": "Installing",
    "category": "section",
    "text": "NODAL runs on Julia nightly. From the Julia REPL, run:Pkg.add(\"NODAL\")If you want the latest version, which may be unstable, run instead:Pkg.clone(\"NODAL\")"
},

{
    "location": "#Documentation-1",
    "page": "Readme",
    "title": "Documentation",
    "category": "section",
    "text": "Please, refer to the documentation for more information and examples."
},

{
    "location": "#Example:-The-Rosenbrock-Function-1",
    "page": "Readme",
    "title": "Example: The Rosenbrock Function",
    "category": "section",
    "text": "The following is a very simple example, and you can find the source code for its latest version in the GitHub repository.We will optimize the Rosenbrock cost function. For this we must define a Configuration that represents the arguments to be tuned. We also have to create and configure a tuning run. First, let\'s import NODAL and define the cost function:addprocs()\n\nimport NODAL\n\n@everywhere begin\n    using NODAL\n    function rosenbrock(x::Configuration, parameters::Dict{Symbol, Any})\n        return (1.0 - x[\"i0\"].value)^2 + 100.0 * (x[\"i1\"].value - x[\"i0\"].value^2)^2\n    end\nendNote:The Rosenbrock function is by no means a good autotuning objetive, although it is a good tool to help you get familiar with the API. NODAL certainly performs worse than most tools for this kind of function.  Look at further examples is this page for more fitting applications.We use the addprocs() function to add the default number of Julia workers, one per processing core, to our application. The import statement loads NODAL in the current Julia worker, and the @everywhere macro defines the rosenbrock function and the module in all Julia workers available.Cost functions must accept a Configuration and a Dict{Symbol, Any} as input. The Configuration is used to define the autotuner\'s search space, and the parameter dictionary can store data or function configurations.Our cost function simply ignores the parameter dictionary, and uses the \"i0\" and \"i1\" parameters of the received configuration to calculate a value. There is no restriction on the names of Configuration parameter.Our configuration will have two FloatParameters, which will be Float64 values constrained to an interval. The intervals are [-2.0, 2.0] for both parameters, and their values start at 0.0. Since we already used the names \"i0\" and \"i1\", we name the parameters the same way:configuration = Configuration([FloatParameter(-2.0, 2.0, 0.0, \"i0\"),\n                               FloatParameter(-2.0, 2.0, 0.0, \"i1\")],\n                               \"rosenbrock_config\")Now we must configure a new tuning run using the Run type. There are many parameters to configure, but they all have default values. Since we won\'t be using them all, please see Run\'s source for further details:tuning_run = Run(cost                = rosenbrock,\n                 starting_point      = configuration,\n                 stopping_criterion  = elapsed_time_criterion,\n                 report_after        = 10,\n                 reporting_criterion = elapsed_time_reporting_criterion,\n                 duration            = 60,\n                 methods             = [[:simulated_annealing 1];\n                                        [:iterative_first_improvement 1];\n                                        [:iterated_local_search 1];\n                                        [:randomized_first_improvement 1];\n                                        [:iterative_greedy_construction 1];])The methods array defines the search methods, and their respective number of instances, that will be used in this tuning run. This example uses one instance of every implemented search technique. The search will start at the point defined by starting_point.The stopping_criterion parameter is a function. It tells your autotuner when to stop iterating. The two default criteria implemented are elapsed_time_criterion and iterations_criterion.  The reporting_criterion parameter is also function, but it tells your autotuner when to report the current results. The two default implementations are elapsed_time_reporting_criterion and iterations_reporting_criterion.  Take a look at the code if you want to dive deeper.We are ready to start autotuning, using the @spawn macro. For more information on how parallel and distributed computing works in Julia, please check the Julia Docs.  This macro call will run the optimize method, which receives a tuning run configuration and runs the search techniques in the background. The autotuner will write its results to a RemoteChannel stored in the tuning run configuration:@spawn optimize(tuning_run)\nresult = take!(tuning_run.channel)The tuning run will use the default neighboring and perturbation methods implemented by NODAL to find new results. Now we can process the current result. In this example we just print it and loop until optimize is done:print(result)\nwhile !result.is_final\n    result = take!(tuning_run.channel)\n    print(result)\nendRunning the complete example, we get:$ julia --color=yes rosenbrock.jl\n[Result]\nCost              : 1.0\nFound in Iteration: 1\nCurrent Iteration : 1\nTechnique         : Initialize\nFunction Calls    : 1\n  ***\n[Result]\nCost              : 1.0\nFound in Iteration: 1\nCurrent Iteration : 3973\nTechnique         : Initialize\nFunction Calls    : 1\n  ***\n[Result]\nCurrent Iteration : 52289\nTechnique         : Iterative First Improvement\nFunction Calls    : 455\n  ***\n[Result]\nCost              : 0.01301071782455056\nFound in Iteration: 10\nCurrent Iteration : 70282\nTechnique         : Randomized First Improvement\nFunction Calls    : 3940\n  ***\n[Result]\nCost              : 0.009463518035824526\nFound in Iteration: 11\nCurrent Iteration : 87723\nTechnique         : Randomized First Improvement\nFunction Calls    : 4594\n  ***\n[Final Result]\nCost                  : 0.009463518035824526\nFound in Iteration    : 11\nCurrent Iteration     : 104261\nTechnique             : Randomized First Improvement\nFunction Calls        : 4594\nStarting Configuration:\n  [Configuration]\n  name      : rosenbrock_config\n  parameters:\n    [NumberParameter]\n    name : i0\n    min  : -2.000000\n    max  : 2.000000\n    value: 1.100740\n    ***\n    [NumberParameter]\n    name : i1\n    min  : -2.000000\n    max  : 2.000000\n    value: 1.216979\nMinimum Configuration :\n  [Configuration]\n  name      : rosenbrock_config\n  parameters:\n    [NumberParameter]\n    name : i0\n    min  : -2.000000\n    max  : 2.000000\n    value: 0.954995\n    ***\n    [NumberParameter]\n    name : i1\n    min  : -2.000000\n    max  : 2.000000\n    value: 0.920639"
},

{
    "location": "autodocs/#",
    "page": "Docstrings",
    "title": "Docstrings",
    "category": "page",
    "text": "NODAL.AbstractResultNODAL.BoolParameterNODAL.ConfigurationNODAL.EnumParameterNODAL.FloatParameterNODAL.IntegerParameterNODAL.NODALNODAL.NumberParameterNODAL.ParameterNODAL.PermutationParameterNODAL.ResultNODAL.ResultChannelNODAL.RunNODAL.StringParameterNODAL.elapsed_time_criterionNODAL.elapsed_time_reporting_criterionNODAL.end_search_tasks!NODAL.evalNODAL.first_improvementNODAL.get_new_bestNODAL.greedy_constructionNODAL.includeNODAL.initialize_search_tasks!NODAL.iterated_local_searchNODAL.iterations_criterionNODAL.iterations_reporting_criterionNODAL.iterative_first_improvementNODAL.iterative_gredy_constructionNODAL.iterative_greedy_constructionNODAL.iterative_probabilistic_improvementNODAL.log_temperatureNODAL.measure_mean!NODAL.metropolisNODAL.neighbor!NODAL.optimizeNODAL.perturb!NODAL.perturb_elements!NODAL.probabilistic_improvementNODAL.rand_inNODAL.random_walkNODAL.randomized_first_improvementNODAL.sequential_measure_mean!NODAL.simulated_annealingNODAL.techniqueNODAL.test_ANODAL.test_namesNODAL.unit_valueNODAL.unit_value!NODAL.update!"
},

]}
